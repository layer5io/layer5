---
title:  "Service Mesh Performance"
subtitle: "Measuring and indexing the performance, overhead, and value of the world's service mesh deployments."
thumbnail: /assets/projects/smp/stacked/smp-white.svg
published: true
---

import { ProjectWrapper } from "../Project.style.js";
import { Container, Row, Col } from "../../../reusecore/Layout";
import smp from "./smp-dark.svg";
import img1 from "./native-and-wasm-at-capacity-100rps.png";
import img2 from "./latency-at-scale.png";
import img3 from "./client-capacity.png";

<ProjectWrapper>

<Row>
  <Col sm={12} md={5}>
    <img src={smp} />
  </Col>
  <Col sm={12} md={7} className="heading">
    <h2>Service Mesh Performance (SMP) is a vendor-neutral specification to standardize service mesh value meausurement.</h2>
  </Col>
</Row>

<p>The Service Mesh Performance Working Group is hosted within CNCF SIG Network. All are welcome to participate. This group is defining the Service Mesh Performance (SMP). Using SMP, MeshMark provides a universal performance index to gauge your mesh’s efficiency against deployments in other organizations’ environments.</p>
<h3 className="center">SMP is a collaborative effort of Layer5, UT Austin, Google, and The Linux Foundation.</h3>

<Row>
  <Col className="center-col" sm={12} md={4}>
    <p>SMP accounts for details of:</p>
    <ul className="bullet">
        <li>Environment and infrastructure details</li>
        <li>Service mesh and its configuration</li>
        <li>Service (workload) details</li>
        <li>Statistical analysis of performance results</li>
    </ul>
  </Col>
  <Col className="center-col" sm={12} md={4}>
    <h2>Learn more at <a href="https://smp-spec.io">smp-spec.io</a></h2>
  </Col>
</Row>
<p>The group is also working in collaboration with the Envoy project to create easy-to-use tooling around <a href="/projects/service-mesh-distributed-performance-management">distributed performance management</a> (distributed load generation and analysis) in context of Istio, Consul, Tanzu Service Mesh, Network Service Mesh, App Mesh, Linkerd, and so on.</p>

<div className="flex-iframe" >
  <h4 class="black-text center">
    Discreetly Studying the Effects of Individual Traffic Control Functions
    <div class="responsive-holder">
    <iframe class="responsive-iframe" width="560" height="315" src="https://www.youtube.com/embed/rgnb0-ntPko"
      frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"allowfullscreen>
    </iframe>        
    </div>
  <p ><i>KubeCon EU 2020 - Lee Calcote & Prateek Sahu</i></p>
  </h4>
</div> 

<div>
  <h3 className="black-text">Performance of Envoy Filters</h3>
  The following analysis compares native Envoy filter performance to WebAssembly (WASM) filter performance using Rust.
  <br /><br />
  <Row>
    <Col sm={12} md={5}>
      <a href="/assets/projects/smp/kubecon-eu/native-and-wasm-at-capacity-100rps.png">
        <img src={img1} />
      </a>
    </Col>
    <Col sm={12} md={7}>
      <b>Native WASM at Capacity:</b> <br />
      When every request goes via the
      rate-limit check and then the actual program logic, we see that
      the latency incurred for the WASM code is higher than the Native
      client. This is expected since the native client has processing
      for rate-limiting locally in a process whereas the rust module
      is invoked as an additional thread to do the processing and the
      communication involved with the module incurs an overhead. This
      is prominent in the minimum response time case which represents
      latency just due to rate-limiting logic where every other part
      of the request is already "warm". As we move towards average
      latency, the overhead gets slightly amortized but is still above
      the native rate-limiting case. Our max latency is slightly lower
      than native, but we attribute it to various other system effects
      like TLS handshake and network latencies that usually contribute
      to the maximum tail latency.
    </Col>
  </Row>
  <br />< br/>
  <Row>
    <Col sm={12} md={5}>
    <a href="/assets/projects/smp/kubecon-eu/latency-at-scale.png">
      <img src={img2}/>
    </a>
    </Col>
    <Col sm={12} md={7}>
    <b>Latency at scale:</b><br />
    When we go beyond the
    application capacity (100 in our example), we start noticing the
    power of a in-line ight wasm module which starts terminating
    requests at the side-car and the core application logic is never
    invoked/loaded. We notice that even the minimum response time
    for a terminated request is about 15-20% faster than invoking of
    application logic since the wasm is a dynamic module in the
    sidecar and we start to avoid complex network redirection and
    invocation of a new container/instance. We also notice that the
    average latency of requests is lower than in the case of native
    client.
    </Col>
  </Row>
  <br /><br />
  <Row>
    <Col className="center-col" sm={12} md={3}>
      <a href="/assets/projects/smp/kubecon-eu//client-capacity.png">
        <img src={img3}/>
      </a>
    </Col>
    <Col className="center-col" sm={12} md={5}>
      <br /><b>Client Capacity:</b><br />
      Client Capacity figure also shows us that we are able to handle
      more requests than in the native case, although this infometric
      needs to be taken with a grain of salt, i.e. the difference
      might reduce if our application capacity was significantly
      larger than 100.
    </Col>
  </Row>
</div>

<br />
<Row>
  <Col className="center-col" sm={12} md={6}>
    <blockquote className="twitter-tweet">
      <p lang="en" dir="ltr">
        Learn more about the service mesh performance initiatives at
        <a href="https://twitter.com/layer5?ref_src=twsrc%5Etfw">@layer5</a>
        <a href="https://t.co/hNUM4pHDqi">https://t.co/hNUM4pHDqi</a>
      </p>
      &mdash; Service Mesh Performance (@smp_spec)
      <a href="https://twitter.com/smp_spec/status/1290428583249354757?ref_src=twsrc%5Etfw">August 3, 2020</a>
    </blockquote>
    <h3 style="text-align: center; font-weight: bold;">
      Jump into the
      <a href="http://slack.layer5.io">#SMP channel</a> to learn more
      about these initatives.
    </h3>
  </Col>
</Row>

</ProjectWrapper>