<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Layer5 Technical Posts]]></title><description><![CDATA[Expect more from your infrastructure. Cloud native, open source software for your cloud native infrastructure and applications. Allowing developers to focus on business logic, not infrastructure concerns. Empowering operators to confidentally run modern infrastructure.]]></description><link>https://layer5.io</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 16 Jan 2023 10:07:37 GMT</lastBuildDate><item><title><![CDATA[The Most Innovative Cloud Management Companies]]></title><link>https://layer5.io/company/news/the-most-innovative-cloud-management-companies</link><guid isPermaLink="false">https://layer5.io/company/news/the-most-innovative-cloud-management-companies</guid><dc:creator><![CDATA[Best Startup Texas]]></dc:creator><pubDate>Wed, 11 Jan 2023 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/bc8c3bab97b6d1c1577838cfae767899/best-startup-texas.jpeg" length="0" type="image/jpeg"/><content:encoded>&lt;div class=&quot;Newsstyle__NewsWrapper-sc-12r6uiw-0 TVDRO&quot;&gt;&lt;p&gt;Layer5 is featured as one of the most innovative cloud management companies by Best Startup Texas. Out of 100,000 Texas-based startups, Layer5 was selected based on oustanding marks in the following categories:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Company track record&lt;/li&gt;&lt;li&gt;Executive leadership&lt;/li&gt;&lt;li&gt;Market share&lt;/li&gt;&lt;li&gt;Innovation&lt;/li&gt;&lt;li&gt;ESG rating&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Learn more about &lt;a href=&quot;/about&quot;&gt;Layer5&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Terraform with Meshery]]></title><description><![CDATA[Terraform Infrastructure as Code with Meshery]]></description><link>https://layer5.io/resources/cloud-native/terraform-with-meshery</link><guid isPermaLink="false">https://layer5.io/resources/cloud-native/terraform-with-meshery</guid><dc:creator><![CDATA[Gaurav Chadha]]></dc:creator><pubDate>Thu, 22 Dec 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/895ec8ea35cf68449389f73e4285cb39/terraform-color.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 gNYEES&quot;&gt;&lt;p&gt;Terraform is a powerful tool that helps users manage and provision infrastructure resources in a consistent and efficient manner. With Terraform, you can define your infrastructure as code, using human-readable configuration files that can be versioned, shared, and reused. This makes it easy to create, modify, and manage your infrastructure resources, whether they are cloud-based or on-premises.&lt;/p&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;It is an open source tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;One way to further enhance your use of Terraform is by integrating it with Meshery. Meshery is a cloud-native management platform that provides a unified interface for managing and monitoring your infrastructure resources, including those managed by Terraform. By integrating Terraform with Meshery, you can leverage the power and flexibility of both tools to streamline your infrastructure management process.&lt;/p&gt;&lt;p&gt;One of the key benefits of using Terraform with Meshery is the ability to manage and monitor your infrastructure resources in a consistent and centralized manner. With Meshery, you can view and manage all of your infrastructure resources, whether they are managed by Terraform or other tools, from a single dashboard. This allows you to quickly identify any issues or potential problems with your infrastructure, and take action to resolve them in a timely manner.&lt;/p&gt;&lt;p&gt;Another benefit of using Terraform with Meshery is the ability to automate your infrastructure management process. With Meshery, you can create and manage automated pipelines for provisioning and managing your infrastructure resources. This can help to reduce the time and effort required to manage your infrastructure, and allow you to focus on other important tasks.&lt;/p&gt;&lt;p&gt;In addition to these benefits, using Terraform with Meshery also provides a number of other advantages. For example, Meshery integrates with a wide range of tools and platforms, allowing you to easily incorporate your existing infrastructure resources into your management process. This can help to reduce the complexity of managing your infrastructure, and make it easier to keep everything running smoothly.&lt;/p&gt;&lt;p&gt;Overall, the use of Terraform with Meshery can help to streamline and improve your infrastructure management process. By integrating these two powerful tools, you can gain greater visibility and control over your infrastructure resources, and automate many of the tasks involved in managing them. This can help to reduce the time and effort required to manage your infrastructure, and allow you to focus on other important tasks. So, it is a good idea to use Terraform with Meshery to improve the efficiency and effectiveness of your infrastructure management process.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Kubernetes 1.26 Highlights, Features, and Deprecations]]></title><description><![CDATA[Release Notes: What changed in Kubernetes 1.26?]]></description><link>https://layer5.io/blog/kubernetes/kubernetes-126-highlights-features-and-deprecations</link><guid isPermaLink="false">https://layer5.io/blog/kubernetes/kubernetes-126-highlights-features-and-deprecations</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Tue, 06 Dec 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/cb5e88af4075e63b16a33b56b3659b9a/kubernetes-new.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 gNYEES&quot;&gt;&lt;p&gt;As the final Kubernetes release of 2022, Kubernetes 1.26 is an exciting new release of the popular container orchestration platform. It offers a number of new features and improvements that will help platform engineers and DevOps engineers manage their Kubernetes clusters more effectively. Here are some of the highlights of this release.&lt;/p&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;As a longstanding CNCF member, Layer5 has donated two of its open source projects to the CNCF: &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt;Meshery&lt;/a&gt; and &lt;a href=&quot;/projects/service-mesh-performance&quot;&gt;Service Mesh Performance&lt;/a&gt;. As an end-to-end, open-source, multi-cluster Kuberentes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;While there are a number of enhancments tracked in this release (38), you need to be aware that there are also a number of features being deprecated (10) in 1.26. In this article, we will focus on some highlighted enhancements, important deprecations, and removals so that you can be confident before upgrading your clusters. &lt;/p&gt;&lt;p&gt;We&amp;#x27;ll breakdown new K8s features by category, starting with networking.&lt;/p&gt;&lt;h2&gt;Networking in Kubernetes 1.26&lt;/h2&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/2086&quot;&gt;Service Internal Traffic Policy&lt;/a&gt; [Stable]&lt;/h3&gt;&lt;p&gt;When requests are made to a Kubernetes service, they are randomly distributed to all available endpoints. The new enhancement enriches the API of a service to use node-local and topology-aware routing for internal traffic. The new internalTrafficPolicy field has two options: Cluster (default) and Local. The Cluster option works like before and tries distributing requests to all available endpoints. On the other hand, the Local option only sends requests to node-local endpoints and drops the request if there is no available instance on the same node. The Local option is useful for sending metrics or logs to an agent running as a DaemonSet. &lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/3070&quot;&gt;Reserve Service IP Ranges for Dynamic and Static IP Allocation&lt;/a&gt; [Stable]&lt;/h3&gt;&lt;p&gt;Kubernetes services are assigned a virtual ClusterIP to be reachable inside the cluster. The ClusterIP is either assigned dynamically from a configured Service IP range, or statically set while creating the service resource. There was no possibility of knowing whether another service in the cluster had already used the static ClusterIP before this new stable enhancement. With this change, the IP range is divided into two; this prevents conflicts between services implementing dynamic IP allocation and static IP assignment. The flag --service-cluster-ip-range, with CIDR notation, is part of the Kubernetes API server configuration and is ready to use with the 1.26 release. &lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/1435&quot;&gt;Support of Mixed Protocols in Services with Type LoadBalancer&lt;/a&gt; [Stable]&lt;/h3&gt;&lt;p&gt;Kubernetes Services that use the LoadBalancer type have only supported a single Layer 4 protocol until now. With this enhancement going from graduating to stable in v1.26, it is possible to define a mix of protocols in the same service definition. In other words, this enhancement allows a LoadBalancer Service to serve different protocols (e.g. UDP, TCP) under the same port (e.g. 443). For example, serving both UDP and TCP requests for a DNS or SIP server on the same port. For instance, you can expose a DNS server with a single load balancer IP for both TCP and UDP requests, such as the following:&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-yaml&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; v1&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; Service&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; multi&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;dns&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;server&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; LoadBalancer&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; dns&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;udp&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token number&quot; style=&quot;color:rgb(247, 140, 108)&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; UDP&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; dns&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;tcp&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token number&quot; style=&quot;color:rgb(247, 140, 108)&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; TCP&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; dns&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;server&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h2&gt;Security in Kubernetes 1.26&lt;/h2&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/2133&quot;&gt;kubelet Credential Provider&lt;/a&gt; [Stable]&lt;/h3&gt;&lt;p&gt;The kubelet agent has a built-in credential provider mechanism to retrieve credentials for container image registries. It natively supports Azure, Google Cloud, and AWS container image registries for dynamically retrieving their credentials. The new stable enhancement in v1.26 offers a replacement for the in-tree implementations, and creates an API for extensible plugins in the future. &lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/3031&quot;&gt;SignedSigning Release Artifacts&lt;/a&gt; [Beta]&lt;/h3&gt;&lt;p&gt;Every Kubernetes release produces a set of artifacts such as binaries, container images, documentation, and metadata. Since the 1.24 release, the artifacts have been signed as an alpha feature. In the 1.26 release, artifact signing graduates to beta to increase software supply chain security for the Kubernetes release process and mitigate man-in-the-middle attacks.&lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/2799&quot;&gt;Reduction of Secret-Based Service Account Tokens&lt;/a&gt; [Beta]&lt;/h3&gt;&lt;p&gt;&lt;code&gt;BoundServiceAccountTokenVolume&lt;/code&gt; has been GA since version 1.22: Service account tokens for pods are obtained via the TokenRequest API and stored as a projected volume. The new enhancement, in beta, eliminates the need to auto-generate secret-based service account tokens. In addition, Kubernetes will warn about using auto-created secret-based service account tokens, and purge the unused ones.&lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/1981&quot;&gt;Windows Privileged Containers&lt;/a&gt; [Stable] and &lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/3503&quot;&gt;Host Networking&lt;/a&gt; [Alpha]&lt;/h3&gt;&lt;p&gt;Privileged containers are the ones that have similar access and capabilities to the host processes running on the servers. In Linux environments, they are used heavily in Kubernetes for storage, networking, and management. In this release, support for privileged containers for the Windows environment graduates to stable. Management of processes is heavily different from the operating system standpoint in Linux and Windows. Therefore, privileged containers will also work differently in two environments, but they will ensure the same level of security and operational experience.&lt;/p&gt;&lt;p&gt;In addition, there is a new alpha-level enhancement in this release to support host networking for Windows pods. Currently, Windows has all the functionality to make containers use the networking namespace of the nodes. The new alpha enhancement enables this functionality from the Kubernetes side, increasing the parity between Linux and Windows containers.&lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/33250&quot;&gt;Self-User Attribute and Authentication API&lt;/a&gt; [Alpha]&lt;/h3&gt;&lt;p&gt;Kubernetes has no resources to identify and manage users as part of its API. Instead, it uses authenticators to get user attributes from tokens, certificates, OIDC providers, or webhooks. The new alpha feature adds a new API endpoint to see what attributes the current users have. The new API is under authentication.k8s.io with the name SelfSubjectReview, and there is a new corresponding command as well: kubectl auth who-am-i. The new feature will reduce the obscurity of complex authentication and help users debug the authentication stack. &lt;/p&gt;&lt;h2&gt;Scheduling in Kubernetes 1.26&lt;/h2&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/2268&quot;&gt;Non-Graceful Node Shutdown for StatefulSet Pods&lt;/a&gt; [Beta]&lt;/h3&gt;&lt;p&gt;Kubernetes is a battle-tested platform that makes itself resistant to disaster in the data center. Graceful node shutdown is already part of kubelet, to detect and move workloads afterward. However, when the shutdown is not detected by the kubelet, the pods of a &lt;code&gt;StatefulSet&lt;/code&gt; are stuck as &lt;code&gt;Terminating&lt;/code&gt; and not transferred to a healthy node. This happens due to a conflict in the Kubernetes machinery: The kubelet on the down node will not delete its pods from Kubernetes API, and the StatefulSet controller will not create new pods with the same name. With this enhancement, moving to beta, the pods will be forcefully deleted with their volume attachments, and new pods will be created on the healthy nodes.&lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/3521&quot;&gt;Pod Scheduling Readiness&lt;/a&gt; [Alpha]&lt;/h3&gt;&lt;p&gt;Currently, pods are considered ready for scheduling as soon as they are created. However, not every pod requires a node, resource allocation, and the start of all its containers immediately after its creation. The new alpha enhancement adds an API to mark pods with their scheduling status: paused and ready. Pods with the .spec.schedulingGates field will be parked in the scheduler and only be assigned to nodes when they are ready to be scheduled.&lt;/p&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/3515&quot;&gt;OpenAPI v3 for kubectl explain&lt;/a&gt; [Alpha]&lt;/h3&gt;&lt;p&gt;Kubernetes has supported OpenAPI v3 as a beta since version 1.24, providing users with a richer representation of the fields in the Kubernetes API. With the new alpha enhancement, the kubectl explain command will use the rich type information specified by OpenAPI v3 instead of OpenAPI v2.&lt;/p&gt;&lt;h2&gt;Deprecations and Removals&lt;/h2&gt;&lt;p&gt;Consistent to the Kubernetes API lifecycle is deprecations and removals of APIs in each release. It is strongly suggested to check whether you are using the following APIs and flags before there are breaking changes.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Removal of the `flowcontrol.apiserver.k8s.io/v1beta1` API group for `FlowSchema` and `PriorityLevelConfiguration` requires a migration to the v1beta2 API version.&lt;/li&gt;&lt;li&gt;Removal of the `autoscaling/v2beta2` API version for HorizontalPodAutoscaler requires a migration to the autoscaling/v2 API version.&lt;/li&gt;&lt;li&gt;Removal of legacy and vendor-specific authentication client-go and kubectl for Azure and Google Cloud requires migration to vendor-neutral authentication plugin mechanisms.&lt;/li&gt;&lt;li&gt;Removal of in-tree CSI integration for OpenStack—namely, the `cinder` volume type—requires a migration to use the CSI driver for OpenStack.&lt;/li&gt;&lt;li&gt;Some unused options and flags for the kubectl run command are marked as deprecated in the 1.26 release, such as `--grace-period`, `--timeout`, and `--wait`.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Last Kubernetes release of 2022&lt;/h2&gt;&lt;p&gt;Kubernetes is an ever-evolving platform. For those of you running workloads on Kubernetes taking detailed note of API changes and enhancements is an important activity as you endevour to keep your clusters upgraded with release releases. A more secure, scalable, and flexible Kubernetes is our collective goal. Dign into more details about deprecation, removals, and the latest changes in the 1.26 &lt;a href=&quot;https://relnotes.k8s.io/&quot;&gt;release notes&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;On behalf of the Layer5 community and all of the CNCF projects that its contributors steward, thank you to everyone who participated in this Kubernetes release, and congratulations! &lt;/p&gt;&lt;p&gt;As an end-to-end, open-source, multi-cluster Kuberentes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you. &lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Structured logging in Kubernetes with Klog]]></title><description><![CDATA[Structured logging in Kubernetes 1.26 with Klog]]></description><link>https://layer5.io/blog/kubernetes/structured-logging-in-kubernetes-with-klog</link><guid isPermaLink="false">https://layer5.io/blog/kubernetes/structured-logging-in-kubernetes-with-klog</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Mon, 05 Dec 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/1fa301d3f987db757b54e34bfe04b9a5/kubernetes-logs.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 gNYEES&quot;&gt;&lt;p&gt;As a platform for developers and system administrators to easily deploy and manage applications in a distributed environment, Kubernetes clusters generate logs and lots of them. One of the key components of Kubernetes is its logging and instrumentation capabilities. The upcoming Kubernetes 1.26 release has a handful of noteworthy changes to its system component logger, &lt;code&gt;klog&lt;/code&gt;.&lt;/p&gt;&lt;h2&gt;Kubernetes System Log&lt;/h2&gt;&lt;p&gt;System component logs record events happening in K8s clusters. More than metrics or traces, logs are the telemetric signal often found to be most useful for debugging. You can configure K8s log verbosity to see more or less detail. Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or scheduler decisions).&lt;/p&gt;&lt;h2&gt;Klog&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes/klog&quot;&gt;klog&lt;/a&gt; is a Kubernetes logging library that provides an API for developers and system administrators to instrument their applications for logging and tracing. klog generates log messages for the Kubernetes system components. It provides a comprehensive set of features, including log levels, structured logging and logging context.&lt;/p&gt;&lt;p&gt;Kubernetes 1.23 introduced structured logging (in beta) in &lt;code&gt;klog&lt;/code&gt;. Structured logging is a uniform structure in log messages allowing for programmatic extraction of information. Structured logs can be stored and processed with less effort and cost. The code which generates a log message determines whether it uses the traditional unstructured &lt;code&gt;klog&lt;/code&gt; output or structured logging.&lt;/p&gt;&lt;p&gt;As a dependency to structured logging (gated behind &lt;code&gt;StructuredLogging&lt;/code&gt; feature gate), Kubernetes 1.24 introducted contextual logging (in alpha) in &lt;code&gt;klog&lt;/code&gt;. Contextual logging builds on top of structured logging. It is primarily about how developers use logging calls: code based on that concept is more flexible and supports additional use cases which will be the topic of a future blog post. &lt;/p&gt;&lt;h3&gt;Klog Deprecations in Kubernetes 1.26&lt;/h3&gt;&lt;p&gt;Kubernetes has recently announced that it intends to deprecate certain flags related to Klog in its components. This means that Klog-specific flags, such as &lt;code&gt;--klog-verbosity&lt;/code&gt;, &lt;code&gt;--klog-vmodule&lt;/code&gt; and &lt;code&gt;--klog-stderrthreshold&lt;/code&gt; will no longer be supported. This is due to the fact that Klog has been largely superseded by the more comprehensive OpenTelemetry project, which provides a more complete solution for logging and instrumentation.&lt;/p&gt;&lt;p&gt;The deprecation of Klog-specific flags is a positive step forward for Kubernetes as it moves to OpenTelemetry. This move will ensure that Kubernetes is using the industry-standard logging and instrumentation solution. It will also provide developers and system administrators with a more comprehensive, reliable and consistent experience when instrumenting their applications.
A goal of this deprecation is one of unblocking development of alternative logging formats. Why does Kubnernetes need another logging format? One reason is performance. Klog performance is much worse than alternatives, for example 7-8x than JSON format:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;logger&lt;/th&gt;&lt;th&gt;time [ns/op]&lt;/th&gt;&lt;th&gt;bytes[B/op]&lt;/th&gt;&lt;th&gt;allocations[alloc/op]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Text Infof&lt;/td&gt;&lt;td&gt;2252&lt;/td&gt;&lt;td&gt;248&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Text InfoS&lt;/td&gt;&lt;td&gt;2455&lt;/td&gt;&lt;td&gt;280&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;JSON Infof&lt;/td&gt;&lt;td&gt;1406&lt;/td&gt;&lt;td&gt;19&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;JSON InfoS&lt;/td&gt;&lt;td&gt;319&lt;/td&gt;&lt;td&gt;67&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Proof of concept implementation of new logging formats were completed to assess the potentional gains of using an alternative format. Results measured on 30s benchmark for passing 2 arguments to format function.&lt;/p&gt;&lt;div class=&quot;tip&quot;&gt;&lt;h3&gt;Tip: Logger Performance Comparison&lt;/h3&gt;&lt;p&gt;Interestingly, Klog isn&amp;#x27;t the fastest logger in the West, but Uber&amp;#x27;s open source project &lt;a href=&quot;https://github.com/uber-go/zap&quot;&gt;zap &lt;/a&gt; appears to hold that title instead. The following performance test benchmark is an examle of one of a number of scenarios in which loggers can be performance analyzed.&lt;/p&gt;&lt;p&gt;Log a message and 10 fields:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Package&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Time&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Time % to zap&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Objects Allocated&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;zap&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;2900 ns/op&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;+0%&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;5 allocs/op&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;zap (sugared)&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;3475 ns/op&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;+20%&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;10 allocs/op&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;zerolog&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;10639 ns/op&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;+267%&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;32 allocs/op&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;go-kit&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;14434 ns/op&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;+398%&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;59 allocs/op&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;logrus&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;17104 ns/op&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;+490%&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;81 allocs/op&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;apex/log&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;32424 ns/op&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;+1018%&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;66 allocs/op&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;log15&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;33579 ns/op&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;+1058%&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;76 allocs/op&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;p&gt;Output will always be written to stderr, regardless of the output format. Output redirection is expected to be handled by the component which invokes a Kubernetes component. This can be a POSIX shell or a tool like systemd.&lt;/p&gt;&lt;p&gt;The deprecation of Klog-specific flags is part of a larger effort to transition Kubernetes to a more modern and comprehensive logging and instrumentation solution. This will provide Kubernetes users with a more reliable, secure and consistent experience when instrumenting and monitoring their applications.&lt;/p&gt;&lt;p&gt;Kubernetes will continue to provide support for Klog-specific flags for the foreseeable future. However, it is recommended that developers and system administrators begin transitioning their applications to the OpenTelemetry framework. This will ensure that their applications are using the industry-standard solution for logging and instrumentation.&lt;/p&gt;&lt;p&gt;Overall, the deprecation of Klog-specific flags is a positive step forward for Kubernetes. It will ensure that Kubernetes users have access to the most reliable and comprehensive solution for logging and instrumentation. It will also help ensure that Kubernetes is using the industry-standard solution and will provide developers and system administrators with a more reliable and consistent experience when instrumenting their applications.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Management of Kubernetes]]></title><description><![CDATA[Management of Kubernetes]]></description><link>https://layer5.io/resources/kubernetes/management-of-kubernetes</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/management-of-kubernetes</guid><dc:creator><![CDATA[Tolulope Ola-David]]></dc:creator><pubDate>Mon, 21 Nov 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/9287b4a708bb510f64057ea305498b77/kubernetes-logo.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;h3&gt; What is Kubernetes Management, and Why Should You Care? &lt;/h3&gt;&lt;p&gt; It is easy to understand why Kubernetes has become one of the most popular tools on the market today. Primarily, it allows you to easily manage Docker containers across your entire infrastructure with very little overhead, making it easier than ever to manage massive amounts of information in a timely manner. But what are you supposed to do with all this information? That’s where Kubernetes management comes into play—the process of using that information in an effective manner can make or break your efforts, so it’s essential that you choose the right solutions from the get-go.&lt;/p&gt;&lt;h3&gt; Defining Kubernetes management &lt;/h3&gt;&lt;p&gt; Kubernetes management is the process of managing your containers on a Kubernetes cluster. This can include things like adding or removing clusters, scaling clusters up or down, balancing workloads across nodes in a cluster, and restarting failed containers or nodes in a cluster. These tasks are complicated and involve many different types of actions. Figuring out how to do them all manually would be extremely time-consuming. Fortunately, there are tools like Meshery that automate these tasks for you, making it easier to see what’s going on within your cluster so you can make informed decisions about what needs to happen next. Staying on top of Kubernetes management will not only keep your cluster running smoothly but also help prevent problems before they occur. Automating this process will save you time and money, leaving more time to focus on other aspects of the business. When things go wrong, automated Kubernetes management allows you to have a plan and know exactly what steps need to be taken to recover from an incident. With these benefits in mind, it’s important that companies with containerized infrastructure use some type of automation for their Kubernetes management.&lt;/p&gt;&lt;h3&gt; The benefits of Kubernetes management &lt;/h3&gt;&lt;p&gt; Kubernetes management can seem like a daunting task. In the past, IT teams had to worry about maintaining large clusters of machines that required constant tweaking and monitoring. Kubernetes simplifies this process by automating tasks such as:&lt;ul&gt;&lt;li&gt; Monitoring cluster health &lt;/li&gt;&lt;li&gt; Deploying apps across nodes &lt;/li&gt;&lt;li&gt; Running rolling updates &lt;/li&gt;&lt;li&gt; Scaling up or down resources on demand &lt;/li&gt;&lt;li&gt; Auto-recover from failures &lt;/li&gt;&lt;li&gt; Application deployment consistency &lt;/li&gt;&lt;li&gt; Managing container upgrades &lt;/li&gt;&lt;/ul&gt;After reading through these benefits, you may be asking yourself, &amp;quot;Why should I care? Here are two reasons why you should care about Kubernetes management: - Kubernetes management has been shown to improve software development efficiency because it reduces time spent waiting for containers to restart and redeploy. A recent study showed that developers using Kubernetes were able to deploy new code changes at least 27% faster than developers without any container orchestration solution.&lt;/p&gt;&lt;blockquote&gt; Developers using Kubernetes were able to deploy new code changes at least 27% faster than developers without any container orchestration solution. &lt;/blockquote&gt;&lt;p&gt; Kubernetes management has also been shown to reduce operational costs because it eliminates the need for manual intervention in scaling applications, updating running containers with new versions, etc. If your IT team was spending 10 hours per week on manual operations before adopting Kubernetes, they&amp;#x27;ll spend only 2 hours after switching over! &lt;/p&gt;&lt;div class=&quot;CTA_FullWidth__CTA_FullWidthWrapper-sc-1rsxguj-0 eQLncZ get-start-kubernetes-resource&quot;&gt;&lt;img src=&quot;static/multi-cluster-kubernetes-management-with-meshery-27c0af75fc17484435e5b0ebd017108e.png&quot; alt=&quot;Multi-Cluster Kubernetes Management with Meshery&quot;/&gt;&lt;div class=&quot;cta-content&quot;&gt;&lt;div&gt;&lt;h3&gt;Layer5 Community&lt;/h3&gt;&lt;p&gt;Multi-Cluster Kubernetes Management with Meshery&lt;/p&gt;&lt;/div&gt;&lt;a href=&quot;/blog/meshery/multi-cluster-kubernetes-management-with-meshery&quot;&gt;&lt;button class=&quot;btnstyle__ButtonStyle-sc-mhxpaj-0 gmVeLL appion__btn&quot; title=&quot;Read blog post&quot;&gt; Read blog post&lt;/button&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;h3&gt; The challenges of Kubernetes management &lt;/h3&gt;&lt;p&gt; Kubernetes management can seem like a difficult endeavour. Between determining how to automate deployment and scaling and comprehending the fundamentals of how it operates, there are numerous factors to consider. Fortunately, there are numerous frameworks that simplify this procedure. But before going into new frameworks or technologies, you must grasp what Kubernetes administration comprises so that you know what you&amp;#x27;re attempting to automate. Kubernetes management comprises a variety of activities, such as building up clusters, keeping apps running on those clusters up-to-date, monitoring usage and providing alarms to keep things running smoothly, and shutting down clusters when they are no longer required. &lt;/p&gt;&lt;p&gt; There are numerous ways to manage these tasks: manually, with containers, with an orchestration system such as Ansible Tower, Cloud Control 12c, or ServiceNow NMS, with containers-as-a-service providers such as Docker Datacenter or AWS EKS, with container service offerings from cloud providers such as Azure Container Instances, by configuring Kubernetes with your own framework, and by installing Kubectl on your laptop for direct control. Each strategy has advantages and disadvantages that may make one more suitable for your organisation than another. Regardless of the approach you adopt, you must plan accordingly. &lt;/p&gt;&lt;p&gt; Importantly, the fact that Kubernetes is gaining popularity does not imply that it will replace your existing infrastructure layers. It augments their capabilities with scalability and large-scale application management (which would have been difficult without automation). In addition, the definition of management varies based on the size of the organisation: small businesses may prefer self-hosted platforms, whilst larger businesses would often primarily rely on SaaS solutions. &lt;/p&gt;&lt;h3&gt; How Meshery makes it easier to run Kubernetes &lt;/h3&gt;&lt;p&gt; Meshery is the only cloud-native manager in the world that supports more adapters than any other project or product. &lt;/p&gt;&lt;img src=&quot;static/meshery-core-architecture-ec315a3f425831b199604a1b7fc15362.png&quot; class=&quot;image-center&quot; alt=&quot;Management of Kubernetes with Meshery&quot;/&gt;&lt;p&gt; Meshery has been designed for the world of many service meshes and many Kubernetes clusters. As such, great attention was made to guarantee that it is an extensible management platform, able to handle a diverse range of infrastructure and new use cases quickly through its plugin mechanism. Meshery Server acts as an operation delegator, determining which Meshery Adapter has registered its capacity for the given operation. The operation is then sent to the appropriate component using a gRPC call. This could be one of Meshery&amp;#x27;s service mesh adapters, like the Istio adapter. &lt;/p&gt;&lt;p&gt; Meshery&amp;#x27;s capability is constantly expanding, from multi-mesh to now multi-cluster, to give developers, operators, and security engineers more control over their infrastructure. Each part of Meshery&amp;#x27;s architecture makes a big difference in how it manages multiple Kubernetes clusters. &lt;/p&gt;&lt;h3&gt; Meshery management across many clusters &lt;/h3&gt;&lt;p&gt; From the settings page, users can do things related to clusters, like add more clusters, remove data from existing clusters, or delete existing clusters. &lt;/p&gt;&lt;img src=&quot;static/settings-3b1ff1da147a0207419aafc162955ca1.png&quot; class=&quot;image-center&quot; alt=&quot;Management of Kubernetes with Meshery&quot;/&gt;&lt;p&gt; Meshery also deploys Meshery operators throughout the cluster it is about to manage. This operator is in charge of the Meshery broker and the MeshSync lifecycle. MeshSync is responsible for monitoring various types of resources by establishing a watch stream over each of them. MeshSync then sends the data to the NATS server, of which the Meshery server is a client. Meshery server then receives all necessary data relating to cluster activity. &lt;/p&gt;&lt;img src=&quot;static/context-switcher-85c34e8a44f40a03522e0b02b5689d68.png&quot; class=&quot;image-center&quot; alt=&quot;Management of Kubernetes with Meshery&quot;/&gt;&lt;p&gt; Meshery, by default, wants to be as aware of your infrastructure as possible in order to deliver value. As such, it deploys its operator across each identified cluster. However, you can fine-tune this configuration by going over each one. &lt;/p&gt;&lt;h3&gt; The future of Kubernetes management &lt;/h3&gt;&lt;p&gt; Kubernetes management has been one of the buzzwords since 2018. But what does it actually mean? And why should you care about it? At its core, Kubernetes management is a system that helps make sense of the nuances of how different containers work together to create an application. As we rely more on containers for our everyday apps, there needs to be a way to keep track of them all. That&amp;#x27;s where Kubernetes comes in with its ability to manage these containers that are spread out across different servers and understand which ones need more resources or want to be shut down because they&amp;#x27;re no longer needed. The easier it becomes for developers and engineers to deploy applications without worrying about how they are going to be managed, the better off everyone will be. Fortunately, as containerization grows in popularity among developers and IT teams alike, so does the number of tools for managing it. &lt;/p&gt;&lt;p&gt; A lot of container platforms provide native management functionality: Docker Swarm allows you to use simple commands like swarm stop or swarm pull when your swarm is up-to-date; Kargo automatically manages clusters using zero-touch configuration; Rancher provides tools to manage containers using any infrastructure stack; and Mesos offers both orchestration capabilities through Marathon as well as advanced resource scheduling features. It&amp;#x27;s not always easy to know which platform will work best for your organization, but it&amp;#x27;s important to find one that suits your company&amp;#x27;s needs—especially if IT is looking forward to a future without manual management tasks! &lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Getting Started with Kubernetes]]></title><description><![CDATA[Introduction to Kubernetes]]></description><link>https://layer5.io/resources/kubernetes/getting-started-with-kubernetes</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/getting-started-with-kubernetes</guid><dc:creator><![CDATA[Tolulope Ola-David]]></dc:creator><pubDate>Wed, 02 Nov 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/b1a646c34f1a9ed49dcf37dd7b9b4662/kubernetes-logo.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;p&gt; Kubernetes, an open-source container orchestration platform, is growing in popularity for deploying and managing cloud-native applications. Kubernetes was created by Google in 2014, and it is now used by many major companies, including IBM, Microsoft, Red Hat, and Amazon. In this article, we&amp;#x27;ll talk about Kubernetes, its benefits, and the best ways for your organization to use it.&lt;/p&gt;&lt;h3&gt; What is Kubernetes? &lt;/h3&gt;&lt;p&gt; Kubernetes offers fully managed and adapted architecture services that optimize your cloud-native application. Kubernetes is a platform that hides virtual machines, shows the infrastructure as an infrastructure-as-a-service (IAAS), network, and load balancer, and offers data storage and operations that are consistent across containers.&lt;/p&gt;&lt;p&gt; For example, Kubernetes nodes work as Kubernetes containers, such as an application, an application server, and control processes in Docker containers. Kubernetes components such as Kubernetes nodes and Kubernetes containers can be defined or modified via configuration files or can be specified subsequently. Individual Kubernetes components can be scaled according to elasticity needs to optimize performance.&lt;/p&gt;&lt;p&gt; Kubernetes optimizes a Kubernetes environment in the cloud, Docker containers on a system for development or testing, and the master or control plane of its cloud cluster management infrastructure.&lt;/p&gt;&lt;h3&gt; What&amp;#x27;s the Difference between Kubernetes and Docker? &lt;/h3&gt;&lt;p&gt; Over the past few years, containers have become increasingly popular within the software development community, and they have now evolved into two major platforms — Docker and Kubernetes. Both are incredibly powerful tools that allow developers to containerize their applications, but they are also slightly different in a number of ways, with more differences on the horizon as Docker announces its new focus on Kubernetes and containers orchestration. How do you decide which one to use? What does the future hold for each? Here’s what you need to know about the difference between Docker and Kubernetes.&lt;/p&gt;&lt;p&gt; Docker is an open-source platform designed to help developers and IT professionals create, deploy, and run applications. This containerization technology is often used in conjunction with orchestration software such as Kubernetes. However, these two technologies are not interchangeable; they serve different purposes. &lt;/p&gt;&lt;h3&gt; Why Should You Care About Kubernetes? &lt;/h3&gt;&lt;p&gt; Kubernetes was first made available for Google&amp;#x27;s internal use for DNS hosting. Open-source software projects were not able to use it. &lt;/p&gt;&lt;p&gt; Today, Kubernetes is in use by large-scale companies that use container orchestration. And in January 2019, The New Stack reported that a survey conducted in that month, which included the Kubernetes user group, discovered that Kubernetes reached more than 40,000 users and 200 companies were working on Kubernetes at that point. In addition, Gartner indicated that Kubernetes Inc. would make some $8.5 billion in 2019. &lt;/p&gt;&lt;h3&gt; What does Kubernetes Do? &lt;/h3&gt;&lt;p&gt; In contrast to an overall infrastructure, Kubernetes is a dynamic layer-oriented computing infrastructure. The essence of Kubernetes is how an entire infrastructure hops! Kubernetes is a container orchestration and management platform that has built-in features for self-replication, elasticity, and scalability. Through these and more features, Kubernetes &amp;quot;promovi-is&amp;quot; for container orchestrators for both production and lab environments. &lt;/p&gt;&lt;h3&gt; Kubernetes Architecture &lt;/h3&gt;&lt;p&gt; Even though Kubernetes is a software platform that lets organizations manage their application workloads in containers, a traditional Kubernetes cluster may not be the best solution for a number of business needs. &lt;/p&gt;&lt;img src=&quot;static/meshery-core-architecture-ec315a3f425831b199604a1b7fc15362.png&quot; class=&quot;image-center&quot; alt=&quot;Kubernetes Architecture&quot;/&gt;&lt;p&gt; A cluster of virtual computing resources is only one option, and it has its drawbacks. What happens when you lose disk space (which can happen if you don&amp;#x27;t add new containers, users, or workloads to a cluster)? Do you have another cluster for redundancy, and how do you integrate the two together? &lt;/p&gt;&lt;p&gt; Hyperconverged infrastructures like Red Hat OpenShift are an alternative that combine several technologies into a single virtual machine or physical machine. &lt;/p&gt;&lt;h3&gt; Best Practices for Kubernetes &lt;/h3&gt;&lt;p&gt; Kubernetes is a container orchestration platform created by Google in 2014. It provides a way for companies to build fully self-sufficient, scalable, multi-container applications every time they need to deploy and manage their own containers. It&amp;#x27;s aimed at pretty much the same audience as Docker and other container orchestration platforms—that is, organizations that run containerized applications and want to deploy scalable, repeatable deployments.&lt;/p&gt;&lt;p&gt; At the most basic and most simplistic level, a group of containers (usually 16) is cross-linked together in a cluster, based on Docker. Containers run inside a cluster of virtual machines (Kubernetes VM) as a single Linux file system. Kubernetes organizes the creation, deletion, and management of containers into container concepts that provide fault tolerance, availability, scaling, permission management, and secure containers that should be able to run together and share resources. Each host runs one or more containers, providing the abstraction of which containers can run on which hosts.&lt;/p&gt;&lt;p&gt; Since Kubernetes services are usually very easy to use, the user experience is very similar to that of centralized solutions. &lt;/p&gt;&lt;p&gt; With Kubernetes, businesses can make data repositories and containers, federate their resources in an efficient way, manage billing, certify capacity, quota, access rights, and more. &lt;/p&gt;&lt;p&gt; It can scale to many nodes simultaneously, so when their machines scale up, then their containers could scale up too. &lt;/p&gt;&lt;h3&gt; Kubernetes Concepts and Terminology &lt;/h3&gt;&lt;p&gt; Kubernetes was developed in 2014 as a Google container orchestrator, a container scheduler and more. Kubernetes was created to manage distributed applications, including Docker containers. According to SUSE, Kubernetes is simple to learn, easy to manage, and supports an on-premise, private, public, or hybrid architecture. Kubernetes is flexible enough to be split up over many servers in your data center. &lt;/p&gt;&lt;p&gt; This simplificator, one example of many, allows one to scale independent containers. &lt;/p&gt;&lt;p&gt; Let&amp;#x27;s understand better what Kubernetes is: &lt;/p&gt;&lt;p&gt; In an application ecosystem of operating system Docker containers, Kubernetes acts as a centralized management guided by distributed logic. Kubernetes can be used to deliver web traffic, graphics work, or IP traffic from IoT devices. The main benefit is that clusters can be easily expanded to a huge size with all functions. &lt;/p&gt;&lt;p&gt; What are pods? Pods are Docker instances that you can use to deploy your containers in environments like Kubernetes, which can be private, public, or a mix of the two.&lt;/p&gt;&lt;p&gt; Environments may be private services or public clouds. &lt;/p&gt;&lt;p&gt; Kubernetes can be used to manage containers because they are easy to use and make it easy to scale your containers. &lt;/p&gt;&lt;p&gt; Installation tutorials are sometimes yoinked without ever reading the help. &lt;/p&gt;&lt;h3&gt; RBAC and Firewall Security &lt;/h3&gt;&lt;p&gt; Today, everything is hackable, and so is your Kubernetes cluster. Hackers often try to find vulnerabilities in the system in order to exploit them and gain access. So, keeping your Kubernetes cluster secure should be a high priority. The first thing to do is make sure you are using RBAC in Kubernetes. RBAC is role-based access control. Assign roles to each user in your cluster and to each service account running in your cluster. Roles in RBAC contain several permissions that a user or service account can perform. You can assign the same role to multiple people, and each role can have multiple permissions.&lt;/p&gt;&lt;p&gt; RBAC settings can also be applied to namespaces, so if you assign roles to a user allowed in one namespace, they will not have access to other namespaces in the cluster. Kubernetes provides RBAC properties such as role and cluster role to define security policies. &lt;/p&gt;&lt;p&gt; You can create a firewall for your API server to prevent attackers from sending connection requests to your API server from the Internet. To do this, you can either use regular firewalling rules or port firewalling rules. If you are using something like GKE, you can use a master authorized network feature in order to limit the IP addresses that can access the API server. &lt;/p&gt;&lt;h3&gt; Managing Kubernetes Clusters &lt;/h3&gt;&lt;p&gt; Kubernetes is a project that lets you create and manage individual containers or a container cluster on a mainframe. Clusters may consist of physical, virtual, or cloud-based computing resources.&lt;/p&gt;&lt;div class=&quot;CTA_FullWidth__CTA_FullWidthWrapper-sc-1rsxguj-0 eQLncZ get-start-kubernetes-resource&quot;&gt;&lt;img src=&quot;static/multi-cluster-kubernetes-management-with-meshery-27c0af75fc17484435e5b0ebd017108e.png&quot; alt=&quot;Multi-Cluster Kubernetes Management with Meshery&quot;/&gt;&lt;div class=&quot;cta-content&quot;&gt;&lt;div&gt;&lt;h3&gt;Layer5 Community&lt;/h3&gt;&lt;p&gt;Multi-Cluster Kubernetes Management with Meshery&lt;/p&gt;&lt;/div&gt;&lt;a href=&quot;/blog/meshery/multi-cluster-kubernetes-management-with-meshery&quot;&gt;&lt;button class=&quot;btnstyle__ButtonStyle-sc-mhxpaj-0 gmVeLL appion__btn&quot; title=&quot;Read blog post&quot;&gt; Read blog post&lt;/button&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt; The Kubernetes projects auto-deploy container clusters anywhere there is a pluggable environment and an open-source base that includes system-config service, service account manager, and kubelet. So, developers and system administrators can easily put containers on a single machine or on nodes of machines in any scalable cluster to save money and time.&lt;/p&gt;&lt;p&gt; Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Kubernetes was made by Google, and the Cloud Native Computing Foundation now takes care of it.&lt;/p&gt;&lt;h3&gt; Set Resource Requests &amp;amp; Limits&lt;/h3&gt;&lt;p&gt; Occasionally, deploying an application to a production cluster can fail due to the limited resources available on that cluster. This is a common challenge when working with a Kubernetes cluster, and it’s caused when resource requests and limits are not set. Without resource requests and limits, pods in a cluster can start utilizing more resources than required. If the pod starts consuming more CPU or memory on the node, then the scheduler may not be able to place new pods, and even the node itself may crash. Resource requests specify the minimum amount of resources a container can use. &lt;/p&gt;&lt;p&gt; For both requests and limits, it’s typical to define CPU in millicores and memory in megabytes or mebibytes. Containers in a pod do not run if the request for resources made is higher than the limit you set.&lt;/p&gt;&lt;p&gt; In this example, we have set the limit of the CPU to 800 millicores and the memory to 256 mebibytes. The maximum request which the container can make at a time is 400 millicores of CPU and 128 mebibyte of memory.&lt;/p&gt;&lt;h3&gt; Guide to Containers &lt;/h3&gt;&lt;p&gt; Containers have been around for a while, but it wasn’t until Docker came along that they really took off. In its early days, developers were using it to build their applications in containers. Now companies like Walmart are using containers to deploy their entire infrastructure.&lt;/p&gt;&lt;p&gt; Containers are lighter-weight than virtual machines because they don&amp;#x27;t need to emulate an entire operating system. This is why containers are typically faster to start up and use less resources. However, containers cannot be moved between hosts like virtual machines can, so a more robust solution may be needed for this use case.&lt;/p&gt;&lt;p&gt; Because they&amp;#x27;re so lightweight and take up less space than VMs do, containers are great for running lots of them at once! If your application needs more computing power or memory than your machine can provide on its own, using multiple containers in parallel will help balance out any resource shortages without having to invest in additional physical hardware like you would with traditional VM-based deployments.&lt;/p&gt;&lt;p&gt; As they&amp;#x27;re isolated from each other, containers are great for running multiple apps at once without worrying about them stepping all over each other&amp;#x27;s toes! This makes them perfect for things like hosting websites or email services where you want lots of different people to be able to use it at the same time without slowing down or crashing because there&amp;#x27;s not enough resources available for everyone. &lt;/p&gt;&lt;p&gt; What&amp;#x27;s more, since they&amp;#x27;re so easy to spin up and take down, they&amp;#x27;re also great for testing out new ideas quickly without having to worry about making permanent changes to your system (or losing any data along the way!). So if you want to try out a new CMS but don&amp;#x27;t want to go through the trouble of installing it on your machine first, just fire up a container with it inside and see how it goes! &lt;/p&gt;&lt;p&gt; One downside to using containers is that they can&amp;#x27;t easily be moved between hosts like virtual machines can, so a more robust solution may be needed for this use case. Fortunately, there are some great open source projects out there that help solve this problem!&lt;/p&gt;&lt;h3&gt; Conclusion &lt;/h3&gt;&lt;p&gt; Kubernetes is a popular containerization solution that continues to see increasing adoption rates. That being said, using it successfully requires thorough consideration of your workflows and departmental best practices. &lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Service Mesh: Istio]]></title><description><![CDATA[Explanation of Istio]]></description><link>https://layer5.io/resources/service-mesh/service-mesh-istio</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/service-mesh-istio</guid><dc:creator><![CDATA[Deepesha Burse]]></dc:creator><pubDate>Wed, 31 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/731763d720780a49c2ffdfede8c28f4b/istio.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;p&gt; Microservice architectures offer some solutions while posing new ones. Application division into separate services makes scaling, updating, and development easier. It also provides you with a lot more moving pieces to connect and secure. It can get quite complicated to manage all of the network services, including load balancing, traffic management, authentication and authorisation, etc. &lt;/p&gt;&lt;p&gt; Istio, an open-source service mesh created by Google, IBM, and Lyft, enables you to connect, monitor, and secure microservices that are hosted on-premises, in the cloud, or with orchestration systems like Kubernetes and Mesos. The beta version of Istio was announced in the year 2018 in KubeCon on Google Cloud. &lt;/p&gt;&lt;p&gt; Before moving on to what Istio is and how it works, let us look into what service meshes are and why there was an urgent need for them as microservices started getting used more. &lt;/p&gt;&lt;h3&gt; Service Mesh &lt;/h3&gt;&lt;p&gt; A service mesh is an infrastructural layer that is used to provide secure communication between different services for on-prem, cloud or multi-cloud infrastructure. It allows us to add features like observability, traffic management, and security without having to add that to our code. The term &amp;quot;service mesh&amp;quot; refers to both the kind of software you employ to carry out this pattern and the security or network domain that results from its application. &lt;/p&gt;&lt;p&gt; Service meshes are divided into two parts: the control plane and the data plane. The control plane&amp;#x27;s responsibilities include securing the mesh, facilitating service discovery, doing regular health checks, enforcing policies, and handling other operational issues. A central registration of services and their corresponding IP addresses is referred to as service discovery. To share with other services how to communicate with it and to assist enforce rules on which services are allowed to communicate with which other services, the application must be registered on the control plane. &lt;/p&gt;&lt;p&gt; The communication between services, on the other hand, is handled by the data plane. Because many service mesh solutions use a sidecar proxy to manage data plane connections, the amount of knowledge that the services must have about the network environment is constrained. &lt;/p&gt;&lt;img src=&quot;static/service-mesh-609aa147db2609960150f75fb05ab088.svg&quot; class=&quot;image-center&quot; alt=&quot;Service Mesh&quot;/&gt;&lt;h3&gt; Inside the Istio service mesh &lt;/h3&gt;&lt;p&gt; A data plane and a control plane are logically separate parts of an Istio service mesh.&lt;ul&gt;&lt;li&gt; A group of intelligent proxies (Envoy) that are deployed as sidecars make up the data plane. All network connection among the microservices is mediated and managed by these proxies. Additionally, they gather and compile data on all mesh communications. &lt;/li&gt;&lt;li&gt; The proxies are controlled and set up by the control plane to route traffic. &lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;&lt;img src=&quot;static/arch-040603070065c5c8a1013a81573f9981.svg&quot; class=&quot;image-center&quot; alt=&quot;Istio Service Mesh Architecture&quot;/&gt;&lt;h4&gt; Envoy &lt;/h4&gt;&lt;p&gt; The data plane of Istio consists of the Envoy sidecar proxy. Envoy is an edge and service proxy that is open source and free that aids in separating network concerns from core applications. Applications don&amp;#x27;t care about the network topology; they just transmit and receive messages to and from localhost. Envoy is fundamentally a network proxy that operates at the OSI model&amp;#x27;s L3 and L4 layers. It operates by processing connections through a series of pluggable network filters. Envoy additionally provides support for an extra L7 layer filter for HTTP-based traffic. Envoy also offers excellent support for the HTTP/2 and gRPC transports. &lt;/p&gt;&lt;p&gt; Many of the features provided by Istio such as security, traffic control, network resiliency are possible due to Envoy. &lt;/p&gt;&lt;h4&gt; Istiod &lt;/h4&gt;&lt;p&gt; Service discovery, configuration, and certificate management are offered by Istiod. &lt;/p&gt;&lt;p&gt; High level routing rules that govern traffic behavior are transformed into Envoy-specific configurations by Istiod and propagated to the sidecars during runtime. Any sidecar that complies with the Envoy API can use Pilot, which synthesizes platform-specific service discovery techniques into an abstract form. &lt;/p&gt;&lt;p&gt; Istio can handle discovery in a variety of settings, including Kubernetes or virtual machines. &lt;/p&gt;&lt;p&gt; To exert finer control over the traffic in your service mesh, you can ask Istiod to modify the Envoy configuration using the Traffic Management API. &lt;/p&gt;&lt;p&gt; Strong service-to-service and end-user authentication are made possible by Istiod security&amp;#x27;s integrated identity and credential management. Istio can be used to enhance unencrypted service mesh traffic. &lt;/p&gt;&lt;p&gt; Operators can enforce regulations with Istio based on service identity rather than on layer 3 or layer 4 network IDs, which are more prone to instability. Additionally, you can limit who has access to your services by using Istio&amp;#x27;s authorisation capability. &lt;/p&gt;&lt;p&gt; In order to enable secure mTLS connection in the data plane, Istiod performs the role of a Certificate Authority (CA) and issues certificates. &lt;/p&gt;&lt;h3&gt; Features &lt;/h3&gt;&lt;h4&gt; Traffic Management &lt;/h4&gt;&lt;p&gt; Performance is impacted by traffic routing, both within and across clusters, which improves deployment strategy. You can simply manage the flow of traffic and API requests between services using Istio&amp;#x27;s traffic routing rules. Istio makes it simple to configure critical activities like A/B testing, canary deployments, and staged rollouts with percentage-based traffic divides, as well as service-level attributes like circuit breakers, timeouts, and retries. &lt;/p&gt;&lt;h4&gt; Observability &lt;/h4&gt;&lt;p&gt; It becomes harder to comprehend behaviour and performance as services become more complicated. Istio produces comprehensive telemetry for each communication taking place within a service mesh. This telemetry makes service activity observable, enabling operators to maintain, optimise, and debug their applications. Even better, you can implement practically all of this instrumentation without making any changes to your applications. Operators are able to fully comprehend how the monitored services are communicating with Istio. &lt;/p&gt;&lt;p&gt; Detailed metrics, distributed traces, and complete access logs are all included in Istio&amp;#x27;s telemetry. You get complete and thorough service mesh observability with Istio. &lt;/p&gt;&lt;h4&gt; Security Capabilities &lt;/h4&gt;&lt;p&gt; Particular security requirements for microservices include defense against man-in-the-middle attacks, adaptable access rules, auditing tools, and mutual TLS. Istio comes with a comprehensive security solution that enables administrators to handle each of these problems. To safeguard your services and data, it offers strong identity, strong policy, transparent TLS encryption, and authentication, authorization, and audit (AAA) tools. &lt;/p&gt;&lt;p&gt; The security architecture used by Istio is built on security-by-default, and it aims to provide in-depth defense so you may deploy security-conscious apps even across networks with a low level of trust. &lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[The Ultimate List of Open Source Cloud-Native Tools]]></title><link>https://layer5.io/company/news/the-ultimate-list-of-open-source-cloud-native-tools</link><guid isPermaLink="false">https://layer5.io/company/news/the-ultimate-list-of-open-source-cloud-native-tools</guid><dc:creator><![CDATA[Bill Doerrfeld]]></dc:creator><pubDate>Mon, 29 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/518e9c75ae1584673318e9d69c4b367a/tools.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;div class=&quot;Newsstyle__NewsWrapper-sc-12r6uiw-0 TVDRO&quot;&gt;&lt;div class=&quot;test&quot;&gt;&lt;p&gt;There are so many great open source cloud-native tools for nearly everything you want to do.
And they’re all in one place—look no further than the Cloud Native Computing Foundation (CNCF).This Linux Foundation body has become a locus of some stellar cloud-native open source projects. The CNCF now hosts an array of helpful packages, spanning container scheduling, observability, persistent storage, container runtime and other areas.&lt;/p&gt;&lt;p&gt;Odds are the cloud-native DevOps tool you need has already been developed—it’s only a matter of finding it. In recent posts, we highlighted many CNCF tools across various areas. Below, we’ll gloss over each category from a birds-eye view. Click each headline for the full rundown, or read below for a summary of the tools in each category.&lt;/p&gt;&lt;h3&gt;Scheduling  Orchestration&lt;/h3&gt;&lt;br/&gt;Kubernetes is the most popular container scheduling tool. It can be used to automate the deployment and management of multi-cloud applications. Other scheduling and orchestration utilities from CNCF include Crossplane, Fluid, Karmada, kube-rs, Open Cluster Management and Volcano.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Observability and Analysis&lt;/h3&gt;&lt;br/&gt;Prometheus tops the list of observability and analysis tools. The platform can be used to power your monitoring and alerting systems with fine-grained metrics and excellent querying capabilities. Other CNCF tools for observability and analysis include Jaeger, Fluentd, Thanos, Cortex and OpenTelemetry.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Security and Compliance&lt;/h3&gt;&lt;br/&gt;Open Policy Agent (OPA) can be used to unify cloud-native policies across the cloud-native stack. OPA uses a common language to express authorization policies and provides a policy engine to make authorization decisions. Other CNCF projects that deliver security-as-code include The Update Framework (TUF), Falco, Notary, cert-manager and Curiefense.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;CI/CD&lt;/h3&gt;&lt;br/&gt;Argo is a suite of packages that help direct jobs on Kubernetes to aid a continuous delivery pipeline. Using Argo, developers can create multi-step custom workflows and share these workflows across a cluster. Other CI/CD tools hosted by CNCF include Flux, Brigade, Keptn, OpenGitOps and OpenKruise.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Service Mesh Tools&lt;/h3&gt;&lt;br/&gt;Linkerd is a highly performant developer-favorite service mesh comprised of a control plane to apply configurations and a data plane that deploys its own unique proxy. This proxy can apply consistent security, observability, monitoring and telemetry features across distributed microservices. Other service mesh tools from CNCF include Kuma, Open Service Mesh (OSM), &lt;a href=&quot;/projects/service-mesh-interface-conformance&quot;&gt;Service Mesh Interface (SMI) &lt;/a&gt; , &lt;a href=&quot;/meshery&quot;&gt;Meshery&lt;/a&gt; and &lt;a href=&quot;/projects/service-mesh-performance&quot;&gt;Service Mesh Performance(SMP)&lt;/a&gt;.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Service Proxies&lt;/h3&gt;&lt;br/&gt;Envoy is a service proxy commonly used within service meshes like Istio and Kuma. Envoy is intended to run alongside applications to help standardize networking and observability within large microservices networks. Other service proxy projects include Contour BFE and OpenELB.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Persistent Storage&lt;/h3&gt;&lt;br/&gt;Rook is a tool that helps automate away some of the pains of managing cloud-native persistent storage. Rook supports file, block and object storage types and can be used for programmatic storage, migration, disaster recovery, monitoring and resource management. Other cloud-native persistent storage projects include Longhorn, CubeFS, K8up, OpenEBS, ORAS, Piraeus Datastore and Vineyard.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Cloud-Native Database Tools&lt;/h3&gt;&lt;br/&gt;TiKV is a unified distributed storage layer that can process large amounts of data. The project supports a key-value API and has rapid response times. Other cloud-native database utilities include Vitess, a clustering system for horizontal scaling of MySQL and SchemaHero, a Kubernetes operator for declarative database schema management.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Container Runtime&lt;/h3&gt;&lt;br/&gt;Containerd is an industry-standard container runtime supported by most container-based systems. Originally built as part of Docker, containerd was donated to the Linux Foundation in 2015. Other notable CNCF container runtime utilities include CRI-O, Inclavare Containers and WasmEdge Runtime.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;App Definition and Build Tools&lt;/h3&gt;&lt;br/&gt;Helm is a prevalent Kubernetes package manager widely used to share a manifest of dependencies. Operators often use Helm charts to find and install third-party applications. Other notable tools which help address operational concerns of Kubernetes include Buildpacks, KubeVirt, Operator Framework and Backstage.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Cloud-Native Networking&lt;/h3&gt;&lt;br/&gt;Cilium is a tool that brings eBPF-based networking, security and observability. Cilium helps build out networking between container workloads and cross-cluster connectivity. Additional cloud-native networking utilities include Antrea, CNI-Genie, Kube-OVN, Network Service Mesh (NSM) and Submariner. There’s also the Container Network Interface (CNI), an interface specification for container networking.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Streaming and Messaging&lt;/h3&gt;&lt;br/&gt;CloudEvents offers a specification intended to help standardize the event-based communication from various event publisher systems. By having a way to describe events consistently, developers could solve interoperability issues. Other CNCF streaming and messaging projects include NATS, Pravega, Strimzi and Tremor.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Chaos Engineering&lt;/h3&gt;&lt;br/&gt;Chaos Mesh is a chaos engineering platform for Kubernetes. Using Chaos Mesh, operators can push their Kubernetes deployments to the limit with stress testing, fault injections, and other testing behaviors. You can also schedule routine tests. Other cloud-native chaos testing tools from CNCF include Litmus and ChaosBlade.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Key Management&lt;/h3&gt;&lt;br/&gt;SPIFFE is defined as a universal identity control plane for a distributed architecture. Using SPIFFE, engineers can quickly construct a standard method to identify workloads and automatically secure service-to-service communication. SPIRE is the product-ready reference implementation of SPIFFE. Other key management tools from CNCF include Athenz and Teller.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;Edge Computing and Bare Metal&lt;/h3&gt;&lt;br/&gt;KubeEdge helps extend cloud-native capabilities into edge computing. It’s specifically designed with the constraints of edge nodes in mind, such as reliability and resource limitations. Other projects that help extend Kubernetes to the edge include Akri, OpenYurt and SuperEdge. Other tools aid in provisioning K8s on bare metal, such as Metal3.io and Tinkerbell.&lt;br/&gt; &lt;br/&gt;&lt;h3&gt;The Forecast Looks Cloud-Native&lt;/h3&gt;&lt;br/&gt;By 2023, the &lt;a href=&quot;https://containerjournal.com/features/majority-of-apps-will-use-cloud-native-development-by-2023/&quot;&gt;majority of applications will be cloud-native&lt;/a&gt;. The cloud-native world is here to stay, and open source is the foundation to support our new era of microservices, containerization and DevOps.&lt;p&gt;Although you could technically self-host your way through development and operations using these open source projects, organizations will most likely adopt a blend of open source, proprietary and as-a-service cloud offerings to get the job done. Regardless, it’s cool to know what’s available for free use.&lt;/p&gt;&lt;p&gt;It should also be mentioned that tools change from time to time. As such, you can always view the up-to-date CNCF landscape here. Stay tuned as we keep an eye on CNCF and related bodies that continue to carry the cloud-native torch forward!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[DevOps Adoption: Identifying the Right Metrics]]></title><link>https://layer5.io/resources/devops/devops-adoption-identifying-the-right-metrics</link><guid isPermaLink="false">https://layer5.io/resources/devops/devops-adoption-identifying-the-right-metrics</guid><pubDate>Tue, 23 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/a2819b9ad130794ff7e748bcf7537899/devops-adoption.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;p&gt;According to Puppet’s State of DevOps Report 2021, 83% of IT professionals report that their organizations have previously implemented DevOps practices or are doing so right now to unlock higher business value, achieve faster time to delivery, and boost security of systems.&lt;/p&gt;&lt;p&gt;However, DevOps teams from many industries frequently struggle to identify the right metrics to monitor and measure success. In this &lt;a href=&quot;/static/devops-adoption-choosing-the-right-metrics-a064d95e7d0e37af18817d28b58ef4ff.pdf&quot;&gt;infographic&lt;/a&gt;, we highlight the metrics all DevOps professionals should measure to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Identify places in the pipeline to speed up deployments.&lt;/li&gt;&lt;li&gt;Make data-driven decisions to improve the deployment process.&lt;/li&gt;&lt;li&gt;Analyze the speed at which products are reaching the market in comparison to competitors.&lt;/li&gt;&lt;/ul&gt;&lt;h3 style=&quot;margin-top:1rem&quot;&gt;Monitor these 5 metrics to understand how to speed up your DevOps toolchain:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Deployment Time&lt;/li&gt;&lt;li&gt;Change Failure Rate&lt;/li&gt;&lt;li&gt;Recovery Time&lt;/li&gt;&lt;li&gt;Release Cadence&lt;/li&gt;&lt;li&gt;Lead Time&lt;/li&gt;&lt;/ul&gt;&lt;a href=&quot;/static/devops-adoption-choosing-the-right-metrics-a064d95e7d0e37af18817d28b58ef4ff.pdf&quot;&gt;&lt;img src=&quot;static/devops-adoption-c302fe6ec5ec33186ca65d846ae9fd4e.png&quot; alt=&quot;Right metrics for adopting DevOps&quot;/&gt;&lt;/a&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[What is GitOps?]]></title><link>https://layer5.io/resources/cloud-native/what-is-gitops</link><guid isPermaLink="false">https://layer5.io/resources/cloud-native/what-is-gitops</guid><pubDate>Tue, 16 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/a8d747801f0e266dbc9bb2b192cd3dc1/github-dark.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;p&gt;GitOps revolves around the central notion that infrastructure can be treated as code. It is an operational framework that incorporates DevOps best practices for infrastructure automation, including version control, collaboration, compliance, and CI/CD tooling, which are often used for application development. Like code, not only can you store your infrastructure configuration in a source code version system, but you can also take your infrastructure configuration and any changes to its configuration through the same change management process that you do when updating your applications and services. In part, GitOps is about change management, and consequently, it is about risk reduction and risk management. When you automate a process and classify the manner in which you systemize the process, risk is reduced through the consistency and series of processes and reviews changes go through.&lt;/p&gt;&lt;p&gt;GitOps is the acknowledgement that declarative systems that everything is (or should be) defined as code. With all code in a source code system, that system becomes the source of truth and in the system of record for how your infrastructure is running. Well, that is, assuming that your infrastructure configuration hasn&amp;#x27;t drifted from its desired state defined in your source code system. If Git is the source of truth, you cannot run operations manually by executing random commands. Doing so would mean that Git would stop being the only source of truth. Instead, the only goal of operations is to define the desired state as code and store it in git. Then, let the machines synchronize that with the actual state. Such synchronization must be continuous so that the two states are (almost) always in sync. In other words, GitOps is about defining everything as code, storing that code in Git, and letting the machines detect the drift between the desired and the actual state – and making sure that drifts are resolved as soon as possible, hence resulting in the two states being almost always in sync.&lt;/p&gt;&lt;h2&gt; Principles of GitOps&lt;/h2&gt;&lt;h3&gt; 1) Declarative&lt;/h3&gt;&lt;p&gt; According to this principle, the entire system should have a declarative description. Let us first understand what a system description is. What is committed to your Git repository is called the System Description. One or more files that define each system component and its state will be included in the system description. According to GitOps, the way in which we store those definitions is crucial, and we must do so declaratively. That implies that the description of our system will be saved as data.&lt;/p&gt;&lt;p&gt; In the declarative approach, we specify how we want the system to look not how we can achieve that state. If we want to make any changes, we change the description instead of the series of steps to get there. Declarative configuration is critical for GitOps because it provides a description of the system that an automated agent can understand and utilize to take action.&lt;/p&gt;&lt;h3&gt; 2) Single Source of Truth&lt;/h3&gt;&lt;p&gt; The second principle mandates that we keep that system description inside of Git. Therefore, we decide to maintain the official blueprints, which outline the ideal system state version in Git. A git commit is required if we wish to modify the blueprint. The blueprint can also be called the desired state. This helps developers, testers, operations, security, and automations to have a single reference and keep uniformity in everyone’s vision.&lt;/p&gt;&lt;p&gt; GitOps also improves a system&amp;#x27;s ability to recover from failure because it&amp;#x27;s simple to roll back an unsuccessful change or restore the entire system from the repository.&lt;/p&gt;&lt;h3&gt; 3) Automated Change Delivery&lt;/h3&gt;&lt;p&gt; Only automation allows us to apply modifications made to the blueprint to systems already in operation. Delivery of changes is entirely automatic. GitOps doesn&amp;#x27;t allow manual editing. Because standard workflows only need GitHub, which is such a well-known platform, automation enables changes to be delivered through simpler for developers to use workflows. Additionally, automation standardizes your delivery processes, improving the predictability and consistency of system operations.&lt;/p&gt;&lt;h3&gt; 4) Automated State Control&lt;/h3&gt;&lt;p&gt; The fourth principle uses automation to keep our operating system in alignment with the desired state. Drift is the deviation of the runtime state of our system from the desired state. The system&amp;#x27;s blueprints and what is actually operating in the system don&amp;#x27;t match. Therefore, if the operating system drifts from what we have specified in Git, an operator will restore it by bringing it back to the intended condition.&lt;/p&gt;&lt;h2&gt; Benefits of GitOps&lt;/h2&gt;&lt;h3&gt; 1) Improves compliance and security:&lt;/h3&gt;&lt;p&gt; Since teams use a single platform for infrastructure management, a streamlined toolchain reduces attack surfaces. Teams can use the version control system to roll back to a desired state in the event of an assault. GitOps lessens outages and downtime as a result, allowing teams to continue working on projects in a secure environment.&lt;/p&gt;&lt;h3&gt; 2) Boosts productivity and cooperation:&lt;/h3&gt;&lt;p&gt; GitOps includes CI/CD pipelines, Git workflows, and infrastructure as code best practices for software development. These prerequisite tools, knowledge, and skill sets are already present in operations teams, thus adopting GitOps won&amp;#x27;t need a steep learning curve. GitOps workflows streamline procedures in order to improve visibility, establish a single source of truth, and have a small number of tools on hand.&lt;/p&gt;&lt;h3&gt; 3) Automation enhances developer efficiency and lowers costs:&lt;/h3&gt;&lt;p&gt; Productivity rises with CI/CD tooling and continuous deployment since teams can concentrate on development rather than laboriously manual processes thanks to automation. Since team members can use any language and tools they like before pushing updates to GitHub, GitOps workflows enhance the developer experience. Infrastructure automation increases output and decreases downtime while enabling better cloud resource management, which can also save costs.&lt;/p&gt;&lt;h3&gt; 4) Increases stability and reliability:&lt;/h3&gt;&lt;p&gt; Human mistake is decreased through infrastructure that is codified and repeatable. Code reviews and collaboration are made easier by merge requests, which also assist teams in finding and fixing issues before they are released to the public. Additionally, there is less risk because all infrastructure changes are tracked through merge requests and may be undone if an iteration is unsuccessful. By allowing rollbacks to a more stable state and providing distributed backup copies in the event of a significant outage, Git processes speed up recovery time. GitOps gives teams the freedom to iterate more quickly and release new features without worrying about creating an unstable environment.&lt;/p&gt;&lt;h3&gt; 5) Faster development and deployment:&lt;/h3&gt;&lt;p&gt; GitOps provides quicker and more frequent deployments, making it easier for teams to make a minimum viable change. Teams can ship many times per day and roll back changes if there is a problem by utilizing GitOps best practices. Team members can offer business and customer value more quickly thanks to high velocity deployments. Teams are more flexible and able to react to customer needs more quickly with continuous integration.&lt;/p&gt;&lt;h2&gt; Key Components of a GitOps workflow&lt;/h2&gt;&lt;p&gt; To summarize, the following are the four components we require to a GitOps workflow:&lt;/p&gt;&lt;ol&gt;&lt;li&gt; Git repository: The code and configuration of the application are verified there. &lt;/li&gt;&lt;li&gt; CD pipeline: It is responsible for building, testing, and deploying the application. &lt;/li&gt;&lt;li&gt; Application deployment tool: It is employed to manage the target environment&amp;#x27;s application resources. &lt;/li&gt;&lt;li&gt; Monitoring system: It keeps tabs on the performance of the application and gives the development team feedback. &lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Service Mesh: Consul]]></title><description><![CDATA[Explanation of Consul Connect]]></description><link>https://layer5.io/resources/service-mesh/service-mesh-consul</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/service-mesh-consul</guid><dc:creator><![CDATA[Deepesha Burse]]></dc:creator><pubDate>Fri, 05 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/ed21c2c53f2c64e86b016cfdfe7018ae/consul.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;h3&gt; What is a Service Mesh? &lt;/h3&gt;&lt;p&gt; A service mesh is a dedicated layer that provides secure service-to-service communication for on-prem, cloud, or multi-cloud infrastructure. Although service meshes are typically used with a microservice architectural pattern, they are useful in any situation involving complex networking. Their functionalities include traffic control, resiliency, observability and security. Traffic steering is used for content and it allows optimal usage of our resources. Service meshes provide control over chaotic situations (which usually arise in complex networks) along with proper identification and policies to enhance security. &lt;/p&gt;&lt;p&gt; Service meshes can be divided into the control plane and the data plane. The role of the control plane is to secure the mesh, facilitate service discovery, conduct frequent health checks, enforce policies and other operational concerns. Service discovery refers to a central registry of the services and their respective IP addresses. The application needs to be registered on the control plane for it to be able to share with other services how to communicate with it and helps to enforce rules on which service gets to communicate with which other services. &lt;/p&gt;&lt;p&gt; The data plane, on the other hand, handles the communication between services. The amount of knowledge that the services need to have about the network environment is limited by the fact that many service mesh solutions use a sidecar proxy to conduct data plane connections. &lt;/p&gt;&lt;img src=&quot;static/service-mesh-609aa147db2609960150f75fb05ab088.svg&quot; class=&quot;image-center&quot; alt=&quot;Service Mesh&quot;/&gt;&lt;h3&gt; What is Consul? &lt;/h3&gt;&lt;p&gt; Consul Service Mesh (also known as Consul Connect) provides service-to-service connection authorization and encryption using mutual Transport Layer Security (TLS). Consul is the control plane of the service mesh. Consul can be used with Virtual Machines (VMs), containers, or with container orchestration platforms such as Nomad and Kubernetes. Applications can use sidecar proxies to establish TLS connections for inbound and outbound connections or natively integrate with Connect by using Connect aware SDKs for optimal performance and security. &lt;/p&gt;&lt;p&gt; It is a multi-networking tool that provides a fully functional service mesh solution to address the networking and security issues associated with running cloud infrastructure and microservices. Consul offers a software technique for segmentation and routing. It also offers advantages such as handling failures, retries, and network observability. You can utilize any of these characteristics alone as required or combine them to create a full service mesh and achieve zero trust security. &lt;/p&gt;&lt;h3&gt; Architecture &lt;/h3&gt;&lt;p&gt; Consul is a distributed system built for a node cluster to operate on. A physical server, cloud instance, virtual machine, or container can all function as a Consul node. The collection of interconnected nodes that Consul runs on is known as a datacenter. Consul supports multiple datacenters and considers this as a common case. It is expected that there will be many clients and three to five servers in a datacenter. This creates a balance between performance and availability in the event of a breakdown because consensus slows down as more machines are added. The number of clients, however, is unlimited and can easily increase to thousands or tens of thousands. &lt;/p&gt;&lt;img src=&quot;static/datacenter-architecture-37152141459a591a624d6878e71817de.png&quot; class=&quot;image-center&quot; alt=&quot;Image of datacenter&quot;/&gt;&lt;p&gt; The Consul Agent is responsible for maintaining membership information, registering services, running checks, responding to queries, etc. It is required to run on every node that is a part of the Consul cluster. In some places, client agents may cache data from the servers to make it available locally for performance and reliability. They can either run in server mode or client mode. Client nodes make up for most of the cluster and are lightweight processes. They act as an interface between server nodes for most operations. They run on every node where services are running. &lt;/p&gt;&lt;p&gt; Along with core agent operations, a server node participates in the consensus quorum. The Raft protocol, which offers excellent consistency and availability in the event of failure, serves as the foundation for the quorum. Because they consume more resources than client nodes, server nodes should run on dedicated instances. &lt;/p&gt;&lt;img src=&quot;static/consul-agent-architecture-089f5381fe7bad46ed0bad733b454cc4.png&quot; class=&quot;image-center&quot; alt=&quot;Consul Agent&quot;/&gt;&lt;p&gt; A per-service proxy sidecar manages incoming and outgoing service connections by automatically wrapping and verifying TLS connections. Consul includes its own built-in L4 proxy and has first class support for Envoy. Other than this, we can choose to use any other proxy to plug in as well. The following diagram shows how proxies work: &lt;/p&gt;&lt;img src=&quot;static/service-proxy-architecture-a6b5364641023e65373a238beaf79ca0.png&quot; class=&quot;image-center&quot; alt=&quot;Side-car proxy&quot;/&gt;&lt;p&gt; The lifecycle of a Consul cluster:&lt;ol&gt;&lt;li&gt; An agent is started. &lt;/li&gt;&lt;li&gt; An agent joins the cluster. &lt;/li&gt;&lt;li&gt; Information of the agent is communicated throughout the cluster&lt;/li&gt;&lt;li&gt; Existing servers will begin replicating to the new node. &lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;&lt;h3&gt; Benefits and Compatibility of Consul Connect &lt;/h3&gt;&lt;p&gt; New methods of networking are necessary due to the development of cloud infrastructure and microservices designs. There are numerous tools and companies, all of which make different attempts to address the issue. The Consul service mesh solution offers a pure software approach with an emphasis on simplicity and wide compatibility and makes no assumptions about the underlying network. &lt;/p&gt;&lt;p&gt; Consul service mesh streamlines application deployment into a zero-trust network and makes service discovery easier in complex networking situations. &lt;/p&gt;&lt;p&gt; Features of Consul Service Mesh:&lt;br/&gt;&lt;ol&gt;&lt;li&gt; Service Discovery&lt;p&gt; Consul provides a service catalog, configurable service routing, health checks, automatic load balancing, and geo-failover across multiple instances of the same service. The capacity to control changes in the service landscape of your network becomes essential when new versions of a service are introduced and must coexist with existing instances of the same application, frequently running on different versions. The agent provides a simple service definition format to declare the availability of a service and to potentially associate it with a health check. &lt;/p&gt;&lt;/li&gt;&lt;li&gt; Zero-trust Security Model&lt;p&gt; Trust can be exploited and with the increasing number of services, there are higher chances of breach. The Consul service mesh control plane can be configured to enforce mutual TLS (mTLS), and will automatically generate and distribute the TLS certificates for every service in the mesh. The certificates are used for both service identity verification and communication encryption. &lt;/p&gt;&lt;/li&gt;&lt;li&gt; Simplify Application Security with Intentions&lt;p&gt; Communication between services is secure within the mesh once the service sidecar proxies have been set up. To designate which services are permitted to communicate with one another, you might want to build a more granular set of policies. Consul Intentions are used to limit which services can make requests or create connections and define access control for services through Connect. We can manage intentions via the UI, CLI, or API. The proxy or a natively integrated application enforces intentions on inbound connections or requests. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;&lt;p&gt; Compatibility of Consul Connect:&lt;br/&gt;&lt;ol&gt;&lt;li&gt; First-Class Kubernetes Support&lt;p&gt; By offering an official Helm chart for installing, configuring, and upgrading Consul on Kubernetes, Consul enables first-class Kubernetes support. The chart automates Kubernetes&amp;#x27;s Consul service mesh installation and configuration. &lt;/p&gt;&lt;/li&gt;&lt;li&gt; Platform Agnostic and Multi-Cluster Mesh&lt;p&gt; Consul works with all cloud providers and architectures. You can expand the scope of your Kubernetes clusters to include services that aren&amp;#x27;t run using Kubernetes by using the service catalog sync and auto-join features. In order to facilitate safe service-to-service communication between Nomad tasks and jobs, Consul additionally interfaces with HashiCorp Nomad. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Multi-Cluster Kubernetes Management with Meshery]]></title><description><![CDATA[Manage all of your Kubernetes clusters with the cloud native management plane, Meshery. Learn how Meshery makes connecting, discovering, and configuring multiple clusters a breeze.]]></description><link>https://layer5.io/blog/meshery/multi-cluster-kubernetes-management-with-meshery</link><guid isPermaLink="false">https://layer5.io/blog/meshery/multi-cluster-kubernetes-management-with-meshery</guid><dc:creator><![CDATA[Ashish Tiwari]]></dc:creator><pubDate>Thu, 28 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/322f708c880bf8749f760d3c3cad9a64/multi-cluster-kubernetes-management-with-meshery.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 gNYEES&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;From multi-mesh to now multi-cluster, &lt;a href=&quot;/meshery&quot;&gt;Meshery&lt;/a&gt; is continuously expanding its capability to give developers, operators, and security engineers more control over their infrastructure. In this post, we&amp;#x27;ll take a look behind the scenes at how each component in Meshery&amp;#x27;s architecture plays a role in the management of many Kubernetes clusters.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;Philosophy behind Meshery&amp;#x27;s multi-cluster management approach&lt;/h2&gt;&lt;p&gt;While designing Meshery for the world of many service meshes, and many Kubernetes clusters, much care has been taken to ensure that Meshery is an &lt;a href=&quot;https://docs.meshery.io/extensibility&quot;&gt;extensible management platform&lt;/a&gt;, ready for handling new types of infrastructure and new use cases rapidly through its plugin model. Under the hood, &lt;a href=&quot;https://docs.meshery.io&quot;&gt;Meshery Server&lt;/a&gt; acts as a delegator of operations by figuring out which Meshery Adapter registered its capability against the given operation. The operation is then sent to that given component (like one of Meshery’s service mesh adapters,e.g. Istio adapter) via a gRPC call. &lt;img src=&quot;static/meshery-core-architecture-ec315a3f425831b199604a1b7fc15362.png&quot; alt=&quot;deploy modal&quot; class=&quot;image-right&quot;/&gt; When the operation involves a Kubernetes cluster(s), the kubeconfig(s) is sent as a parameter to the RPCs. It is then the job of the handling adapter to respect that and perform the operation across the passed clusters from kubeconfigs as needed. The operations not requiring a kubeconfig are managed through the same RPC, with the only difference being that the handling component would ignore the &lt;code&gt;kubeconfigs&lt;/code&gt; field altogether making the system work not just for Kubernetes, but for other cloud native use cases. This approach of reusing the same RPC for different types of requests is pretty common and sometimes debatable with the other approach of being strict with the RPCs. This is what makes Meshery completely pluggable and extensible.&lt;/p&gt;&lt;h2&gt;Using multi cluster with Meshery&lt;/h2&gt;&lt;p&gt;From a client&amp;#x27;s perspective, there are two uses of the multi context feature in general. While deploying a &lt;a href=&quot;/meshmap&quot;&gt;MeshMap&lt;/a&gt; design or performing any other operation on their cluster(s), selecting any number of Kubernetes contexts will allow them to uniformly and parallely perform the operation across the clusters. And while visualizing the state of their cluster(s), the same context switcher will allow them to filter across the clusters whose view they want to see.&lt;/p&gt;&lt;p&gt;All cluster specific operations are now applied over a number of clusters uniformly. So if you have 10 clusters to manage and 8 of those start with the exact same set of pods, deployments, service mesh, etc then &lt;a href=&quot;/meshery&quot;&gt;Meshery&lt;/a&gt; can help you to apply these operations quickly and easily.&lt;/p&gt;&lt;p&gt;It is as simple as selecting the specific cluster(s) from the Kubernetes context switcher in the navbar, and then applying whatever operation you wanted to, whether that be deploying a sample app, a service mesh, or a &lt;a href=&quot;/meshmap&quot;&gt;MeshMap&lt;/a&gt; design.&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;static/context-switcher-85c34e8a44f40a03522e0b02b5689d68.png&quot; alt=&quot;context switcher&quot; class=&quot;image-center-shadow&quot;/&gt;&lt;/p&gt;&lt;p&gt;Just before applying the operation, you will be prompted with a confirmation modal which will provide the information about which cluster(s) that operation will be performed against. As the User interface improves, this same modal will also convey more useful information about the operation they are going to perform.&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;static/deploy-modal-19f06d7c3eb0bfa27b2b8a26c563f83d.png&quot; alt=&quot;deploy modal&quot; class=&quot;image-center-shadow&quot;/&gt;&lt;/p&gt;&lt;br/&gt;&lt;h3&gt;Using MeshMap visualizer&lt;/h3&gt;&lt;p&gt;You can switch between views of your cluster in visualizer mode while using &lt;a href=&quot;/meshmap&quot;&gt;MeshMap&lt;/a&gt;.&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;static/meshmap-cluster2-b545db5f3f28f44fb8154d6aab13d867.png&quot; alt=&quot;visualizer showing data of context1&quot; class=&quot;slides-right&quot;/&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;static/meshmap-cluster1-89fa20ad5fb543da3ded5afc8f57775f.png&quot; alt=&quot;visualizer showing data of context2&quot; class=&quot;slides-left&quot;/&gt;&lt;/p&gt;&lt;h3&gt;Managing Meshery on multiple clusters&lt;/h3&gt;&lt;p&gt;Users can perform cluster related operations from the settings page like adding more clusters, removing data from existing clusters and removing existing clusters.&lt;/p&gt;&lt;img src=&quot;static/settings-3b1ff1da147a0207419aafc162955ca1.png&quot; alt=&quot;Settings page&quot; class=&quot;slides-right&quot;/&gt;&lt;p&gt;Meshery also deploys Meshery operator across the cluster it’s about to manage. This operator manages the lifecycle of a Meshery broker and MeshSync. MeshSync pumps the blood into Meshery’s core, in other words, it is responsible for watching all different types of resources by establishing a watch stream over each of them. MeshSync then pumps that data into the NATS server, of which Meshery server itself is a client. From there, Meshery server gets all the relevant data related to activities in the cluster.&lt;/p&gt;&lt;p&gt;By default, &lt;a href=&quot;/meshery&quot;&gt;Meshery&lt;/a&gt; wants to be as much aware about your infrastructure as possible to provide value so it deploys its operator across each detected cluster. But you can fine tune this configuration by going over each one of them from the table as shown.&lt;/p&gt;&lt;img src=&quot;static/cluster-mgmt-c9a54f4b6037ccb065b5e8993f49a895.png&quot; alt=&quot;Kubernetes multi-cluster management with Meshery&quot; class=&quot;image-center-shadow&quot;/&gt;&lt;p&gt;If you disconnect your cluster and do not want to persist the data from that cluster then you can perform a fine-grained deletion by deleting all MeshSync data (which are the Kubernetes objects) for that specific cluster.&lt;/p&gt;&lt;img src=&quot;static/flush-meshsync-417e908da2cf120f8936e90a43a07dbf.png&quot; alt=&quot;flushing MeshSync data&quot; class=&quot;image-center-shadow&quot;/&gt;&lt;h2&gt;Future of multi-cluster&lt;/h2&gt;&lt;p&gt;Meshery as an extension point to your infrastructure provides out-of-the-box value by adding components which can be Kubernetes specific, service mesh specific or custom components to add new functionality. We can now add multi-cluster specific components to provide more abstraction. This model can be used along with Meshery’s multi-mesh capabilities to give an overall multi-mesh multi-cluster experience to the user. For instance, your Istio service mesh spanning across multiple clusters can be abstracted and managed by Meshery using custom components such as VirtualGateway and VirtualDestinationRules. In this case, Meshery’s Istio adapter will handle the logic of converting a VirtualGateway into gateways across the clusters. This abstraction provides high value by powering the service mesh to span across the clusters while the Ops team can configure the mesh with minimal effort.&lt;/p&gt;&lt;p&gt;Just like the example above, many such Meshery extension points are in Meshery to add logic into and add useful functionality. And as more of such extension points are added, Meshery will continue to give more and more power to your cloud native infrastructure.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[How to deploy Meshery on AKS]]></title><description><![CDATA[How to deploy Meshery on Azure Kubernetes service(AKS).]]></description><link>https://layer5.io/blog/meshery/how-to-deploy-meshery-on-aks</link><guid isPermaLink="false">https://layer5.io/blog/meshery/how-to-deploy-meshery-on-aks</guid><dc:creator><![CDATA[Srinivas Karnati]]></dc:creator><pubDate>Thu, 21 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/4f9edd5fbd1229cce133102a7fa8d084/Meshery-on-AKS.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 gNYEES&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://meshery.io/&quot;&gt;Meshery&lt;/a&gt;&amp;#x27;s goal is to make the operation of cloud native infrastructure and the service mesh layer of cloud simplified. Originally created by Layer5, Meshery is an open source project with hundreds of contributors world-wide and is actively maintained by engineers from Red Hat, VMware, Intel, Layer5 and others.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;Setup and run Meshery on AKS&lt;/h2&gt;&lt;p&gt;The following instructions expects you to have an active Azure subscription, and Azure CLI installed on your system. &lt;/p&gt;&lt;h3&gt; Spin up the AKS Cluster&lt;/h3&gt;&lt;p&gt;Create the resource group (a logical group where all our resources will be deployed). The following command creates  a resource group named MesheryGroup in &lt;code&gt;southindia&lt;/code&gt; location. &lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;az group create --name MesheryGroup --location southindia&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;Create AKS cluster using &lt;code&gt;az aks create&lt;/code&gt;. The following command creates aks cluster with a single node. &lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;az aks create --resource-group MesheryGroup --name MesheryAKS --node-count &lt;/span&gt;&lt;span class=&quot;token number&quot; style=&quot;color:rgb(247, 140, 108)&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; --generate-ssh-keys&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;After a few minutes, the command completes and returns a JSON formatted information about the cluster.&lt;/p&gt;&lt;p&gt;You can connect with your cluster by using &lt;code&gt;az aks get-credentials&lt;/code&gt; ,  which basically downloads credentials and configure the Kubernetes CLI. &lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;az aks get-credentials --resource-group MesheryGroup --name MesheryAKS&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;Verify the connection to your cluster using the &lt;code&gt;kubectl get command&lt;/code&gt;. &lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;$kubectl get nodes&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h3&gt;Install Meshery into your AKS cluster&lt;/h3&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;helm repo add meshery https://meshery.io/charts/&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;helm install meshery meshery/meshery --namespace meshery --create-namespace&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;Meshery server supports customizing authentication flow callback URL, which can be configured in the following way.&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;helm install meshery meshery/meshery --namespace meshery --set env.MESHERY_SERVER_CALLBACK_URL=https://custom-host --create-namespace&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;Port forward to Meshery UI&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;export POD_NAME=$(kubectl get pods --namespace meshery -l &amp;quot;app.kubernetes.io/name=meshery,app.kubernetes.io/instance=meshery&amp;quot; -o jsonpath=&amp;quot;{.items[0].metadata.name}&amp;quot;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;$ kubectl --namespace meshery port-forward $POD_NAME 9081:8080&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;Meshery should now be running in your AKS cluster and the Meshery UI should be accessible at the specified endpoint you’ve exposed to. Navigate to the meshery service endpoint to log into Meshery.&lt;/p&gt;&lt;div&gt;&lt;img src=&quot;static/mesheryui-72711ded6ef62dddc649eba788fb6c88.png&quot; class=&quot;image-center&quot; alt=&quot;Meshery UI Dashboard&quot;/&gt;&lt;/div&gt;&lt;p&gt;From here, your Meshery deployment on AKS is ready to use. In order to login to Meshery, authenticate with your chosen provider from the list.&lt;/p&gt;&lt;p&gt;There are different ways to configure a Meshery on AKS. Join the &lt;a href=&quot;https://layer5.io/community&quot;&gt;community&lt;/a&gt; and share your deployment’s configuration on the &lt;a href=&quot;https://discuss.layer5.io/&quot;&gt; discussion forum &lt;/a&gt;today! &lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Managing Containers]]></title><link>https://layer5.io/resources/kubernetes/managing-containers</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/managing-containers</guid><pubDate>Wed, 06 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/70f4c7f444e8b3494ddc0fb955f86d40/docker.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Learn more about managing containers with our &lt;a class=&quot;blog&quot; href=&quot;https://github.com/layer5io/containers-101-workshop&quot;&gt;Containers 101 Workshop&lt;/a&gt;. Walk-through four hands-on exercises with Docker.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Container management refers to a set of practices that govern and maintain containerization software. Container management tools automate the creation, deployment, destruction and scaling of application or systems containers. Containerization is an approach to software development that isolates processes that share an OS kernel -- unlike virtual machines (VMs), which require their own -- and binds application libraries and dependencies into one deployable unit. This makes containers lightweight to run, as they require only the application configuration information and code from the host OS. This design also increases interoperability compared to VM hosting. Each container instance can scale independently with demand.&lt;/p&gt;&lt;p&gt;Modern Linux container technology was popularized by the Docker project, which started in 2013. Interest soon expanded beyond containerization itself, to the intricacies of how to effectively and efficiently deploy and manage containers.&lt;/p&gt;&lt;p&gt;In 2015, Google introduced the container orchestration platform Kubernetes, which was based on its internal data center management software called Borg. At its most basic level, open source Kubernetes automates the process of running, scheduling, scaling and managing a group of Linux containers. With more stable releases throughout 2017 and 2018, Kubernetes rapidly attracted industry adoption, and today it is the de facto container management technology.&lt;/p&gt;&lt;p&gt;IT teams use containers for cloud-native, distributed -- often microservices- based -- applications, and to package legacy applications for increased portability and efficient deployment. Containers have surged in popularity as IT organizations embrace DevOps, which emphasizes rapid application deployment. Organizations can containerize application code from development through test and deployment.&lt;/p&gt;&lt;h2&gt;Benefits of container management&lt;/h2&gt;&lt;p&gt;The chief benefit of container management is simplified management for clusters of container hosts. IT admins and developers can start, stop and restart containers, as well as release updates or check health status, among other actions. Container management includes orchestration and schedulers, security tools, storage, and virtual network management systems and monitoring.&lt;/p&gt;&lt;h3&gt;Wrangling container sprawl&lt;/h3&gt;&lt;p&gt;Organizations can set policies that ensure containers share a host -- or cannot share a host -- based on application design and resource requirements For example, IT admins should colocate containers that communicate heavily to avoid latency. Or, containers with large resource requirements might require an anti-affinity rule to avoid physical storage overload. Container instances can spin up to meet demand -- then shut down -- frequently. Containers also must communicate for distributed applications to work, without opening an attack surface to hackers.&lt;/p&gt;&lt;p&gt;A container management ecosystem automates orchestration, log management, monitoring, networking, load balancing, testing and secrets management, along with other processes. Automation enables IT organizations to manage large containerized environments that are too vast for a human operator to keep up with.&lt;/p&gt;&lt;h2&gt;Challenges of container management&lt;/h2&gt;&lt;p&gt;One drawback to container management is its complexity, particularly as it relates to open source container orchestration platforms such as Kubernetes and Apache Mesos. The installation and setup for container orchestration tools can be arduous and error prone. IT operations staff need container management skills and training. It is crucial, for example, to understand the relationships between clusters of host servers as well as how the container network corresponds to applications and dependencies.&lt;/p&gt;&lt;p&gt;Issues of persistence and storage present significant container management challenges. Containers are ephemeral -- designed to exist only when needed. Stateful application activities are difficult because any data produced within a container ceases to exist when the container spins down.&lt;/p&gt;&lt;p&gt;Container security is another concern. Container orchestrators have several components, including an API server and monitoring and management tools. These pieces make it a major attack vector for hackers. Container management system vulnerabilities mirror standard types of OS vulnerabilities, such as those related to access and authorization, images and intercontainer network traffic. Organizations should minimize risk with security best practices -- for example, identify trusted image sources and close network connections unless they&amp;#x27;re needed.&lt;/p&gt;&lt;h2&gt;Container management strategy&lt;/h2&gt;&lt;p&gt;Forward-thinking enterprise IT organizations and startups alike use containers and container management tools to quickly deploy and update applications. IT organizations must first implement the correct infrastructure setup for containers, with a solid grasp of the scope and scale of the containerization project in terms of business projections for growth and developers&amp;#x27; requirements. IT admins must also know how the existing infrastructure&amp;#x27;s pieces connect and communicate to preserve those relationships in a containerized environment. Containers can run on bare-metal servers, VMs or in the cloud -- or in a hybrid setup -- based on IT requirements.&lt;/p&gt;&lt;p&gt;In addition, the container management tool or platform should meet the project&amp;#x27;s needs for multi-tenancy; user and application isolation; authentication; resource requirements and constraints; logging, monitoring and alerts; backup management; license management; and other management tasks. IT organizations should understand their hosting commitment and future container plans, such as if the company will adopt multiple cloud platforms or a microservices architecture.&lt;/p&gt;&lt;h2&gt;Kubernetes implementation considerations&lt;/h2&gt;&lt;p&gt;As described above, containers are arranged into pods in Kubernetes, which run on clusters of nodes; pods, nodes and clusters are controlled by a master. One pod can include one or multiple containers. IT admins should carefully consider the relationships between pods, nodes and clusters when they set up Kubernetes.&lt;/p&gt;&lt;p&gt;Organizations should plan their container deployment based on how many pieces of the application can scale under load -- this depends on the application, not the deployment method. Additionally, capacity planning is vital for balanced pod-to-node mapping, and IT admins should ensure high availability with redundancy with master node components.&lt;/p&gt;&lt;p&gt;IT organizations can address container security concerns by applying some general IT security best practices to containerization. For example, create multiple security layers throughout the environment, scan all container images for vulnerabilities, enforce signed certificates and run the most up-to-date version of any container or application image. Containers introduce the benefits of an immutable infrastructure methodology as well; the regular disposal and redeployment of containers, with their associated components and dependencies, improves overall system availability and security. Additionally, Kubernetes multi-tenancy promises greater resource isolation, but recently revealed security vulnerabilities make multicluster management preferred for now.&lt;/p&gt;&lt;p&gt;Networking is another significant factor. Kubernetes networking occurs within pods, between pods and in user-to-containerized resource connections. Kubernetes enables pods and nodes to communicate without address translation, allocating subnets as necessary. Lastly, IT admins working with Kubernetes should prepare to troubleshoot common container performance problems, including those caused by unavailable nodes and noisy neighbors, in an implementation.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Kubernetes Architecture 101]]></title><link>https://layer5.io/resources/kubernetes/kubernetes-architecture-101</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/kubernetes-architecture-101</guid><pubDate>Tue, 05 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/9287b4a708bb510f64057ea305498b77/kubernetes.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;p&gt;The way Kubernetes is architected is what makes it powerful. Kubernetes has a basic client and server architecture, but it goes way beyond that. Kubernetes has the ability to do rolling updates, it also adapts to additional workloads by auto scaling nodes if it needs to and it can also self-heal in the case of a pod meltdown. These innate abilities provide developers and operations teams with a huge advantage in that your applications will have little to no down time. In this section we provide a brief overview of the master and its worker nodes with a high level overview of how Kubernetes manages workloads.&lt;/p&gt;&lt;div class=&quot;right&quot;&gt;&lt;img src=&quot;static/kubernetes-highlevel-architecture-5759d8cb4ac0991f91ef736d48e531fc.png&quot; alt=&quot;Simple Kubernetes Architecture Diagram&quot;/&gt;&lt;i&gt;Simple Kubernetes Architecture Diagram&lt;/i&gt;&lt;/div&gt;&lt;h1&gt;Kubernetes Components&lt;/h1&gt;&lt;p&gt;Let&amp;#x27;s dive into each of the Kubernetes components, starting with the Master node.&lt;/p&gt;&lt;h2&gt;Kubernetes Master&lt;/h2&gt;&lt;p&gt;The Kubernetes master is the primary control unit for the cluster. The master is responsible for managing and scheduling the workloads in addition to the networking and communications across the entire cluster. The master node is responsible for the management of Kubernetes cluster. This is the entry point of all administrative tasks. The master node is the one taking care of orchestrating the worker nodes, where the actual services are running.&lt;/p&gt;&lt;p&gt;These are the components that run on the master:&lt;/p&gt;&lt;h3&gt;Etcd Storage&lt;/h3&gt;&lt;p&gt;Etcd is an open-source key-value data store that can be accessed by all nodes in the cluster. It stores configuration data of the cluster’s state. etcd is a simple, distributed, consistent key-value store. It’s mainly used for shared configuration and service discovery.&lt;/p&gt;&lt;p&gt;It provides a REST API for CRUD operations as well as an interface to register watchers on specific nodes, which enables a reliable way to notify the rest of the cluster about configuration changes.&lt;/p&gt;&lt;p&gt;An example of data stored by Kubernetes in etcd is jobs being scheduled, created and deployed, pod/service details and state, namespaces and replication information, etc.&lt;/p&gt;&lt;h3&gt;Kube-API-Server&lt;/h3&gt;&lt;p&gt;Kube-API-Server manages requests from the worker nodes, and it receives REST requests for modifications, and serves as a front-end to control cluster. The API server is the entry points for all the REST commands used to control the cluster. It processes the REST requests, validates them, and executes the bound business logic. The result state has to be persisted somewhere, and that brings us to the next component of the master node.&lt;/p&gt;&lt;h3&gt;Kube-scheduler&lt;/h3&gt;&lt;p&gt;Kube-scheduler schedules the pods on nodes based on resource utilization and also decides where services are deployed. The deployment of configured pods and services onto the nodes happens thanks to the scheduler component. The scheduler has the information regarding resources available on the members of the cluster, as well as the ones required for the configured service to run and hence is able to decide where to deploy a specific service.&lt;/p&gt;&lt;h3&gt;Kube-controller-manager&lt;/h3&gt;&lt;p&gt;Kube-controller-manager runs a number of distinct controller processes in the background to regulate the shared state of the cluster and perform routine tasks. When there is a change to a service, the controller recognizes the change and initiates an update to bring the cluster up to the desired state. Optionally you can run different kinds of controllers inside the master node. controller-manager is a daemon embedding those.&lt;/p&gt;&lt;p&gt;A controller uses apiserver to watch the shared state of the cluster and makes corrective changes to the current state to change it to the desired one.
An example of such a controller is the Replication controller, which takes care of the number of pods in the system. The replication factor is configured by the user, and it&amp;#x27;s the controller’s responsibility to recreate a failed pod or remove an extra-scheduled one. Other examples of controllers are endpoints controller, namespace controller, and serviceaccounts controller, but we will not dive into details here.&lt;/p&gt;&lt;h2&gt;Worker Nodes&lt;/h2&gt;&lt;p&gt;These nodes run the workloads according the schedule provided by the master. The interaction between the master and worker nodes are what’s known as the control plane. The pods are run here, so the worker node contains all the necessary services to manage the networking between the containers, communicate with the master node, and assign resources to the containers scheduled.&lt;/p&gt;&lt;h3&gt;Kubelet&lt;/h3&gt;&lt;p&gt;Kubelet ensures that all containers in the node are running and are in a healthy state.  If a node fails, a replication controller observes this change and launches pods on another healthy pod. Integrated into the kubelet binary is ‘cAdvisor` that auto-discovers all containers and collects CPU, memory, file system, and network usage statistics and also provides machine usage stats by analyzing the ‘root’ container. &lt;/p&gt;&lt;p&gt;Kubelet gets the configuration of a pod from the apiserver and ensures that the described containers are up and running. This is the worker service that’s responsible for communicating with the master node. It also communicates with etcd, to get information about services and write the details about newly created ones.&lt;/p&gt;&lt;h3&gt;Kube Proxy&lt;/h3&gt;&lt;p&gt;Kube Proxy acts as a network proxy and a load balancer for a service on a single worker node. . It takes care of the network routing for TCP and UDP packets. It forwards the request to the correct pods across isolated networks in a cluster. &lt;/p&gt;&lt;h3&gt;Pods&lt;/h3&gt;&lt;p&gt;A pod is the basic building block on Kubernetes. It represents the workloads that get deployed. Pods are generally collections of related containers, but a pod may also only have one container. A pod shares network/storage and also a specification for how to run the containers.&lt;/p&gt;&lt;h3&gt;Containers&lt;/h3&gt;&lt;p&gt;Containers are the lowest level of microservice. These are placed inside of the pods and need external IP addresses to view any outside processes. Docker is not the only supported container runtime, but is by far, the most popular. Docker runs on each of the worker nodes, and runs the configured pods. It takes care of downloading the images and starting the containers.&lt;/p&gt;&lt;h3&gt;kubectl&lt;/h3&gt;&lt;p&gt;Kubectl is a command line tool to communicate with the API service and send commands to the master node. kubectl must be configured to communicate with your cluster. If you have multiple clusters, you might try using kubectx, which makes switching between contexts easy.&lt;/p&gt;&lt;h4&gt;Managing objects with kubectl&lt;/h4&gt;&lt;p&gt;You can divide a Kubernetes cluster into multiple environments by using namespaces (e.g., Dev1, Dev2, QA1, QA2, etc.), and each environment can be managed by a different user. One of the inconveniences of writing kubectl commands is that every time you write a command, you need the --namespace option at the end. People often forget this and end up creating objects (pods, services, deployments) in the wrong namespace. &lt;/p&gt;&lt;p&gt;With this trick, you can set the namespace preference before running kubectl commands. Run the following command before executing the kubectl commands, and it will save the namespace for all subsequent kubectl commands for your current context:&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;kubectl config set-context $(kubectl config current-context) --namespace=mynamespace&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[What is Multi-Cluster Kubernetes?]]></title><link>https://layer5.io/resources/kubernetes/what-is-multi-cluster-kubernetes</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/what-is-multi-cluster-kubernetes</guid><pubDate>Tue, 05 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/9287b4a708bb510f64057ea305498b77/kubernetes.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;Developers who work in fast-paced environments face the risk of infrastructure sprawl in their VMs or servers. Even with the rise in containerized deployments on Kubernetes and other platforms, admins still must determine how to efficiently manage hundreds and thousands of clusters for various projects.&lt;p&gt;Common concerns for an organization’s project deployments include how to run multiple workloads and whether a cluster is large enough to handle the work.&lt;/p&gt;&lt;p&gt;A Kubernetes multi-cluster setup can solve these problems. Multi-cluster architecture is a strategy for spinning up several clusters to achieve better isolation, availability, and scalability. In this type of implementation, an application’s infrastructure is distributed and maintained across multiple clusters. Because this strategy can also make cluster management more difficult, it needs to be handled properly.&lt;/p&gt;&lt;h2&gt;What Is a Kubernetes Multi-Cluster Setup?&lt;/h2&gt;&lt;p&gt;Kubernetes works with clusters to efficiently run and manage workloads.&lt;/p&gt;&lt;p&gt;In Kubernetes multi-cluster orchestration, platforms such as managed services help you to run workloads across multiple clusters and environments. The multiple clusters can be configured within a single physical host, within multiple hosts in the same data center, or even in a single cloud provider across different regions. This allows you to provision your workloads in several clusters, rather than just one.&lt;/p&gt;&lt;p&gt;This type of deployment enables more scalability, availability, and isolation for your workloads and environments. It also enables you to better coordinate the planning, delivery, and management of these environments.&lt;/p&gt;&lt;p&gt;A key feature of multi-cluster Kubernetes architecture is that each cluster is highly independent, managing its internal state for maximum resource provisioning and service configuration.&lt;/p&gt;&lt;h2&gt;Why Use a Kubernetes Multi-Cluster Setup?&lt;/h2&gt;&lt;p&gt;There are multiple use cases for a multi-cluster deployment. You can use it to deploy workloads spanning multiple regions for increased availability, eliminate cloud blast radius, prevent compliance issues, and enforce security around your clusters and tenants.&lt;/p&gt;&lt;p&gt;As your environment grows, so do the potential issues you need to solve in order to align your cluster maintenance with your business needs. Using a Kubernetes multi-cluster setup can help with the following concerns.&lt;/p&gt;&lt;h2&gt;Cluster Discovery and Tenant Isolation&lt;/h2&gt;&lt;p&gt;It is common for projects to exist in dev, staging, and production environments. To achieve this kind of isolation, you require multiple Kubernetes environments.&lt;/p&gt;&lt;p&gt;Conventionally, using namespaces would be enough for discovery and isolation in a single cluster, but Kubernetes isn’t a direct multitenant system. Namespaces are also not great for isolation since any compromise in the namespace means that your cluster is also compromised. Additionally, badly configured applications in a namespace can consume more resources than expected, which impacts other applications in the cluster.&lt;/p&gt;&lt;p&gt;Kubernetes multi-cluster environments enable you to isolate users and projects by cluster, simplifying the process.&lt;/p&gt;&lt;h2&gt;Failover&lt;/h2&gt;&lt;p&gt;Architecting multi-cluster workloads minimizes the downtime issues common within a single cluster, because you can freely transfer the workloads to other running clusters.&lt;/p&gt;&lt;h1&gt;Multi-Cluster, Multitenancy, or a Mix?&lt;/h1&gt;&lt;p&gt;Kubernetes is a complex, high-level platform that offers multiple options for your deployments: single server, multitenant, or multi-cluster.&lt;/p&gt;&lt;p&gt;Multitenancy means a cluster is shared among several workloads, or tenants. Multiple users share the same cluster resources and control plane. Multitenant clusters require fair allocation of resources to the tenants as well as isolation of tenants from each other, in order to minimize the effects of a faulty tenant on other tenants and the overall cluster.&lt;/p&gt;&lt;p&gt;A multi-cluster setup, on the other hand, involves several clusters deployed across one or many data centers. This type of deployment can be used to separate development and production. It improves availability and enhances security around workloads.&lt;/p&gt;&lt;p&gt;The best choice for your organization depends on factors that include the technical expertise of your team, your infrastructure availability, and your budget. Many organizations separate their critical production services from non-critical services by placing them in separate tenants across tiers, teams, locations, or infrastructure providers. Projects that are time- and resource-dependent (where resources are spun up and down on the go) are, however, suitable for multi-cluster architecture.&lt;/p&gt;&lt;h1&gt;When to Use a Multi-Cluster Setup&lt;/h1&gt;&lt;p&gt;To decide whether your projects would function best in a multi-cluster deployment, you first need to define your goals.&lt;/p&gt;&lt;p&gt;You should know the challenges you are trying to solve and how transitioning to a multi-cluster setup would help your organization. Projects that are performance-dependent with workloads that are sensitive to factors like latency can take advantage of the high availability and isolation available in multi-cluster setups. In other words, you can run workloads with intensive computations that don’t need to share resources.&lt;/p&gt;&lt;p&gt;You’ll need to collect workload data and other feedback from your various teams before making a decision. You should assess your teams’ expertise: are they well-versed in provisioning single clusters, even before transitioning to multi-clusters? You’ll also need to evaluate your business model and how such an infrastructure transition could affect your users or customers.&lt;/p&gt;&lt;p&gt;The following are some of the advantages of transitioning to a Kubernetes multi-cluster setup.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Tenant Isolation&lt;/li&gt;You might want to establish order while accommodating your development teams. The multi-cluster architecture allows workload isolation. For example, you could spin up separate clusters for staging and production.&lt;p&gt;With multiple clusters, any tenant configuration changes affect only that specific cluster. This way, cluster admins can easily identify issues, run new feature experiments, and carry out workload shifts without troubling other tenants and clusters.&lt;/p&gt;&lt;li&gt;No  Single Point of Failure&lt;/li&gt;Running a single cluster can expose your project to a single point of failure, in which one malfunctioning component can bring down an entire system. Using a multi-cluster environment enables you to shift your workloads between clusters so that your projects continue to function if one cluster is down or even disappears entirely.&lt;li&gt;No Vendor Lock-In&lt;/li&gt;There are multiple third-party cloud vendors available with varying resource offerings. Because of evolving resource pricing and models, organizations change their usage models over time as well. A Kubernetes multi-cluster setup ensures your workloads are cloud-agnostic so that you can safely use multiple vendors or move workloads from one cloud to another.&lt;/ul&gt;&lt;p&gt;Kubernetes provisions clusters that run and manage our workloads. Depending on the needs of an organization, Kubernetes deployments can be replicated to have the same workloads accessible across multiple nodes and environments. This concept is called Kubernetes multi-cluster orchestration. It’s simply provisioning your workloads in several Kubernetes clusters (going beyond a single cluster). &lt;/p&gt;&lt;p&gt;A Kubernetes multi-cluster defines deployment strategies to introduce scalability, availability, and isolation for your workloads and environments. A Kubernetes multi-cluster is fully embraced when an organization coordinates the planning, delivery, and management of several Kubernetes environments using appropriate tools and processes.&lt;/p&gt;&lt;h2&gt;Why Do You Need a Kubernetes Multi-Cluster?&lt;/h2&gt;&lt;p&gt;In simple deployment cases, Kubernetes can spin workloads in a single cluster. However, some cases need advanced deployment models, and for such scenarios, a multi-cluster architecture is suitable and can improve the performance of your workloads.&lt;/p&gt;&lt;p&gt;Simply put, a development team may need a Kubernetes multi-cluster to handle workloads spanning regions, eliminate a cloud blast radius, manage compliance requirements, solve multi-tenancy conflicts, and enforce security around clusters and tenants.&lt;/p&gt;&lt;h3&gt;Cluster Upgrades and Security Management&lt;/h3&gt;&lt;p&gt;Teams that rely heavily on Kubernetes for deployments need to plan for regular upgrades and patches on their environments for comprehensive security fixes.&lt;/p&gt;&lt;p&gt;Running cluster upgrades without due care or proper tools can break more things, and more so when dependent resources are overloaded. Tools like kOPs and Cluster APIs can therefore be used to apply upgrades to your running clusters.&lt;/p&gt;&lt;p&gt;The tools that you install to run your clusters depend entirely on the workloads that your clusters support. How you upgrade a cluster and its tools also depends on how you initially deployed and ran the Kubernetes cluster, that is, whether you’re using a hosted Kubernetes provider or some other means for deployment. Most hosted providers support and handle automatic upgrades, which relieves developers from manual upgrades and patching.&lt;/p&gt;&lt;p&gt;Upgrading a cluster and its toolset follows the approach of upgrading the control plane first, then the nodes in a cluster, followed by upgrading clients such as &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;&lt;h3&gt;Managing Kubernetes Multi-Cluster Complexity&lt;/h3&gt;&lt;p&gt;The complexity of management tasks across multiple Kubernetes clusters greatly increases your the number of clusters increase. You need higher-level view and control as you manage workloads across clusters; need to be able to simply switch between clusters; you need a management plane. &lt;/p&gt;&lt;a class=&quot;blog&quot; href=&quot;/meshery&quot;&gt;Meshery&lt;/a&gt; is the open source, cloud native management plane that enables the adoption, operation, and management of Kubernetes, any service mesh, and their workloads.&lt;p&gt;MeshSync, a custom controller managed by Meshery Operator, uniquely contains cluster-wide details of all objects across any number of managed clusters separated by Kubernetes Cluster ID. &lt;/p&gt;&lt;h3&gt;Deprovisioning Clusters That Are No Longer Needed&lt;/h3&gt;&lt;p&gt;When you deprovision a cluster, its running resources are also deleted. The control plane resources, the node instances, pods, and stored data are all deleted.&lt;/p&gt;&lt;p&gt;Different hosted Kubernetes providers have varying ways of deleting Kubernetes clusters. For instance, GKE supports deletion of clusters from the Google Cloud CLI and Cloud Console. Other tools for spinning Kubernetes clusters such as kOps and Amazon EKS also support the deletion from their CLIs and consoles.&lt;/p&gt;&lt;p&gt;Suppose you have provisioned your clusters with the Google Kubernetes Engine; you can run the following command in the gcloud CLI to deprovision your clusters that are no longer needed:&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;gcloud container clusters delete CLUSTER_NAME&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;At this point, you’ve seen the operations around managing a cluster lifecycle, that is, creation, deletion, and upgrading of clusters.&lt;/p&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Teams want working with clusters to be as easy as possible. This ease in operating clusters can be ensured by managing the cluster lifecycle. In this article, you learned what’s involved in managing a cluster lifecycle. You’ve seen how clusters are created at scale using various tools. You’ve also seen what cluster upgrades and security patch management involve while trying to maintain the health of your clusters.&lt;/p&gt;&lt;p&gt;The complexity of Kubernetes environments does present challenges, but setting clear goals and objectives for deploying your clusters can help you overcome any obstacles as your organization makes the transition.&lt;/p&gt;&lt;p&gt;Finally, multi-cluster deployments are a good choice for organizations that are building highly distributed systems, with geographic and regulatory control in check to help scale workloads beyond the limits of single clusters. Multi-cluster deployment and management is useful for minimizing exposure of production services, preventing access to sensitive data in environments like development and testing. Organizations are now opting to deploy their more critical workloads on separate multiple clusters from their less critical ones.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[TechStrong TV Interview]]></title><link>https://layer5.io/resources/interview/techstrong-tv-interview</link><guid isPermaLink="false">https://layer5.io/resources/interview/techstrong-tv-interview</guid><pubDate>Sat, 25 Jun 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/12cf27fc20e91ecb3f581a9f5b187743/techstrong.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;p&gt;TechStrong TV hosts a variety of live conversations and panel discussions with world’s leading technology experts and leaders at global tech events and user conferences. In this episode of TechStrong TV, straight out of &lt;a href=&quot;/community/events/open-source-summit-north-america-2022&quot;&gt;Open Source Summit NA 2022&lt;/a&gt; , catch guest &lt;a href=&quot;https://layer5.io/community/members/lee-calcote&quot;&gt;Lee Calcote&lt;/a&gt; from Layer5 and host Alan Shimel discuss the power of &lt;a href=&quot;/projects&quot;&gt;Layer5 projects&lt;/a&gt; in managing service meshes, Kubernetes and the rest of your cloud native infrastucture. They also dive into some of the other network-centric CNCF projects like CoreDNS and gRPC. Tune in now!&lt;/p&gt;&lt;a href=&quot;https://digitalanarchist.com/videos/open-source-summit-na-2022/lee-calcote-layer5&quot;&gt;&lt;button class=&quot;btnstyle__ButtonStyle-sc-mhxpaj-0 gmVeLL appion__btn btn-center&quot;&gt;&lt;h3&gt;Check out the TechStrong TV Interview with Layer5!&lt;/h3&gt; &lt;/button&gt;&lt;/a&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Istio Virtual Service]]></title><link>https://layer5.io/resources/service-mesh/istio-virtual-service</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/istio-virtual-service</guid><pubDate>Thu, 16 Jun 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/731763d720780a49c2ffdfede8c28f4b/istio.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Resourcesstyle__ResourcesWrapper-sc-1y33ukx-0 gHfJzF&quot;&gt;&lt;p&gt;Istio Virtual Service defines a set of traffic routing rules to apply when host is addressed. Each routing rule defines standards for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service defined in the registry.&lt;/p&gt;&lt;p&gt;The source of traffic can also be matched within a routing rule that allows routing to be customized for every specific client context.&lt;/p&gt;&lt;div class=&quot;fact-left&quot;&gt;&lt;p&gt;The below example on Kubernetes routes all HTTP traffic by default to pods of the reviews service with the label “version: v1”. Additionally, HTTP requests with path starting with /wpcatalog/ or /consumercatalog/ will be rewritten to /newcatalog and sent to the pods with label “version: v2”.&lt;/p&gt;&lt;/div&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;apiVersion: networking.istio.io/v1alpha3&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;kind: VirtualService&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;metadata:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  name: reviews-route&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;spec:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  hosts:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  - reviews.prod.svc.cluster.local&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  http:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  - name: &amp;quot;reviews-v2-routes&amp;quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    match:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    - uri:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;        prefix: &amp;quot;/wpcatalog&amp;quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    - uri:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;        prefix: &amp;quot;/consumercatalog&amp;quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    rewrite:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      uri: &amp;quot;/newcatalog&amp;quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    route:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    - destination:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;        host: reviews.prod.svc.cluster.local&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;        subset: v2&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  - name: &amp;quot;reviews-v1-route&amp;quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    route:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    - destination:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;        host: reviews.prod.svc.cluster.local&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;        subset: v1&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h2&gt;Virtual Service Configuration Affecting Traffic Routing &lt;/h2&gt;&lt;p&gt;A single Virtual Service can be used to describe all the traffic properties of the hosts, including those for multiple HTTP and TCP ports.&lt;/p&gt;&lt;div&gt;&lt;h3&gt;Hosts&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The application traffic created by hosts, clients, servers, and applications that use the network as a transport is contained in the physical network data plane (also known as the forwarding plane). As a result, data plane traffic should never have source or destination IP addresses that are assigned to network elements like routers and switches; instead, it should be originated from and delivered to end devices like PCs and servers. To forward data plane traffic as swiftly as possible, routers and switches use hardware chips called application-specific integrated circuits (ASICs). A forwarding information base is referenced by the physical networking data plane (FIB).&lt;/li&gt;&lt;li&gt;The destination hosts to which traffic is being sent it could be a DNS name with wildcard prefix or an IP address depending on the platform.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Gateways&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The names of gateways and sidecars that should apply all these routes. Gateways in other namespaces may be referred to by &lt;code&gt; gateway namespace&amp;gt;/gateway name &lt;/code&gt;; specifying a gateway with no namespace qualifier is the same as specifying the VirtualService’s namespace.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;HTTP&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;An ordered list of route rules for HTTP traffic. The HTTP routes will be applied to the platform service ports named &lt;code&gt;‘http-’/‘http2-’/‘grpc-*’, gateway ports with protocol HTTP/HTTP2/GRPC/ TLS-terminated-HTTPS &lt;/code&gt; and service entry ports using HTTP/HTTP2/GRPC protocols.&lt;/li&gt;&lt;li&gt;The first rule is matching an incoming request which is used.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;TCP&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;An ordered list of all the routing rules for opaque TCP traffic. TCP routes will be applied to any of the port which is not a HTTP or TLS port.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;ExportTo&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Exporting a virtual service allows it to be used by the sidecars and the gateways defined in other namespaces.&lt;/li&gt;&lt;li&gt;If no namespaces are specified then the virtual service is exported to all namespaces by default.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;h2&gt;Destination&lt;/h2&gt;&lt;p&gt;A destination indicates that the network addressable service to which the request/connection will be sent. A DestinationRule defines policies that apply to traffic intended for a service after routing has occurred.&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;apiVersion: networking.istio.io/v1alpha3&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;kind: DestinationRule&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;metadata:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  name: reviews-destination&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;spec:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  host: reviews.prod.svc.cluster.local&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  subsets:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  - name: v1&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    labels:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      version: v1&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  - name: v2&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    labels:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      version: v2&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;div class=&quot;fact-left&quot;&gt;&lt;p&gt;A version of the route destination is identified with a reference to a named service subset which should be declared in a corresponding DestinationRule.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Validating Meshery CLI Functionality]]></title><description><![CDATA[Guide for End to End manual testing Meshery CLI (mesheryctl)]]></description><link>https://layer5.io/blog/meshery/validating-meshery-cli-functionality</link><guid isPermaLink="false">https://layer5.io/blog/meshery/validating-meshery-cli-functionality</guid><dc:creator><![CDATA[Piyush Singariya]]></dc:creator><pubDate>Fri, 10 Jun 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/263b448473ae33e3174e15410c694e8c/thumbnail.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 gNYEES&quot;&gt;&lt;p&gt;Hola folks, &lt;/p&gt;&lt;p&gt;As a contributor, each of us is always striving hard in the ocean to open more and more pull-requests, but being a contributor just doesn&amp;#x27;t mean only raising PRs, it also means reviewing other PRs, pointing out mistakes, helping others in improving the code-quality/code-reusability/code-readability, helping in finding missing edge-cases that haven&amp;#x27;t been tackled yet, giving your opinions, writing LGTM, CITY helps nothing but just improving the confidence and engagement of the PR author.&lt;/p&gt;&lt;p&gt;So put on your &lt;strong&gt;Quality Tester&lt;/strong&gt; hats because here I&amp;#x27;ll talk about how to test the PRs with the label &lt;code&gt;component/mesheryctl&lt;/code&gt; i.e. pull-requests related to &lt;code&gt;mesheryctl&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Okay before we start, I&amp;#x27;ll like to tell you about &lt;a href=&quot;https://github.com/cli/cli&quot;&gt;GitHub CLI&lt;/a&gt;, it helps you checkout PRs very easily in your local system.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;The very first step is to review the PR, suggest changes if you think of any, ask queries, help the author to improve the code quality/readability/reusability, ask questions because asking helps you learn asking more better questions next time.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;PR authors either attach a video showcasing expected behavior or add written instructions about their fix under &lt;strong&gt;User Acceptance Behavior&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Now it&amp;#x27;s the time to checkout PR in your local system, we can check out any PR like this&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;gh &lt;/span&gt;&lt;span class=&quot;token function&quot; style=&quot;color:rgb(130, 170, 255)&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; checkout https://github.com/meshery/meshery/pull/4823&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;You can check if you&amp;#x27;re into the same branch as the PR author with &lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token function&quot; style=&quot;color:rgb(130, 170, 255)&quot;&gt;git&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; branch&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Well, if we&amp;#x27;re testing a PR related to mesheryctl, we need to build the binary from the same branch. change your directory to &lt;code&gt;mesheryctl&lt;/code&gt; folder and run&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token function&quot; style=&quot;color:rgb(130, 170, 255)&quot;&gt;make&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;This will create a mesheryctl binary according to your OS in the same directory&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Now it&amp;#x27;s time to test out this newly built binary according to what&amp;#x27;s been tackled in the PR and related issues. For e.g. &lt;code&gt;system start&lt;/code&gt; has some new functionality, make sure you followed the pull-request/linked-issue instruction for env setup, as sometimes fix/features are tackling an issue with a specific type of environment.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;./mesheryctl system start&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;the &lt;code&gt;./&lt;/code&gt; helps us in using the newly built cli-binary present in the current directory which we built in 5th step&lt;/p&gt;&lt;ol start=&quot;7&quot;&gt;&lt;li&gt;make sure we have a similar experience as mentioned in the Video or the instructions added to the PR. but the wait is it okay to give green flags to the PR? not yet tbh. We as a tester should turn a little evil and think of the relevant situations/environments which might not have been tackled but should be(basically we&amp;#x27;re trying to break the new feature/fix)&lt;/li&gt;&lt;li&gt;After spending a good amount of time testing the new behaviors, old standard behaviors, new test cases, few edge cases. We can provide new insights to the PR author about the behavior in your system, depending on our experience we can ask the PR author to address our new queries, or we can appreciate the work, or give green flags to the PR.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Wow, that was a ton of work there. well being a Tester is tough but very important before we merge pull requests. Every PR should be marked green with end-to-end testing before merging, we as a project are using GH Workflows to perform standard golang-testing but manual end-to-end testing completely removes margins of error.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Evolution of the Meshery CLI Command Reference]]></title><description><![CDATA[Autogeneration of Meshery CLI command reference]]></description><link>https://layer5.io/blog/meshery/evolution-of-the-meshery-cli-command-reference</link><guid isPermaLink="false">https://layer5.io/blog/meshery/evolution-of-the-meshery-cli-command-reference</guid><dc:creator><![CDATA[Aadhitya Amarendiran]]></dc:creator><pubDate>Thu, 09 Jun 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/0ec8439c9cc7b3a2b248d2fc9b3ce62c/mesheryctl.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 gNYEES&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Documentation plays a major role in any project. Even if the project is small or too big, the creator or the team behind the project needs to curate the documentation very well such that it&amp;#x27;ll be useful for new end users to refer and learn to use the project, troubleshoot the problems occurred and lot more. Thus, we, Layer5 have curated the documentation for Meshery to meet such purposes. Not to mention, &lt;code&gt;mesheryctl&lt;/code&gt;, the CLI client of Meshery needs a curated documentation as well. This blog describes about the evolution of &lt;code&gt;mesheryctl&lt;/code&gt; command reference page. &lt;/p&gt;&lt;/div&gt;&lt;h3&gt;Initial Command Reference Design&lt;/h3&gt;&lt;p&gt;The initial design of &lt;code&gt;mesheryctl&lt;/code&gt; command reference page is all made using pure markdown and the functionality is handled using Jekyll, the main framework used for Meshery Docs. This handled great at initial stage but had many limitations, such as:&lt;ul&gt;&lt;li&gt;Updation of YAML for data is often required&lt;/li&gt;&lt;li&gt;Design was obselete at initial stage&lt;/li&gt;&lt;li&gt;No separate pages for each command and subcommand&lt;/li&gt;&lt;/ul&gt;Thus, the idea for redesigning the &lt;code&gt;mesheryctl&lt;/code&gt; reference page was desperately needed.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.meshery.io&quot; alt=&quot;Meshery Documentation&quot; target=&quot;_parent&quot;&gt;&lt;img src=&quot;static/initial-design-5812f801a3ab38a2d0ac62b76983f6c4.png&quot; class=&quot;image-center-shadow&quot; alt=&quot;Initial design of mesheryctl command reference&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Updated Command Reference Design&lt;/h3&gt;&lt;p&gt;To tackle the shortcomings of the previous design, I was tasked to redesign the &lt;code&gt;mesheryctl&lt;/code&gt; command reference page entirely. This was a big task at first glance to me, as I was a new contributor back then. Eventually after manipulating the reference section with help of great folks, I was able to pull off the task and the design was updated. &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.meshery.io&quot; alt=&quot;Meshery Documentation&quot;&gt;&lt;img src=&quot;static/mesheryctl-docs-859bc4ba845ff2038916a47482464756.png&quot; class=&quot;image-center-shadow&quot; alt=&quot;Meshery CLI command reference&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The redesign work was done with help of HTML in markdown and with optimization in YAML code. A sample is given below.&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&amp;lt;!-- Copy this template to create individual doc pages for each mesheryctl commands --&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &amp;lt;!-- Name of the command --&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    # mesheryctl mesh&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &amp;lt;!-- Description of the command. Preferably a paragraph --&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    ## Description&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    {% assign name = site.data.mesheryctlcommands.cmds[page.command] %}&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    {{ name.description }}&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &amp;lt;!-- Basic usage of the command --&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &amp;lt;pre class=&amp;quot;codeblock-pre&amp;quot;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &amp;lt;div class=&amp;quot;codeblock&amp;quot;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    mesheryctl mesh [flags] &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &amp;lt;/pre&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    ...........&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h3&gt;Adding auto generation feature in reference&lt;/h3&gt;&lt;p&gt;As time passed, we realized that the command reference missed something for a while, though the design has been changed. Then, we thought the idea of automating the generation of docs such that developers don&amp;#x27;t need to change the code in docs section while working towards &lt;code&gt;mesheryctl&lt;/code&gt;. That&amp;#x27;s where we got to know that Cobra library (the library for CLI apps made using golang) has a feature to make doc pages automatically. So we decided to incorporate that feature into &lt;code&gt;mesheryctl&lt;/code&gt; docs page as well! After making several changes and a PR, I was finally able to introduce the feature in the docs site!&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 dWFHS&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 evYsys prism-code language-undefined&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;var startCmd = &amp;amp;cobra.Command {&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    Use:   &amp;quot;start&amp;quot;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    Short: &amp;quot;Start Meshery&amp;quot;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    Long:  `Start Meshery and each of its service mesh components.`,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    Args:  cobra.NoArgs,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    Example: `&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;// Start meshery&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;mesheryctl system start&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;// To create a new context for in-cluster Kubernetes deployments and set the new context as your current-context&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;mesheryctl system context create k8s -p kubernetes -s&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;// (optional) skip checking for new updates available in Meshery.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;mesheryctl system start --skip-update&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;// Reset Meshery&amp;#x27;s configuration file to default settings.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;mesheryctl system start --reset&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;// Silently create Meshery&amp;#x27;s configuration file with default settings&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;mesheryctl system start --yes&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;.....&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;}&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 jIVRXe&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    `,&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;Using this information provided above in each golang file, the markdown page is generated using Cobra CLI library and thus reducing the workload on the developer by automating via &lt;a href=&quot;https://github.com/meshery/meshery/blob/master/.github/workflows/mesheryctl-ci.yml#L73&quot;&gt;GitHub Actions&lt;/a&gt;.&lt;/p&gt;&lt;br/&gt;&lt;p&gt;This is so far on how the &lt;code&gt;mesheryctl&lt;/code&gt; command reference is evolved for now. And I hope that it&amp;#x27;ll continue to evolve in the field of documentation to serve the users to use Meshery in best way possible.&lt;/p&gt;&lt;/div&gt;</content:encoded></item></channel></rss>