<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Layer5 Resources]]></title><description><![CDATA[Expect more from your infrastructure. Cloud native, open source software for your cloud native infrastructure and applications. Allowing developers to focus on business logic, not infrastructure concerns. Empowering operators to confidently run modern infrastructure.]]></description><link>https://layer5.io</link><generator>GatsbyJS</generator><lastBuildDate>Fri, 19 Sep 2025 16:53:46 GMT</lastBuildDate><item><title><![CDATA[Layer5 Academy vs Moocit]]></title><description><![CDATA[
import { ResourcesWrapper } from "../../Resources.style.js";


<ResourcesWrapper>
<div className="comparison">

The [Layer5 Academy](https://cloud.layer5.io/academy) provides a structured platform for cloud native learning, combining core educational content with practical, hands-on experiences. Learners can enroll in guided paths, complete interactive labs, and take assessments that reinforce understanding at every stage.

The Academy supports progression through quizzes, tests, and certification exams, offering grades, retake options, and recognition through badges and certificates. For organizations, administrators can monitor learner performance, create new content, and send customized communications, ensuring both individual growth and collective visibility.

## Detailed Comparison

The following details highlight the key differences between Layer5 Academy and Moocit across various dimensions. Be sure to read the [Layer5 Academy documentation](https://docs.layer5.io/cloud/academy) for more in-depth information.

### Course Creation & Management

It is the backbone of the platform, enabling instructors to easily design, structure, and deliver courses. This includes tools for creating multimedia lessons, managing modules, setting prerequisites, and organizing assessments to ensure a seamless teaching process.


| Feature | Moocit | Layer5 | Notes |
|---------|--------|--------|-------|
| Content Variety | ✅ | ✅ | Both: Support text, images, videos, PDFs |
| Interactive Learning | ✅ | ✅ | Both: Quizzes, tests, self-assessment |
| Learning Paths/Tracks | ✅ | ✅ | Both: Support sequential paths and prerequisites |
| Course Publishing | ✅ | ✅ | Moocit: Web-based approach to content creation and publishing with incremental instant and scheduled releases. Layer5: Gitflow based approach to content creation and publishing is gitflow based with version-controlled publishing.  |
| Course Management | ✅ | ✅ | Both: Tools to organize, assign, and manage |
| Course Tracking | ✅ | ✅ | Both: Track learner progress and completion |
| Course Authoring | ⚠️ | ✅ | Moocit: uses the Open edX platform with XML-based authoring. Layer5: uses a modern Hugo static site generator with Markdown-based authoring. |
| Public Catalog | ❌ | ✅ | Moocit: Lacks browsable, public catalog. Layer5: Provides comprehensive discoverability with browsable, public catalog. |

### Learner Experience

Learner experience emphasizes intuitive design, collaboration, and flexibility. Both platforms enable personalized learning, progress tracking, and interactive engagement, with Layer5 extending into real-time collaboration via [Kanvas](/cloud-native-management/kanvas).

| Feature | Moocit | Layer5 | Notes |
|---------|--------|--------|-------|
| Learner Portal | ✅ | ✅ | |
| Self-paced Learning | ✅ | ✅ | |
| Instructor-led Learning | ✅ | ✅ | | 
| Mobile Learning | ✅ | ✅ | |
| Certificate Generation | ✅ | ✅ | Both: Shareable certifications |
| Social Learning | ✅ | ✅ | Both: Forums, discussions. Layer5: Kanvas and Slack. |
| Learner Profile | ⚠️ | ✅| Moocit: Limited support for interactive profiles. Layer5: Comprehensive learner profiles with progress tracking and social features. |
| Group / Synchronous Learning | ❌ | ✅ | Layer5: Real-time, collaborative learning. |
| Hands-on Labs | ❌ | ✅ | Layer5:  through Kanvas |

### Administration & Analytics

Both platforms deliver robust administration capabilities with user management, analytics, and compliance tools. Layer5 extends customization through its cloud-native ecosystem integration.

| Feature | Moocit | Layer5 | Notes |
|---------|--------|--------|-------|
| Assessment Management | ✅ | ✅ | Both: Build and evaluate assessments. |
| Gradebook | ✅ | ✅ | Both: Track grades and scores. |
| Reporting & Statistics | ✅ | ✅ | Both: Insights into learner engagement and performance. |
| User Management | ⚠️ | ✅ | Moocit: Manage user accounts. Layer5: Manage user accounts with granular and customizable permissions and roles. |
| Group Creation and Management |  ✅ | ✅ | Both: Allow instructors to organize students and content into different groups. |
| Single Sign-On (SSO) | ❌ | ✅ | Layer5: Enhanced identity and granular access control. |
| Branded Academy Invitations | ❌ | ✅ | Layer5: Approval queues, domain-based allowances. |
| Branded Content Announcements | ❌ | ✅ | Layer5: New content announcements |
| Public Clouds | ❌ | ✅ | Layer5: AWS, Azure, GCP, DigitalOcean, and an Kubernetes service |
| Orchestration Support | ❌ | ✅ | Layer5: Hundreds of CNCF ecosystem integrations |

### Additional Features

Extra functionalities make the platforms more engaging. While both cover gamification and eCommerce, Layer5 distinguishes itself with advanced white-labeling and community-driven Slack support.

| Feature | Moocit | Layer5 | Notes |
|---------|--------|--------|-------|
| Gamification | ✅ | ✅ | Layer5: badges and learning leaderboards. |
| eCommerce Management | ✅ | ✅ | Both: Support Stripe as a built-in integration. |
| White-labeling | ✅ | ✅ | Layer5: Superior branding flexibility. |
| Chatbot Support | ⚠️ | ✅ | Moocit: website chat. Layer5: community-centric Slack. |
| Cloud Native Focused | ❌ | ✅  | Layer5 is a cloud native ecosystem stalwart, tightly aligned with the CNCF. |

## Summary

While Moocit lacks content attribution and seamless orchestration with hands-on labs, Layer5 Academy stands out by enabling true learning through interactive, hands-on labs powered by Kanvas—where real learning happens. Layer5 Academy's labs drive students to utilize third-party public cloud providers' infrastructure, meaning more time spent on hands-on activities can increase costs for users paying for their chosen provider's resources. However, public cloud providers featuring Academy content and labs can incentivize adoption by offering credits and discount codes, presented to learners after enrolling in a Challenge, Learning Path, or Certification. Furthermore, Layer5 enhances administrative efficiency with support for bulk invitations for new learners and email announcements for new academy content, underscoring its superior strengths in content creation, learner experience, and administrative capabilities compared to Moocit.

</div>
</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/academy/layer5-academy-vs-moocit</link><guid isPermaLink="false">https://layer5.io/resources/academy/layer5-academy-vs-moocit</guid><pubDate>Mon, 18 Aug 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/c78e99b249ac1d7cd5251ac9aea491f6/academy.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[HPE's Adoption of Layer5 Meshery and Kanvas]]></title><description><![CDATA[
import { ResourcesWrapper } from "../../Resources.style.js";
import { HPEfacts, HPEintro, HPEbenefits } from "./hpe.style.js";
import Blockquote from "../../../../reusecore/Blockquote";
import { Link } from "gatsby";
import Button from "../../../../reusecore/Button";
import GlobeIcon from "./globe.svg";
import UsersIcon from "./users.svg";
import NetworkIcon from "./network.svg";
import LayersIcon from "./layers.svg";
import InlineQuotes from "../../../../components/Inline-quotes";
import MesheryIntegration from "./meshery-integrations.svg";
import Maxi from "../../../../collections/members/maximiliano-churichi/Maximiliano-Churichi.webp";
import Yogi from "./yogi.webp"

<ResourcesWrapper>

<HPEintro>
HPE's adoption of Meshery was driven by the need to simplify Kubernetes cluster management and monitoring. Meshery is an open source cloud native management tool that provides a self-service platform for designing, visualizing, deploying, testing, and operating cloud native infrastructure.
</HPEintro>

<div style={{display: "flex", justifyContent: "center", padding: "2%"}}>

   <Button $secondary title="Download Case Study" alt="Download HPE's Adoption of Layer5 Meshery and Kanvas" $url="/case-studies/hpe-adoption-of-layer5-meshery-and-kanvas.pdf" />

</div>

<p>
   HPE, a leading technology company specializing in enterprise infrastructure, adopted Meshery Extension to enhance their Kubernetes deployments. HPE uses Kubernetes as a primary platform to build and deploy their containerized applications. The company has a large and complex Kubernetes environment, which requires robust networking solutions for efficient communication between services.
</p>



<HPEfacts>
   <tr><td colspan="2"><h4>HPE FAST FACTS</h4></td></tr>
   <tr>
      <td>
      <img src={UsersIcon} />
      Full Time Employees: 60,000+
      </td>
      <td>
      <img src={GlobeIcon} />
      Market Presence: HPE is one of the largest technology companies globally, serving customers in over 150 countries.
      </td>
   </tr>
   <tr>
      <td>
      <img src={LayersIcon} />
      HPE GreenLake: HPE GreenLake is a key offering by the company, providing a flexible and scalable IT infrastructure model known as "everything-as-a-service."
      </td>
      <td>
      <img src={NetworkIcon} />
      Research and Development: HPE invests significantly in research and development to drive innovation. It operates HPE Labs, which focuses on developing cutting-edge technologies and solutions for the future.
      </td>
   </tr>
</HPEfacts>

<p>
SPIRE is a toolchain of APIs for establishing trust between software systems across a wide variety of hosting platforms. SPIRE exposes the SPIFFE Workload API, which can attest running software systems and issue SPIFFE IDs and SVIDs to them. <br/><br/>
</p>

<HPEbenefits>
    <b>Meshery offers several benefits to HPE, including:</b>
    <br/>
    ✔️ Consistent service mesh management: HPE can now manage all their service meshes consistently, regardless of the underlying infrastructure or cloud provider.
    <br/>
    ✔️ Improved observability: With Meshery's built-in visualization and observability tools, HPE can gain insights into the behavior of their service meshes, detect anomalies, and troubleshoot issues in real-time.
    <br/>
    ✔️ Simplified testing and validation: Meshery's service mesh validation capabilities enable HPE to easily test and validate their service meshes, ensuring that they meet their performance, security, and compliance requirements.
    <br/>
    ✔️ Enhanced security: With Meshery's security features, HPE can ensure that their service meshes are secure and compliant with their organization's security policies.
</HPEbenefits>
<br/>

<p>
    Overall, Meshery has helped HPE to streamline their integration of the identity management control plane to reduce complexity, and improve the overall reliability and performance of their Kubernetes environment. SPIFFE is a set of open-source specifications for a framework capable of bootstrapping and issuing identity to services across heterogeneous environments and organizational boundaries. The lifecycle of SPIFFE identities, SVIDs, is managed by SPIRE, a production-ready implementation of the SPIFFE APIs that performs node and workload attestation in order to securely issue SVIDs to workloads, and verify the SVIDs of other workloads, based on a predefined set of conditions.
</p>

<p>
    <b>HPE's adoption of Meshery has also been enhanced by the platform's ability to integrate with other popular technologies, such as SPIRE and Istio</b>
</p>
<p style={{display: "flex", justifyContent: "center", alignItem: "center", padding: "2%"}}>
<img src={MesheryIntegration}></img>
</p>
<p>
   SPIRE is an open-source project that provides a secure and scalable solution for service identity and authentication in distributed systems. HPE uses SPIRE to authenticate and authorize services in their Kubernetes environment, which ensures that only authorized services can communicate with each other.
</p>

<p>
   Meshery's integration with SPIRE enables HPE to manage SPIRE instances, issue and revoke service certificates, and automate the management of SPIRE agents across their Kubernetes clusters. This integration ensures that HPE's service meshes are secure, and only authorized services can communicate with each other.
</p>
  <InlineQuotes
          quote="With a goal to bring workload identity and attestation to all service meshes, HPE Security Engineering uses the Meshery's Extension to deploy their cloud native infrastructure of choice and test the performance of our SPIFFE and SPIRE-based identity solution."
          person="Maximiliano Churichi"
          title="Software Engineer at HPE"
          image={Maxi} />

<p>
   In addition, HPE's use of Meshery has been enhanced by its integration with Istio, an open-source service mesh that provides a comprehensive solution for traffic management, security, and observability in Kubernetes environments.
</p>

<p>
   Meshery's integration with Istio enables HPE to manage Istio service meshes and configurations, automate the deployment of Istio components, and monitor and visualize Istio metrics and traces. This integration enables HPE to simplify the management of their Istio service meshes, ensure their security and compliance, and gain insights into their behavior for better decision-making.
</p>

<p>
   Overall, HPE's adoption of Meshery, along with its integration with SPIRE and Istio, has enabled the company to streamline their service mesh management, ensure the security and compliance of their Kubernetes environment, and gain valuable insights into the behavior of their service meshes for improved performance and reliability.
</p>

<p>
   <b>Meshery also implements the Service Mesh Performance (SMP) specification</b>
</p>

<p>
   SMP is a community-driven effort that provides a standard for measuring and comparing the performance of different service meshes. It is designed to help users select the best service mesh for their needs by providing a common framework for benchmarking.
</p>
 <InlineQuotes
          quote="The Layer5 team has been amazing. Our project wouldn’t have been successful with out Meshery."
          person="Yogi Porla"
          title="Engineering Manager, HPE"
          image={Yogi} />
<p>
   Meshery implements SMP by providing a simple and easy-to-use interface for running performance tests against different service meshes. Users can select the service mesh they want to test, configure the test parameters (such as the number of requests per second and the number of concurrent clients), and run the test. Meshery will then generate a report that shows the performance metrics for each service mesh, such as latency, throughput, and error rates.
</p>

<p>
   By implementing SMP, Meshery provides a valuable tool for developers and operators who are evaluating different service meshes. Instead of having to create their own benchmarks, they can use SMP to get an objective and standardized view of each service mesh's performance characteristics. This can save a significant amount of time and effort, and help users make more informed decisions when choosing a service mesh.
</p>

<p>
Overall, HPE's use of Meshery and the Docker Extension for Meshery demonstrates the power of cloud native technologies and the importance of open source collaboration. By leveraging these tools, HPE has been able to streamline its development and deployment processes, improve performance and security, and stay at the forefront of the cloud native movement.
</p>

</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/case-study/hpes-adoption-of-meshery-and-kanvas</link><guid isPermaLink="false">https://layer5.io/resources/case-study/hpes-adoption-of-meshery-and-kanvas</guid><dc:creator><![CDATA[Layer5 Team]]></dc:creator><pubDate>Sat, 17 Jun 2023 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/91d038ef73208e4f202eecb42cb08c1f/meshery-and-hpe.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Configuring Highly Available Docker Swarm]]></title><description><![CDATA[
import {ResourcesWrapper} from "../../Resources.style.js";
import {Link} from "gatsby"
import DockerExtensionCTA from "../../../../sections/Docker-Meshery/docker-extension-CTA.js"
import PlaygroundCTA from "../../../../sections/Playground/playground-CTA.js"

<ResourcesWrapper>
<p>
Docker Swarm is a{" "}
<Link to="../../articles/kubernetes/management-of-kubernetes">
container orchestration
</Link>{" "}
tool that makes it easy to manage and scale your existing Docker
infrastructure. It consists of a pool of Docker hosts that run in Swarm
mode with some nodes acting as managers, workers, or both. Using Docker
Swarm mode to manage your Docker containers brings the following
benefits:
</p>
<ul>
<li>It allows you to incrementally apply updates with zero downtime.</li>
<li>
It increases application resilience to outages by reconciling any
differences between the actual state and your expressed desired state.
</li>
<li>
It eases the process of scaling your applications since you only need to
define the desired number of replicas in the cluster.
</li>
<li>
It is built into the <code>docker</code> CLI, so you don't need
additional software to get up and running.
</li>
<li>
It enables multi-host networking such that containers deployed on
different nodes can communicate with each other easily.
</li>
</ul>
<p>
In this tutorial, you will learn key concepts in Docker Swarm and set up a
highly available Swarm cluster that is resilient to failures. You will
also learn some best practices and recommendations to ensure that your
Swarm setup is fault tolerant.
</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>
Before proceeding with this tutorial, ensure that you have access to five
Ubuntu 22.04 servers. This is necessary to demonstrate a highly available
set up, although it is also possible to run Docker Swarm on a single
machine. You also need to configure each server with a user that has
administrative privileges.
</p>
<p>
The following ports must also be available on each server for communication
purposes between the nodes. On Ubuntu 22.04, they are open by default:
</p>
<ul>
<li>TCP port 2377 for cluster management communications,</li>
<li>TCP and UDP port 7946 for communication among nodes,</li>
<li>TCP and UDP port 4789 for overlay network traffic.</li>
</ul>
<h2 id="explaining-docker-swarm-terminology">Explaining Docker Swarm terminology</h2>
<p>
Before proceeding with this tutorial, let's examine some terms and
definitions in Docker Swarm so that you have enough understanding of what
each one means when they are used in this article and in other Docker Swarm
resources.
</p>

<div>
    <ul>
    <li>
        <strong>Node</strong>: refers to an instance of the Docker engine in the Swarm cluster.
    </li>
    <li>
        <strong>Manager nodes</strong>: they are tasked with handling orchestration and cluster management functions, and dispatching incoming tasks to worker nodes. They can also act as worker nodes unless placed in Drain mode (recommended).
    </li>
    <li>
        <strong>Leader</strong>: this is a specific manager node that is elected to perform orchestration tasks and management/maintenance operations by all the manager nodes in the cluster using the <a rel="" target="_blank" className="whitespace-nowrap" href="https://raft.github.io/">Raft Consensus Algorithm</a>.
    </li>
    <li>
        <strong>Worker nodes</strong>: are Docker instances whose sole purpose is to receive and execute Swarm tasks from manager nodes.
    </li>
    <li>
        <strong>Swarm task</strong>: refers to a Docker container and the commands that run inside the container. Once a task is assigned to a node, it can run or fail but it cannot be transferred to a different node.
    </li>
    <li>
        <strong>Swarm service</strong>: this is the mechanism for defining tasks that should be executed on a node. It involves specifying the container image and commands that should run inside the container.
    </li>
    <li>
        <strong>Drain</strong>: means that new tasks are no longer assigned to a node, and existing tasks are reassigned to other available nodes.
    </li>
    </ul>
    
    <h2 id="docker-swarm-requirements-for-high-availability">Docker Swarm requirements for high availability</h2>
    <p>A highly available Docker Swarm setup ensures that if a node fails, services on the failed node are re-provisioned and assigned to other available nodes in the cluster. A Docker Swarm setup that consists of one or two manager nodes is not considered highly available because any incident will cause operations on the cluster to be interrupted. Therefore the minimum number of manager nodes in a highly available Swarm cluster should be three.</p>
    <p>The table below shows the number of failures a Swarm cluster can tolerate depending on the number of manager nodes in the cluster:</p>
    <div>
    <table>
        <thead>
        <tr>
            <th>Manager Nodes</th>
            <th>Failures tolerated</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td>1</td>
            <td>0</td>
        </tr>
        <tr>
            <td>2</td>
            <td>0</td>
        </tr>
        <tr>
            <td>3</td>
            <td>1</td>
        </tr>
        <tr>
            <td>4</td>
            <td>1</td>
        </tr>
        <tr>
            <td>5</td>
            <td>2</td>
        </tr>
        <tr>
            <td>6</td>
            <td>2</td>
        </tr>
        <tr>
            <td>7</td>
            <td>3</td>
        </tr>
        </tbody>
    </table>
    </div>
    <p>As you can see, having an even number of manager nodes does not help with failure tolerance, so you should always maintain an odd number of manager nodes. Fault tolerance improves as you add more manager nodes, but Docker recommends no more than seven managers so that performance is not negatively impacted since each node must acknowledge proposals to update the state of the cluster.</p>
    <p>You should also distribute your manager nodes in separate locations so they are not affected by the same outage. If they run on the same server, a hardware problem could cause them all to go down. The high availability Swarm cluster that you will be set up in this tutorial will therefore exhibit the following characteristics:</p>
    <ul>
    <li>5 total nodes (2 workers and 3 managers) with each one running on a separate server.</li>
    <li>2 worker nodes (<code>worker-1</code> and <code>worker-2</code>).</li>
    <li>3 manager nodes (<code>manager-1</code>, <code>manager-2</code>, and <code>manager-3</code>).</li>
    </ul>
    <PlaygroundCTA/>
    <DockerExtensionCTA/>
    <h2 id="step-1-installing-docker">Step 1 — Installing Docker</h2>
    <p>In this step, you will install Docker on all five Ubuntu servers. Therefore, execute all the commands below (and in step 2) on all five servers. If your host offers a snapshot feature, you may be able to run the commands on a single server and use that server as a base for the other four instances.</p>
    <p>Let's start by installing the latest version of the Docker Engine (20.10.18 at the time of writing). Go ahead and update the package information list from all configured sources on your system:</p>
    <pre>
    <code>
        sudo apt update
    </code>
    </pre>
    <p>Afterward, install the following packages to allow <code>apt</code> to use packages over HTTPS:</p>
    <pre>
    <code>
        sudo apt install apt-transport-https ca-certificates curl software-properties-common
    </code>
    </pre>
    <p>Next, add the GPG key for the official Docker repository to the server:</p>
    <pre>
    <code>
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    </code>
    </pre>
    <p>Once the GPG key is added, include the official Docker repository in the server's apt sources list:</p>
    <pre>
    <code>
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
    </code>
    </pre>
    <p>Finally, update apt once again and install the Docker Engine:</p>
    <pre>
    <code>
        sudo apt update
    </code>
    </pre>
    <pre>
    <code>
        sudo apt install docker-ce
    </code>
    </pre>
    <p>Once the relevant packages are installed, you can check the status of the <code>docker</code> service using the command below:</p>
    <pre>
    <code>
        sudo systemctl status docker
    </code>
    </pre>
    <p>If everything goes well, you should observe that the container engine is active and running on your server.</p>
    
    <h2 id="step-2-executing-the-docker-command-without-sudo">Step 2 — Executing the Docker command without sudo</h2>
    <p>By default, the <code>docker</code> command can only be executed by the root user or any user in the <code>docker</code> group (auto created on installation). If you execute a <code>docker</code> command without prefixing it with <code>sudo</code> or running it through a user that belongs to the <code>docker</code> group, you will get a permission error that looks like this:</p>
    <pre>
    <code>
        Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json": dial unix /var/run/docker.sock: connect: permission denied
    </code>
    </pre>
    <p>As mentioned earlier, using <code>sudo</code> with <code>docker</code> is a security risk, so the solution to the above error is to add the relevant user to the <code>docker</code> group. This can be achieved through the command below:</p>
    <pre>
    <code>
        sudo usermod -aG docker $USER
    </code>
    </pre>
    <p>Next, run the following command and enter the user's password when prompted for the changes to take effect:</p>
    <pre>
    <code>
        su - $USER
    </code>
    </pre>
    <p>You should now be able to run <code>docker</code> commands without prefixing them with <code>sudo</code>. For example, when you run the command <code>docker ps</code>, you should observe the output.</p>
    <p>Before proceeding to the next step, ensure that all the commands in step 1 and step 2 have been executed on all five servers.</p>
    
    <h2>Step 3 — Initializing the Swarm Cluster</h2>
    <p>At this point, each of your five Docker instances are acting as separate hosts and not as part of a Swarm cluster. Therefore, in this step, we will initialize the Swarm cluster on the <code>manager-1</code> server and add the hosts to the cluster accordingly.</p>
    <p>Start by logging into one of the Ubuntu servers (<code>manager-1</code>) and retrieve the private IP address of the machine using the following command:</p>
    <pre>
    <code>hostname -I | awk '&#123;print $1&#125;'
    </code>
    </pre>
    <p>Copy the IP address to your clipboard and replace the <code>&lt;manager_1_server_ip&gt;</code> placeholder in the command below to initialize Swarm mode:</p>
    <pre>
    <code>
        docker swarm init --advertise-addr &lt;manager_1_server_ip&gt;
    </code>
    </pre>
    <p>If the command is successful, you will see output indicating that the Swarm has been initialized and that the current node is now a manager. It will also provide a command to join worker nodes to the cluster. Copy the command for later use.</p>
    
    <p>Next, SSH into each of the other four Ubuntu servers (manager-2, manager-3, worker-1, and worker-2) and run the command you copied earlier to join them to the Swarm cluster. The command should look like this:</p>
    <pre>
    <code>
        docker swarm join --token &lt;token&gt; &lt;manager_1_server_ip&gt;:&lt;port&gt;
    </code>
    </pre>
    
    <p>After running the command on each server, you should see output indicating that the node has joined the Swarm as either a manager or a worker. To verify the status of the Swarm cluster, you can run the command <code>docker node ls</code> on the manager node:</p>
    <pre>
    <code>
        docker node ls
    </code>
    </pre>
    
    <p>You should see a list of all the nodes in the Swarm cluster, including their IDs, hostname, status, availability, and whether they are a manager or a worker.</p>
    
    <h2>Step 4 — Deploying the Application Stack</h2>
    <p>Now that you have a functioning Docker Swarm cluster, you can deploy your application stack. In this tutorial, we will use a simple example of a web application stack consisting of a front-end service and a back-end service.</p>
    
    <p>Start by creating a new directory for your application stack on the manager node:</p>
    <pre>
    <code>
        mkdir app-stack
        cd app-stack
    </code>
    </pre>
    
    <p>Next, create a file called <code>docker-compose.yml</code> in the <code>app-stack</code> directory and open it in a text editor:</p>
    <pre>
    <code>
        nano docker-compose.yml
    </code>
    </pre>
    
    <p>Copy and paste the following YAML code into the <code>docker-compose.yml</code> file:</p>
    <pre>
    <code>
        version: '3.8'
        
        services:
        frontend:
            image: nginx:latest
            ports:
            - 80:80
            deploy:
            replicas: 2
            restart_policy:
                condition: on-failure
        
        backend:
            image: httpd:latest
            ports:
            - 8080:80
            deploy:
            replicas: 2
            restart_policy:
                condition: on-failure
    </code>
    </pre>
    
    <p>This Docker Compose file defines two services: <code>frontend</code> and <code>backend</code>. The <code>frontend</code> service uses the <code>nginx:latest</code> image and maps port 80 of the host to port 80 of the container. It is configured to have 2 replicas and to restart on failure. The <code>backend</code> service uses the <code>httpd:latest</code> image and maps port 8080 of the host to port 80 of the container. It is also configured to have 2 replicas and to restart on failure.</p>
    
    <p>Save and close the <code>docker-compose.yml</code> file.</p>
    
    <p>To deploy the application stack, run the following command:</p>
    <pre>
    <code>
        docker stack deploy -c docker-compose.yml app-stack
    </code>
    </pre>
    
    <p>If the command is successful, you should see output indicating that the services are being deployed. You can check the status of the services by running the command <code>docker service ls</code>:</p>
    <pre>
    <code>
        docker service ls
    </code>
    </pre>
    
    <p>You should see a list of the services in the stack, including their names, mode, replicas, and ports.</p>
    
    <h2>Conclusion</h2>
    <p>In this tutorial, you learned how to set up a highly available Docker Swarm cluster and deploy a simple application stack. This setup provides fault tolerance and load balancing for your applications, allowing you to scale them easily as your needs grow.</p>
    
    <p>Next steps:</p>
    <ul>
    <li>Explore more Docker Swarm features, such as service updates and rolling updates.</li>
    <li>Deploy your own application stack using Docker Compose.</li>
    <li>Learn about Docker networking and how to create overlay networks.</li>
    </ul>
</div>

</ResourcesWrapper>]]></description><link>https://layer5.io/resources/docker/configuring-highly-available-docker-swarm</link><guid isPermaLink="false">https://layer5.io/resources/docker/configuring-highly-available-docker-swarm</guid><pubDate>Thu, 18 May 2023 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/5b72df1802df0608ac6ee6e6bff70c9d/docker-swarm.webp" length="0" type="image/webp"/><content:encoded></content:encoded></item><item><title><![CDATA[Terraform with Meshery]]></title><description><![CDATA[
import { ResourcesWrapper } from "../../Resources.style.js";
import { Link } from "gatsby";
import Button from "../../../../reusecore/Button";

<ResourcesWrapper>
<p>
    Terraform is a powerful tool that helps users manage and provision infrastructure resources in a consistent and efficient manner. With Terraform, you can define your infrastructure as code, using human-readable configuration files that can be versioned, shared, and reused. This makes it easy to create, modify, and manage your infrastructure resources, whether they are cloud-based or on-premises.
</p>
<div class="intro">
  <p>
     It is an open source tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned.
  </p>
</div>

<p>
  One way to further enhance your use of Terraform is by integrating it with Meshery. Meshery is a cloud-native management platform that provides a unified interface for managing and monitoring your infrastructure resources, including those managed by Terraform. By integrating Terraform with Meshery, you can leverage the power and flexibility of both tools to streamline your infrastructure management process.
</p>

<p>
  One of the key benefits of using Terraform with Meshery is the ability to manage and monitor infrastructure resources in a consistent and centralized manner. With Meshery, you can view and manage all of your infrastructure resources, whether they are managed by Terraform or other tools, from a single dashboard. This allows you to quickly identify any issues or potential problems with your infrastructure, and take action to resolve them in a timely manner.
</p>

<p>
  Another benefit of using Terraform with Meshery is the ability to automate your infrastructure management process. With Meshery, you can create and manage automated pipelines for provisioning and managing your infrastructure resources. This can help to reduce the time and effort required to manage your infrastructure, and allow you to focus on other important tasks.
</p>

<p>
  In addition to these benefits, using Terraform with Meshery also provides a number of other advantages. For example, Meshery integrates with a wide range of tools and platforms, allowing you to easily incorporate your existing infrastructure resources into your management process. This can help to reduce the complexity of managing your infrastructure, and make it easier to keep everything running smoothly.
</p>

<p>
  Overall, the use of Terraform with Meshery can help to streamline and improve your infrastructure management process. By integrating these two powerful tools, you can gain greater visibility and control over your infrastructure resources, and automate many of the tasks involved in managing them. This can help to reduce the time and effort required to manage your infrastructure, and allow you to focus on other important tasks. So, it is a good idea to use Terraform with Meshery to improve the efficiency and effectiveness of your infrastructure management process.
</p>

</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/cloud-native/terraform-with-meshery</link><guid isPermaLink="false">https://layer5.io/resources/cloud-native/terraform-with-meshery</guid><dc:creator><![CDATA[Gaurav Chadha]]></dc:creator><pubDate>Thu, 22 Dec 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/895ec8ea35cf68449389f73e4285cb39/terraform-color.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Management of Kubernetes]]></title><description><![CDATA[import { ResourcesWrapper } from "../../../Resources.style.js";
import picture1 from "./meshery-core-architecture.webp"; 
import picture2 from "./settings.webp"; 
import picture3 from "./context-switcher.webp"; 
import ClusterImg from "./multi-cluster-kubernetes-management-with-meshery.webp";

<ResourcesWrapper>
  
  <h3> What is Kubernetes Management, and Why Should You Care? </h3>
  
  <p> It is easy to understand why Kubernetes has become one of the most popular tools on the market today. Primarily, it allows you to easily manage Docker 
    containers across your entire infrastructure with very little overhead, making it easier than ever to manage massive amounts of information in a timely manner. 
    But what are you supposed to do with all this information? That’s where Kubernetes management comes into play—the process of using that information in an 
    effective manner can make or break your efforts, so it’s essential that you choose the right solutions from the get-go.
 </p>
  
  <h3> Defining Kubernetes management </h3>
  
  <p> Kubernetes management is the process of managing your containers on a Kubernetes cluster. This can include things like adding or removing clusters, scaling 
    clusters up or down, balancing workloads across nodes in a cluster, and restarting failed containers or nodes in a cluster. These tasks are complicated and 
    involve many different types of actions. Figuring out how to do them all manually would be extremely time-consuming. Fortunately, there are tools like Meshery 
    that automate these tasks for you, making it easier to see what’s going on within your cluster so you can make informed decisions about what needs to happen 
    next. Staying on top of Kubernetes management will not only keep your cluster running smoothly but also help prevent problems before they occur. Automating this 
    process will save you time and money, leaving more time to focus on other aspects of the business. When things go wrong, automated Kubernetes management allows 
    you to have a plan and know exactly what steps need to be taken to recover from an incident. With these benefits in mind, it’s important that companies with 
    containerized infrastructure use some type of automation for their Kubernetes management.
  </p>
  
  <h3> The benefits of Kubernetes management </h3>
  
  <p> Kubernetes management can seem like a daunting task. In the past, IT teams had to worry about maintaining large clusters of machines that required constant 
    tweaking and monitoring. Kubernetes simplifies this process by automating tasks such as: 
    <ul>
      <li> Monitoring cluster health </li>
      <li> Deploying apps across nodes </li>
      <li> Running rolling updates </li>
      <li> Scaling up or down resources on demand </li>
      <li> Auto-recover from failures </li>
      <li> Application deployment consistency </li>
      <li> Managing container upgrades </li>
    </ul>
  After reading through these benefits, you may be asking yourself, "Why should I care? Here are two reasons why you should care about Kubernetes management: - 
  Kubernetes management has been shown to improve software development efficiency because it reduces time spent waiting for containers to restart and redeploy. 
  A recent study showed that developers using Kubernetes were able to deploy new code changes at least 27% faster than developers without any container orchestration 
  solution.
  </p>
  
  <p> Kubernetes management has also been shown to reduce operational costs because it eliminates the need for manual intervention in scaling applications, updating 
  running containers with new versions, etc. If your IT team was spending 10 hours per week on manual operations before adopting Kubernetes, they'll spend only 2 
  hours after switching over! </p>
  
<CTA_FullWidth 
  image={ClusterImg}
  alt="Multi-Cluster Kubernetes Management with Meshery"
  content="Multi-Cluster Kubernetes Management with Meshery"
  button_text="Read blog post"
  url="/blog/meshery/multi-cluster-kubernetes-management-with-meshery"
  external_link={false}
  className="get-start-kubernetes-resource"
/>
  
  <h3> The challenges of Kubernetes management </h3>
  
  <p> Kubernetes management can seem like a difficult endeavour. Between determining how to automate deployment and scaling and comprehending the fundamentals of 
  how it operates, there are numerous factors to consider. Fortunately, there are numerous frameworks that simplify this procedure. But before going into new 
  frameworks or technologies, you must grasp what Kubernetes administration comprises so that you know what you're attempting to automate. Kubernetes management 
  comprises a variety of activities, such as building up clusters, keeping apps running on those clusters up-to-date, monitoring usage and providing alarms to keep 
  things running smoothly, and shutting down clusters when they are no longer required. </p>
  
  <p> There are numerous ways to manage these tasks: manually, with containers, with an orchestration system such as Ansible Tower, Cloud Control 12c, or ServiceNow 
  NMS, with containers-as-a-service providers such as Docker Datacenter or AWS EKS, with container service offerings from cloud providers such as Azure Container 
  Instances, by configuring Kubernetes with your own framework, and by installing Kubectl on your laptop for direct control. Each strategy has advantages and 
  disadvantages that may make one more suitable for your organisation than another. Regardless of the approach you adopt, you must plan accordingly. </p>
  
  <p> Importantly, the fact that Kubernetes is gaining popularity does not imply that it will replace your existing infrastructure layers. It augments their 
  capabilities with scalability and large-scale application management (which would have been difficult without automation). In addition, the definition of 
  management varies based on the size of the organisation: small businesses may prefer self-hosted platforms, whilst larger businesses would often primarily rely on 
  SaaS solutions. </p>
  
  <h3> How Meshery makes it easier to run Kubernetes </h3>
  
  <p> Meshery is the only cloud-native manager in the world that supports more adapters than any other project or product. </p>
  
  <img src = {picture1} class="image-center" alt="Management of Kubernetes with Meshery" />
  
  <p> Meshery has been designed for the world of many service meshes and many Kubernetes clusters. As such, great attention was made to guarantee that it is an 
  extensible management platform, able to handle a diverse range of infrastructure and new use cases quickly through its plugin mechanism. Meshery Server acts as an 
  operation delegator, determining which Meshery Adapter has registered its capacity for the given operation. The operation is then sent to the appropriate component 
  using a gRPC call. This could be one of Meshery's service mesh adapters, like the Istio adapter. </p> 
  
  <p> Meshery's capability is constantly expanding, from multi-mesh to now multi-cluster, to give developers, operators, and security engineers more control over 
  their infrastructure. Each part of Meshery's architecture makes a big difference in how it manages multiple Kubernetes clusters. </p>
  
  <h3> Meshery management across many clusters </h3>
  
  <p> From the settings page, users can do things related to clusters, like add more clusters, remove data from existing clusters, or delete existing clusters. </p>
  
  <img src = {picture2} class="image-center" alt="Management of Kubernetes with Meshery" />
  
  <p> Meshery also deploys Meshery operators throughout the cluster it is about to manage. This operator is in charge of the Meshery broker and the MeshSync 
  lifecycle. MeshSync is responsible for monitoring various types of resources by establishing a watch stream over each of them. MeshSync then sends the data to the 
  NATS server, of which the Meshery server is a client. Meshery server then receives all necessary data relating to cluster activity. </p>
  
  <img src = {picture3} class="image-center" alt="Management of Kubernetes with Meshery" />
  
  <p> Meshery, by default, wants to be as aware of your infrastructure as possible in order to deliver value. As such, it deploys its operator across each identified 
  cluster. However, you can fine-tune this configuration by going over each one. </p>
  
  <h3> The future of Kubernetes management </h3>
  
  <p> Kubernetes management has been one of the buzzwords since 2018. But what does it actually mean? And why should you care about it? At its core, Kubernetes 
  management is a system that helps make sense of the nuances of how different containers work together to create an application. As we rely more on containers for 
  our everyday apps, there needs to be a way to keep track of them all. That's where Kubernetes comes in with its ability to manage these containers that are spread 
  out across different servers and understand which ones need more resources or want to be shut down because they're no longer needed. The easier it becomes for 
  developers and engineers to deploy applications without worrying about how they are going to be managed, the better off everyone will be. Fortunately, as 
  containerization grows in popularity among developers and IT teams alike, so does the number of tools for managing it. </p>
  
  <p> A lot of container platforms provide native management functionality: Docker Swarm allows you to use simple commands like swarm stop or swarm pull when your 
  swarm is up-to-date; Kargo automatically manages clusters using zero-touch configuration; Rancher provides tools to manage containers using any infrastructure 
  stack; and Mesos offers both orchestration capabilities through Marathon as well as advanced resource scheduling features. It's not always easy to know which 
  platform will work best for your organization, but it's important to find one that suits your company's needs—especially if IT is looking forward to a future 
  without manual management tasks! </p>
 
</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/kubernetes/management-of-kubernetes</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/management-of-kubernetes</guid><dc:creator><![CDATA[Tolulope Ola-David]]></dc:creator><pubDate>Mon, 21 Nov 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/9287b4a708bb510f64057ea305498b77/kubernetes-logo.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Getting Started with Kubernetes]]></title><description><![CDATA[import { ResourcesWrapper } from "../../../Resources.style.js";
import picture1 from "./Picture1.webp";
import ClusterImg from "./multi-cluster-kubernetes-management-with-meshery.webp";
import KanvasDesigner from "./KanvasDesigner.webp"
import KanvasVisualizer from "./KanvasVisualizer.webp"

<ResourcesWrapper>
<p>
Kubernetes, an open-source container orchestration platform, is growing in popularity for deploying and managing cloud-native applications. Kubernetes was created by Google in 2014, and it is now used by many major companies, including IBM, Microsoft, Red Hat, and Amazon. In this article, we'll talk about Kubernetes, its benefits, and the best ways for your organization to use it.
 </p>
  
  <h3> What is Kubernetes? </h3>
  
<p>
Kubernetes offers fully managed and adapted architecture services that optimize your cloud-native application. Kubernetes is a platform that hides virtual machines, shows the infrastructure as an infrastructure-as-a-service (IAAS), network, and load balancer, and offers data storage and operations that are consistent across containers.
</p>
  
  <p> For example, Kubernetes nodes work as Kubernetes containers, such as an application, an application server, and control processes in Docker containers. Kubernetes components such as Kubernetes nodes and Kubernetes containers can be defined or modified via configuration files or can be specified subsequently. Individual Kubernetes components can be scaled according to elasticity needs to optimize performance.</p>
  
  <p> Kubernetes optimizes a Kubernetes environment in the cloud, Docker containers on a system for development or testing, and the master or control plane of its cloud cluster management infrastructure.</p>
  
  <h3> What's the Difference between Kubernetes and Docker? </h3>
  
<p>
Over the past few years, containers have become increasingly popular within the software development community, and they have now evolved into two major platforms — Docker and Kubernetes. Both are incredibly powerful tools that allow developers to containerize their applications, but they are also slightly different in a number of ways, with more differences on the horizon as Docker announces its new focus on Kubernetes and containers orchestration. How do you decide which one to use? What does the future hold for each? Here’s what you need to know about the difference between Docker and Kubernetes.
</p>
  
  <p> Docker is an open-source platform designed to help developers and IT professionals create, deploy, and run applications. This containerization technology is often used in conjunction with orchestration software such as Kubernetes. However, these two technologies are not interchangeable; they serve different purposes. </p>
  
  <h3> Why Should You Care About Kubernetes? </h3>
  
  <p> Kubernetes was first made available for Google's internal use for DNS hosting. Open-source software projects were not able to use it. </p>
  
  <p> Today, Kubernetes is in use by large-scale companies that use container orchestration. And in January 2019, The New Stack reported that a survey conducted in that month, which included the Kubernetes user group, discovered that Kubernetes reached more than 40,000 users and 200 companies were working on Kubernetes at that point. In addition, Gartner indicated that Kubernetes Inc. would make some $8.5 billion in 2019. </p> 
  
  <h3> What does Kubernetes Do? </h3>
  
  <p> In contrast to an overall infrastructure, Kubernetes is a dynamic layer-oriented computing infrastructure. The essence of Kubernetes is how an entire infrastructure hops! Kubernetes is a container orchestration and management platform that has built-in features for self-replication, elasticity, and scalability. Through these and more features, Kubernetes "promovi-is" for container orchestrators for both production and lab environments. </p>
  
  <h3> Kubernetes Architecture </h3>
  
  <p> Even though Kubernetes is a software platform that lets organizations manage their application workloads in containers, a traditional Kubernetes cluster may not be the best solution for a number of business needs. </p>
 
  <img src = {picture1} class="image-center" alt="Kubernetes Architecture" />
  
  <p> A cluster of virtual computing resources is only one option, and it has its drawbacks. What happens when you lose disk space (which can happen if you don't add new containers, users, or workloads to a cluster)? Do you have another cluster for redundancy, and how do you integrate the two together? </p>

  <p> Hyperconverged infrastructures like Red Hat OpenShift are an alternative that combine several technologies into a single virtual machine or physical machine. </p> 
  
  <h3> Best Practices for Kubernetes </h3>
  
<p>
Kubernetes is a container orchestration platform created by Google in 2014. It provides a way for companies to build fully self-sufficient, scalable, multi-container applications every time they need to deploy and manage their own containers. It's aimed at pretty much the same audience as Docker and other container orchestration platforms—that is, organizations that run containerized applications and want to deploy scalable, repeatable deployments.
</p>
  
<p>
At the most basic and most simplistic level, a group of containers (usually 16) is cross-linked together in a cluster, based on Docker. Containers run inside a cluster of virtual machines (Kubernetes VM) as a single Linux file system. Kubernetes organizes the creation, deletion, and management of containers into container concepts that provide fault tolerance, availability, scaling, permission management, and secure containers that should be able to run together and share resources. Each host runs one or more containers, providing the abstraction of which containers can run on which hosts.
</p>
  
<p>Since Kubernetes services are usually very easy to use, the user experience is very similar to that of centralized solutions. </p>
  
<p>With Kubernetes, businesses can make data repositories and containers, federate their resources in an efficient way, manage billing, certify capacity, quota, access rights, and more. </p>
  
<p>It can scale to many nodes simultaneously, so when their machines scale up, then their containers could scale up too. </p>
  
<h3>Kubernetes Concepts and Terminology </h3>
  
<p>Kubernetes was developed in 2014 as a Google container orchestrator, a container scheduler and more. Kubernetes was created to manage distributed applications, including Docker containers. According to SUSE, Kubernetes is simple to learn, easy to manage, and supports an on-premise, private, public, or hybrid architecture. Kubernetes is flexible enough to be split up over many servers in your data center. </p>
  
<p>This simplificator, one example of many, allows one to scale independent containers. </p>
  
<p> Let's understand better what Kubernetes is: </p>
  
<p> In an application ecosystem of operating system Docker containers, Kubernetes acts as a centralized management guided by distributed logic. Kubernetes can be used to deliver web traffic, graphics work, or IP traffic from IoT devices. The main benefit is that clusters can be easily expanded to a huge size with all functions. </p>
 
<p>
What are pods? Pods are Docker instances that you can use to deploy your containers in environments like Kubernetes, which can be private, public, or a mix of the two.
</p>
  
<p> Environments may be private services or public clouds. </p>
  
<p> Kubernetes can be used to manage containers because they are easy to use and make it easy to scale your containers. </p>
  
<p> Installation tutorials are sometimes yoinked without ever reading the help. </p>
  
<h3> RBAC and Firewall Security </h3>
  
<p>
Today, everything is hackable, and so is your Kubernetes cluster. Hackers often try to find vulnerabilities in the system in order to exploit them and gain access. So, keeping your Kubernetes cluster secure should be a high priority. The first thing to do is make sure you are using RBAC in Kubernetes. RBAC is role-based access control. Assign roles to each user in your cluster and to each service account running in your cluster. Roles in RBAC contain several permissions that a user or service account can perform. You can assign the same role to multiple people, and each role can have multiple permissions.
</p>
  
<p> RBAC settings can also be applied to namespaces, so if you assign roles to a user allowed in one namespace, they will not have access to other namespaces in the cluster. Kubernetes provides RBAC properties such as role and cluster role to define security policies. </p>
  
<p> You can create a firewall for your API server to prevent attackers from sending connection requests to your API server from the Internet. To do this, you can either use regular firewalling rules or port firewalling rules. If you are using something like GKE, you can use a master authorized network feature in order to limit the IP addresses that can access the API server. </p>
  
  <h3> Managing Kubernetes Clusters </h3>
  
<p>
Kubernetes is a project that lets you create and manage individual containers or a container cluster on a mainframe. Clusters may consist of physical, virtual, or cloud-based computing resources.
</p>
<CTA_FullWidth 
  image={ClusterImg}
  alt="Multi-Cluster Kubernetes Management with Meshery"
  content="Multi-Cluster Kubernetes Management with Meshery"
  button_text="Read blog post"
  url="/blog/meshery/multi-cluster-kubernetes-management-with-meshery"
  external_link={false}
  className="get-start-kubernetes-resource"
/>
<p>
The Kubernetes projects auto-deploy container clusters anywhere there is a pluggable environment and an open-source base that includes system-config service, service account manager, and kubelet. So, developers and system administrators can easily put containers on a single machine or on nodes of machines in any scalable cluster to save money and time.
</p>
  
<p>
Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Kubernetes was made by Google, and the Cloud Native Computing Foundation now takes care of it.
</p>

<h3>Kubernetes Cluster Visualization and Designing using Kanvas</h3>

<p>
Kanvas has been developed for visualizing and managing kubernetes clusters. You can learn more about Kanvas <a href="https://layer5.io/cloud-native-management/kanvas">here</a>
</p>

<p>Users can drag-and-drop your cloud native infrastructure using a palette of thousands of versioned Kubernetes components. Integrate advanced performance analysis into your pipeline.</p>
  <img src = {KanvasDesigner} class="image-center" alt="Kubernetes Architecture" />
<p>Users can deploy their designs, apply patterns, manage and operate their deployments in real-time bringing all the Kubernetes clusters under a common point of management. Interactively connect to terminal sessions or initiate and search log streams from your containers.</p>
  <img src = {KanvasVisualizer} class="image-center" alt="Kubernetes Architecture" />
  
  <h3> Set Resource Requests & Limits</h3>
  
  <p> Occasionally, deploying an application to a production cluster can fail due to the limited resources available on that cluster. This is a common challenge when working with a Kubernetes cluster, and it’s caused when resource requests and limits are not set. Without resource requests and limits, pods in a cluster can start utilizing more resources than required. If the pod starts consuming more CPU or memory on the node, then the scheduler may not be able to place new pods, and even the node itself may crash. Resource requests specify the minimum amount of resources a container can use. </p>

<p>
For both requests and limits, it’s typical to define CPU in millicores and memory in megabytes or mebibytes. Containers in a pod do not run if the request for resources made is higher than the limit you set.
</p>
  
<p>
In this example, we have set the limit of the CPU to 800 millicores and the memory to 256 mebibytes. The maximum request which the container can make at a time is 400 millicores of CPU and 128 mebibyte of memory.
</p>
  
  <h3> Guide to Containers </h3>
  
<p>
Containers have been around for a while, but it wasn’t until Docker came along that they really took off. In its early days, developers were using it to build their applications in containers. Now companies like Walmart are using containers to deploy their entire infrastructure.
</p>
  
<p>
Containers are lighter-weight than virtual machines because they don't need to emulate an entire operating system. This is why containers are typically faster to start up and use less resources. However, containers cannot be moved between hosts like virtual machines can, so a more robust solution may be needed for this use case.
</p>
  
<p>
Because they're so lightweight and take up less space than VMs do, containers are great for running lots of them at once! If your application needs more computing power or memory than your machine can provide on its own, using multiple containers in parallel will help balance out any resource shortages without having to invest in additional physical hardware like you would with traditional VM-based deployments.
</p>
  
  <p> As they're isolated from each other, containers are great for running multiple apps at once without worrying about them stepping all over each other's toes! This makes them perfect for things like hosting websites or email services where you want lots of different people to be able to use it at the same time without slowing down or crashing because there's not enough resources available for everyone. </p>
  
  <p> What's more, since they're so easy to spin up and take down, they're also great for testing out new ideas quickly without having to worry about making permanent changes to your system (or losing any data along the way!). So if you want to try out a new CMS but don't want to go through the trouble of installing it on your machine first, just fire up a container with it inside and see how it goes! </p>
  
<p>
One downside to using containers is that they can't easily be moved between hosts like virtual machines can, so a more robust solution may be needed for this use case. Fortunately, there are some great open source projects out there that help solve this problem!
</p>
  
  <h3> Conclusion </h3>
  
  <p> Kubernetes is a popular containerization solution that continues to see increasing adoption rates. That being said, using it successfully requires thorough consideration of your workflows and departmental best practices. </p>
  
  
</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/kubernetes/getting-started-with-kubernetes</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/getting-started-with-kubernetes</guid><dc:creator><![CDATA[Tolulope Ola-David]]></dc:creator><pubDate>Wed, 02 Nov 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/b1a646c34f1a9ed49dcf37dd7b9b4662/kubernetes-logo.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Service Mesh: Istio]]></title><description><![CDATA[import { ResourcesWrapper } from "../../Resources.style.js";
import serviceMesh from "./service-mesh.svg";
import arch from "./arch.svg";

<ResourcesWrapper>
  
<p> Microservice architectures offer some solutions while posing new ones. Application division into separate services makes scaling, updating, and development easier. It also provides you with a lot more moving pieces to connect and secure. It can get quite complicated to manage all of the network services, including load balancing, traffic management, authentication and authorisation, etc. </p>

<p> Istio, an open-source service mesh created by Google, IBM, and Lyft, enables you to connect, monitor, and secure microservices that are hosted on-premises, in the cloud, or with orchestration systems like Kubernetes and Mesos. The beta version of Istio was announced in the year 2018 in KubeCon on Google Cloud. </p> 

<p> Before moving on to what Istio is and how it works, let us look into what service meshes are and why there was an urgent need for them as microservices started getting used more. </p>

  <h3> Service Mesh </h3>
  
  <p> A service mesh is an infrastructural layer that is used to provide secure communication between different services for on-prem, cloud or multi-cloud infrastructure. It allows us to add features like observability, traffic management, and security without having to add that to our code. The term "service mesh" refers to both the kind of software you employ to carry out this pattern and the security or network domain that results from its application. </p>
  
  <p> Service meshes are divided into two parts: the control plane and the data plane. The control plane's responsibilities include securing the mesh, facilitating service discovery, doing regular health checks, enforcing policies, and handling other operational issues. A central registration of services and their corresponding IP addresses is referred to as service discovery. To share with other services how to communicate with it and to assist enforce rules on which services are allowed to communicate with which other services, the application must be registered on the control plane. </p>
  
  <p> The communication between services, on the other hand, is handled by the data plane. Because many service mesh solutions use a sidecar proxy to manage data plane connections, the amount of knowledge that the services must have about the network environment is constrained. </p>
 
  <img src = {serviceMesh} class="image-center" alt="Service Mesh" />
  
  <h3> Inside the Istio service mesh </h3>
  
  <p> A data plane and a control plane are logically separate parts of an Istio service mesh.
  <ul>
    <li> A group of intelligent proxies (Envoy) that are deployed as sidecars make up the data plane. All network connection among the microservices is mediated and managed by these proxies. Additionally, they gather and compile data on all mesh communications. </li>
    <li> The proxies are controlled and set up by the control plane to route traffic. </li>
  </ul>
  </p>
  
   <img src = {arch} class="image-center" alt="Istio Service Mesh Architecture" />
  
  <h4> Envoy </h4>
  
  <p> The data plane of Istio consists of the Envoy sidecar proxy. Envoy is an edge and service proxy that is open source and free that aids in separating network concerns from core applications. Applications don't care about the network topology; they just transmit and receive messages to and from localhost. Envoy is fundamentally a network proxy that operates at the OSI model's L3 and L4 layers. It operates by processing connections through a series of pluggable network filters. Envoy additionally provides support for an extra L7 layer filter for HTTP-based traffic. Envoy also offers excellent support for the HTTP/2 and gRPC transports. </p>
  
  <p> Many of the features provided by Istio such as security, traffic control, network resiliency are possible due to Envoy. </p>
  
  <h4> Istiod </h4> 
  
  <p> Service discovery, configuration, and certificate management are offered by Istiod. </p>
  
  <p> High level routing rules that govern traffic behavior are transformed into Envoy-specific configurations by Istiod and propagated to the sidecars during runtime. Any sidecar that complies with the Envoy API can use Pilot, which synthesizes platform-specific service discovery techniques into an abstract form. </p>
  
  <p> Istio can handle discovery in a variety of settings, including Kubernetes or virtual machines. </p>
  
  <p> To exert finer control over the traffic in your service mesh, you can ask Istiod to modify the Envoy configuration using the Traffic Management API. </p>
  
  <p> Strong service-to-service and end-user authentication are made possible by Istiod security's integrated identity and credential management. Istio can be used to enhance unencrypted service mesh traffic. </p>
  
  <p> Operators can enforce regulations with Istio based on service identity rather than on layer 3 or layer 4 network IDs, which are more prone to instability. Additionally, you can limit who has access to your services by using Istio's authorisation capability. </p>
  
  <p> In order to enable secure mTLS connection in the data plane, Istiod performs the role of a Certificate Authority (CA) and issues certificates. </p>
  
  <h3> Features </h3>
  
  <h4> Traffic Management </h4>
  
  <p> Performance is impacted by traffic routing, both within and across clusters, which improves deployment strategy. You can simply manage the flow of traffic and API requests between services using Istio's traffic routing rules. Istio makes it simple to configure critical activities like A/B testing, canary deployments, and staged rollouts with percentage-based traffic divides, as well as service-level attributes like circuit breakers, timeouts, and retries. </p>
  
  <h4> Observability </h4>
  
  <p> It becomes harder to comprehend behaviour and performance as services become more complicated. Istio produces comprehensive telemetry for each communication taking place within a service mesh. This telemetry makes service activity observable, enabling operators to maintain, optimise, and debug their applications. Even better, you can implement practically all of this instrumentation without making any changes to your applications. Operators are able to fully comprehend how the monitored services are communicating with Istio. </p>
  
  <p> Detailed metrics, distributed traces, and complete access logs are all included in Istio's telemetry. You get complete and thorough service mesh observability with Istio. </p>
  
  <h4> Security Capabilities </h4>
  
  <p> Particular security requirements for microservices include defense against man-in-the-middle attacks, adaptable access rules, auditing tools, and mutual TLS. Istio comes with a comprehensive security solution that enables administrators to handle each of these problems. To safeguard your services and data, it offers strong identity, strong policy, transparent TLS encryption, and authentication, authorization, and audit (AAA) tools. </p>
  
  <p> The security architecture used by Istio is built on security-by-default, and it aims to provide in-depth defense so you may deploy security-conscious apps even across networks with a low level of trust. </p>
  
</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/service-mesh/service-mesh-istio</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/service-mesh-istio</guid><dc:creator><![CDATA[Deepesha Burse]]></dc:creator><pubDate>Wed, 31 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/731763d720780a49c2ffdfede8c28f4b/istio.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[DevOps Adoption: Identifying the Right Metrics]]></title><description><![CDATA[
import { Link } from "gatsby";
import { ResourcesWrapper } from "../../Resources.style.js";
import DevOps from "./devops-adoption-choosing-the-right-metrics.pdf";
import DevOpsAdoption from "./devops-adoption.webp";

<ResourcesWrapper>
<p>
According to Puppet’s State of DevOps Report 2021, 83% of IT professionals report that their organizations have previously implemented DevOps practices or are doing so right now to unlock higher business value, achieve faster time to delivery, and boost security of systems.
</p>

<p>
However, DevOps teams from many industries frequently struggle to identify the right metrics to monitor and measure success. In this <Link to={DevOps}>infographic</Link>, we highlight the metrics all DevOps professionals should measure to:
</p>

<ul>
<li>Identify places in the pipeline to speed up deployments.</li>
<li>Make data-driven decisions to improve the deployment process.</li>
<li>Analyze the speed at which products are reaching the market in comparison to competitors.</li>
</ul>

<h3 style="margin-top: 1rem;">Monitor these 5 metrics to understand how to speed up your DevOps toolchain:</h3>
<ul>
<li>Deployment Time</li>
<li>Change Failure Rate</li>
<li>Recovery Time</li>
<li>Release Cadence</li>
<li>Lead Time</li>
</ul>

<Link to={DevOps}>
<img src={DevOpsAdoption} alt="Right metrics for adopting DevOps" />
</Link>
</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/devops/devops-adoption-identifying-the-right-metrics</link><guid isPermaLink="false">https://layer5.io/resources/devops/devops-adoption-identifying-the-right-metrics</guid><pubDate>Tue, 23 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/eba4e5898081df468c1c4288ce73623d/devops-adoption.webp" length="0" type="image/webp"/><content:encoded></content:encoded></item><item><title><![CDATA[What is GitOps?]]></title><description><![CDATA[
import { ResourcesWrapper } from "../../Resources.style.js";

<ResourcesWrapper>
<p>
    GitOps revolves around the central notion that infrastructure can be treated as code. It is an operational framework that incorporates DevOps best practices for infrastructure automation, including version control, collaboration, compliance, and CI/CD tooling, which are often used for application development. Like code, not only can you store your infrastructure configuration in a source code version system, but you can also take your infrastructure configuration and any changes to its configuration through the same change management process that you do when updating your applications and services. In part, GitOps is about change management, and consequently, it is about risk reduction and risk management. When you automate a process and classify the manner in which you systemize the process, risk is reduced through the consistency and series of processes and reviews changes go through.
</p>
<p> 
   GitOps is the acknowledgement that declarative systems that everything is (or should be) defined as code. With all code in a source code system, that system becomes the source of truth and in the system of record for how your infrastructure is running. Well, that is, assuming that your infrastructure configuration hasn't drifted from its desired state defined in your source code system. If Git is the source of truth, you cannot run operations manually by executing random commands. Doing so would mean that Git would stop being the only source of truth. Instead, the only goal of operations is to define the desired state as code and store it in git. Then, let the machines synchronize that with the actual state. Such synchronization must be continuous so that the two states are (almost) always in sync. In other words, GitOps is about defining everything as code, storing that code in Git, and letting the machines detect the drift between the desired and the actual state – and making sure that drifts are resolved as soon as possible, hence resulting in the two states being almost always in sync.
</p>

<h2> Principles of GitOps</h2>
 
<h3> 1) Declarative</h3>

 <p> According to this principle, the entire system should have a declarative description. Let us first understand what a system description is. What is committed to your Git repository is called the System Description. One or more files that define each system component and its state will be included in the system description. According to GitOps, the way in which we store those definitions is crucial, and we must do so declaratively. That implies that the description of our system will be saved as data. 
</p>
 
 <p> In the declarative approach, we specify how we want the system to look not how we can achieve that state. If we want to make any changes, we change the description instead of the series of steps to get there. Declarative configuration is critical for GitOps because it provides a description of the system that an automated agent can understand and utilize to take action. 
 </p>

<h3> 2) Single Source of Truth</h3>

 <p> The second principle mandates that we keep that system description inside of Git. Therefore, we decide to maintain the official blueprints, which outline the ideal system state version in Git. A git commit is required if we wish to modify the blueprint. The blueprint can also be called the desired state. This helps developers, testers, operations, security, and automations to have a single reference and keep uniformity in everyone’s vision. 
 </p>
 
 <p> GitOps also improves a system's ability to recover from failure because it's simple to roll back an unsuccessful change or restore the entire system from the repository.
</p>

<h3> 3) Automated Change Delivery</h3>

 <p> Only automation allows us to apply modifications made to the blueprint to systems already in operation. Delivery of changes is entirely automatic. GitOps doesn't allow manual editing. Because standard workflows only need GitHub, which is such a well-known platform, automation enables changes to be delivered through simpler for developers to use workflows. Additionally, automation standardizes your delivery processes, improving the predictability and consistency of system operations. 
 </p>

<h3> 4) Automated State Control</h3>

 <p> The fourth principle uses automation to keep our operating system in alignment with the desired state. Drift is the deviation of the runtime state of our system from the desired state. The system's blueprints and what is actually operating in the system don't match. Therefore, if the operating system drifts from what we have specified in Git, an operator will restore it by bringing it back to the intended condition. 
</p>

<h2> Benefits of GitOps</h2>

<h3> 1) Improves compliance and security:</h3>

 <p> Since teams use a single platform for infrastructure management, a streamlined toolchain reduces attack surfaces. Teams can use the version control system to roll back to a desired state in the event of an assault. GitOps lessens outages and downtime as a result, allowing teams to continue working on projects in a secure environment.
 </p>
 
 <h3> 2) Boosts productivity and cooperation:</h3>

 <p> GitOps includes CI/CD pipelines, Git workflows, and infrastructure as code best practices for software development. These prerequisite tools, knowledge, and skill sets are already present in operations teams, thus adopting GitOps won't need a steep learning curve. GitOps workflows streamline procedures in order to improve visibility, establish a single source of truth, and have a small number of tools on hand.
</p>

 <h3> 3) Automation enhances developer efficiency and lowers costs:</h3>

<p> Productivity rises with CI/CD tooling and continuous deployment since teams can concentrate on development rather than laboriously manual processes thanks to automation. Since team members can use any language and tools they like before pushing updates to GitHub, GitOps workflows enhance the developer experience. Infrastructure automation increases output and decreases downtime while enabling better cloud resource management, which can also save costs.
</p>
 
 <h3> 4) Increases stability and reliability:</h3>

 <p> Human mistake is decreased through infrastructure that is codified and repeatable. Code reviews and collaboration are made easier by merge requests, which also assist teams in finding and fixing issues before they are released to the public. Additionally, there is less risk because all infrastructure changes are tracked through merge requests and may be undone if an iteration is unsuccessful. By allowing rollbacks to a more stable state and providing distributed backup copies in the event of a significant outage, Git processes speed up recovery time. GitOps gives teams the freedom to iterate more quickly and release new features without worrying about creating an unstable environment.
</p>
 
 <h3> 5) Faster development and deployment:</h3>

 <p> GitOps provides quicker and more frequent deployments, making it easier for teams to make a minimum viable change. Teams can ship many times per day and roll back changes if there is a problem by utilizing GitOps best practices. Team members can offer business and customer value more quickly thanks to high velocity deployments. Teams are more flexible and able to react to customer needs more quickly with continuous integration.
 </p>
 
 <h2> Key Components of a GitOps workflow</h2>

 <p> To summarize, the following are the four components we require to a GitOps workflow:</p>
   <ol>
      <li> Git repository: The code and configuration of the application are verified there. </li>
      <li> CD pipeline: It is responsible for building, testing, and deploying the application. </li>
      <li> Application deployment tool: It is employed to manage the target environment's application resources. </li>
      <li> Monitoring system: It keeps tabs on the performance of the application and gives the development team feedback. </li>
   </ol>

</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/cloud-native/what-is-gitops</link><guid isPermaLink="false">https://layer5.io/resources/cloud-native/what-is-gitops</guid><pubDate>Tue, 16 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/a8d747801f0e266dbc9bb2b192cd3dc1/github-dark.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Service Mesh: Consul]]></title><description><![CDATA[import { ResourcesWrapper } from "../../Resources.style.js";
import serviceMesh from "./consul-service-mesh.webp";
import agent from "./consul-agent-architecture.webp";
import datacenter from "./datacenter-architecture.webp";
import proxy from "./service-proxy-architecture.webp";

<ResourcesWrapper>
  
  <h3> What is a Service Mesh? </h3>
  
  <p> A service mesh is a dedicated layer that provides secure service-to-service communication for on-prem, cloud, or multi-cloud infrastructure. Although service meshes are typically used with a microservice architectural pattern, they are useful in any situation involving complex networking. Their functionalities include traffic control, resiliency, observability and security. Traffic steering is used for content and it allows optimal usage of our resources. Service meshes provide control over chaotic situations (which usually arise in complex networks) along with proper identification and policies to enhance security. </p>
  
  <p> Service meshes can be divided into the control plane and the data plane. The role of the control plane is to secure the mesh, facilitate service discovery, conduct frequent health checks, enforce policies and other operational concerns. Service discovery refers to a central registry of the services and their respective IP addresses. The application needs to be registered on the control plane for it to be able to share with other services how to communicate with it and helps to enforce rules on which service gets to communicate with which other services. </p>
  
  <p> The data plane, on the other hand, handles the communication between services. The amount of knowledge that the services need to have about the network environment is limited by the fact that many service mesh solutions use a sidecar proxy to conduct data plane connections. </p>
  
  <img src = {serviceMesh} class="image-center" alt="Service Mesh" />
  
  <h3> What is Consul? </h3>
  
  <p> Consul Service Mesh (also known as Consul Connect) provides service-to-service connection authorization and encryption using mutual Transport Layer Security (TLS). Consul is the control plane of the service mesh. Consul can be used with Virtual Machines (VMs), containers, or with container orchestration platforms such as Nomad and Kubernetes. Applications can use sidecar proxies to establish TLS connections for inbound and outbound connections or natively integrate with Connect by using Connect aware SDKs for optimal performance and security. </p>
  
  <p> It is a multi-networking tool that provides a fully functional service mesh solution to address the networking and security issues associated with running cloud infrastructure and microservices. Consul offers a software technique for segmentation and routing. It also offers advantages such as handling failures, retries, and network observability. You can utilize any of these characteristics alone as required or combine them to create a full service mesh and achieve zero trust security. </p>
  
  <h3> Architecture </h3>
  
  <p> Consul is a distributed system built for a node cluster to operate on. A physical server, cloud instance, virtual machine, or container can all function as a Consul node. The collection of interconnected nodes that Consul runs on is known as a datacenter. Consul supports multiple datacenters and considers this as a common case. It is expected that there will be many clients and three to five servers in a datacenter. This creates a balance between performance and availability in the event of a breakdown because consensus slows down as more machines are added. The number of clients, however, is unlimited and can easily increase to thousands or tens of thousands. </p>
  
  <img src = {datacenter} class="image-center" alt="Image of datacenter" />
  
  <p> The Consul Agent is responsible for maintaining membership information, registering services, running checks, responding to queries, etc. It is required to run on every node that is a part of the Consul cluster. In some places, client agents may cache data from the servers to make it available locally for performance and reliability. They can either run in server mode or client mode. Client nodes make up for most of the cluster and are lightweight processes. They act as an interface between server nodes for most operations. They run on every node where services are running. </p>
  
  <p> Along with core agent operations, a server node participates in the consensus quorum. The Raft protocol, which offers excellent consistency and availability in the event of failure, serves as the foundation for the quorum. Because they consume more resources than client nodes, server nodes should run on dedicated instances. </p>
  
  <img src = {agent} class="image-center" alt="Consul Agent" />
  
  <p> A per-service proxy sidecar manages incoming and outgoing service connections by automatically wrapping and verifying TLS connections. Consul includes its own built-in L4 proxy and has first class support for Envoy. Other than this, we can choose to use any other proxy to plug in as well. The following diagram shows how proxies work: </p>
  
  <img src = {proxy} class="image-center" alt="Side-car proxy" />
  
  <p> The lifecycle of a Consul cluster:
    <ol>
      <li> An agent is started. </li>
      <li> An agent joins the cluster. </li>
      <li> Information of the agent is communicated throughout the cluster</li>
      <li> Existing servers will begin replicating to the new node. </li>
    </ol>    
  </p>
  
  <h3> Benefits and Compatibility of Consul Connect </h3>
  
  <p> New methods of networking are necessary due to the development of cloud infrastructure and microservices designs. There are numerous tools and companies, all of which make different attempts to address the issue. The Consul service mesh solution offers a pure software approach with an emphasis on simplicity and wide compatibility and makes no assumptions about the underlying network. </p>
  
  <p> Consul service mesh streamlines application deployment into a zero-trust network and makes service discovery easier in complex networking situations. </p>
  
  <p> Features of Consul Service Mesh:
  <br />
    <ol>
      <li> Service Discovery 
        <p> Consul provides a service catalog, configurable service routing, health checks, automatic load balancing, and geo-failover across multiple instances of the same service. The capacity to control changes in the service landscape of your network becomes essential when new versions of a service are introduced and must coexist with existing instances of the same application, frequently running on different versions. The agent provides a simple service definition format to declare the availability of a service and to potentially associate it with a health check. </p> 
      </li>
      <li> Zero-trust Security Model
        <p> Trust can be exploited and with the increasing number of services, there are higher chances of breach. The Consul service mesh control plane can be configured to enforce mutual TLS (mTLS), and will automatically generate and distribute the TLS certificates for every service in the mesh. The certificates are used for both service identity verification and communication encryption. </p> 
      </li>
      <li> Simplify Application Security with Intentions 
        <p> Communication between services is secure within the mesh once the service sidecar proxies have been set up. To designate which services are permitted to communicate with one another, you might want to build a more granular set of policies. Consul Intentions are used to limit which services can make requests or create connections and define access control for services through Connect. We can manage intentions via the UI, CLI, or API. The proxy or a natively integrated application enforces intentions on inbound connections or requests. </p> 
      </li>
    </ol>
  </p>
  
  <p> Compatibility of Consul Connect:
  <br />
  <ol>
    <li> First-Class Kubernetes Support 
        <p> By offering an official Helm chart for installing, configuring, and upgrading Consul on Kubernetes, Consul enables first-class Kubernetes support. The chart automates Kubernetes's Consul service mesh installation and configuration. </p> 
     </li>
    <li> Platform Agnostic and Multi-Cluster Mesh
        <p> Consul works with all cloud providers and architectures. You can expand the scope of your Kubernetes clusters to include services that aren't run using Kubernetes by using the service catalog sync and auto-join features. In order to facilitate safe service-to-service communication between Nomad tasks and jobs, Consul additionally interfaces with HashiCorp Nomad. </p> 
     </li>
  </ol>
  </p>
  
</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/service-mesh/service-mesh-consul</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/service-mesh-consul</guid><dc:creator><![CDATA[Deepesha Burse]]></dc:creator><pubDate>Fri, 05 Aug 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/ed21c2c53f2c64e86b016cfdfe7018ae/consul.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Managing Containers]]></title><description><![CDATA[
import { Link } from "gatsby";
import { ResourcesWrapper } from "../../Resources.style.js";

<ResourcesWrapper>
<div className="intro">
  <p>Learn more about managing containers with our <a className="blog" href="https://github.com/layer5io/containers-101-workshop">Containers 101 Workshop</a>. Walk-through four hands-on exercises with Docker.</p>
</div>

<p>
  Container management refers to a set of practices that govern and maintain
  containerization software. Container management tools automate the creation,
  deployment, destruction and scaling of application or systems containers.
  Containerization is an approach to software development that isolates
  processes that share an OS kernel -- unlike virtual machines (VMs), which
  require their own -- and binds application libraries and dependencies into one
  deployable unit. This makes containers lightweight to run, as they require
  only the application configuration information and code from the host OS. This
  design also increases interoperability compared to VM hosting. Each container
  instance can scale independently with demand.
</p>
<p>
  Modern Linux container technology was popularized by the Docker project, which
  started in 2013. Interest soon expanded beyond containerization itself, to the
  intricacies of how to effectively and efficiently deploy and manage
  containers.
</p>
<p>
  In 2015, Google introduced the container orchestration platform Kubernetes,
  which was based on its internal data center management software called Borg.
  At its most basic level, open source Kubernetes automates the process of
  running, scheduling, scaling and managing a group of Linux containers. With
  more stable releases throughout 2017 and 2018, Kubernetes rapidly attracted
  industry adoption, and today it is the de facto container management
  technology.
</p>
<p>
  IT teams use containers for cloud-native, distributed -- often microservices-
  based -- applications, and to package legacy applications for increased
  portability and efficient deployment. Containers have surged in popularity as
  IT organizations embrace DevOps, which emphasizes rapid application
  deployment. Organizations can containerize application code from development
  through test and deployment.
</p>
<h2>Benefits of container management</h2>
<p>
  The chief benefit of container management is simplified management for
  clusters of container hosts. IT admins and developers can start, stop and
  restart containers, as well as release updates or check health status, among
  other actions. Container management includes orchestration and schedulers,
  security tools, storage, and virtual network management systems and
  monitoring.
</p>

<h3>Wrangling container sprawl</h3>
<p>
  Organizations can set policies that ensure containers share a host -- or
  cannot share a host -- based on application design and resource requirements
  For example, IT admins should colocate containers that communicate heavily to
  avoid latency. Or, containers with large resource requirements might require
  an anti-affinity rule to avoid physical storage overload. Container instances
  can spin up to meet demand -- then shut down -- frequently. Containers also
  must communicate for distributed applications to work, without opening an
  attack surface to hackers.
</p>
<p>
  A container management ecosystem automates orchestration, log management,
  monitoring, networking, load balancing, testing and secrets management, along
  with other processes. Automation enables IT organizations to manage large
  containerized environments that are too vast for a human operator to keep up
  with.
</p>

<h2>Challenges of container management</h2>
<p>
  One drawback to container management is its complexity, particularly as it
  relates to open source container orchestration platforms such as Kubernetes
  and Apache Mesos. The installation and setup for container orchestration tools
  can be arduous and error prone. IT operations staff need container management
  skills and training. It is crucial, for example, to understand the
  relationships between clusters of host servers as well as how the container
  network corresponds to applications and dependencies.
</p>
<p>
  Issues of persistence and storage present significant container management
  challenges. Containers are ephemeral -- designed to exist only when needed.
  Stateful application activities are difficult because any data produced within
  a container ceases to exist when the container spins down.
</p>
<p>
  Container security is another concern. Container orchestrators have several
  components, including an API server and monitoring and management tools. These
  pieces make it a major attack vector for hackers. Container management system
  vulnerabilities mirror standard types of OS vulnerabilities, such as those
  related to access and authorization, images and intercontainer network
  traffic. Organizations should minimize risk with security best practices --
  for example, identify trusted image sources and close network connections
  unless they're needed.
</p>
<h2>Container management strategy</h2>
<p>
  Forward-thinking enterprise IT organizations and startups alike use containers
  and container management tools to quickly deploy and update applications. IT
  organizations must first implement the correct infrastructure setup for
  containers, with a solid grasp of the scope and scale of the containerization
  project in terms of business projections for growth and developers'
  requirements. IT admins must also know how the existing infrastructure's
  pieces connect and communicate to preserve those relationships in a
  containerized environment. Containers can run on bare-metal servers, VMs or in
  the cloud -- or in a hybrid setup -- based on IT requirements.
</p>
<p>
  In addition, the container management tool or platform should meet the
  project's needs for multi-tenancy; user and application isolation;
  authentication; resource requirements and constraints; logging, monitoring and
  alerts; backup management; license management; and other management tasks. IT
  organizations should understand their hosting commitment and future container
  plans, such as if the company will adopt multiple cloud platforms or a
  microservices architecture.
</p>
<h2>Kubernetes implementation considerations</h2>
<p>
  As described above, containers are arranged into pods in Kubernetes, which run
  on clusters of nodes; pods, nodes and clusters are controlled by a master. One
  pod can include one or multiple containers. IT admins should carefully
  consider the relationships between pods, nodes and clusters when they set up
  Kubernetes.
</p>
<p>
  Organizations should plan their container deployment based on how many pieces
  of the application can scale under load -- this depends on the application,
  not the deployment method. Additionally, capacity planning is vital for
  balanced pod-to-node mapping, and IT admins should ensure high availability
  with redundancy with master node components.
</p>
<p>
  IT organizations can address container security concerns by applying some
  general IT security best practices to containerization. For example, create
  multiple security layers throughout the environment, scan all container images
  for vulnerabilities, enforce signed certificates and run the most up-to-date
  version of any container or application image. Containers introduce the
  benefits of an immutable infrastructure methodology as well; the regular
  disposal and redeployment of containers, with their associated components and
  dependencies, improves overall system availability and security. Additionally,
  Kubernetes multi-tenancy promises greater resource isolation, but recently
  revealed security vulnerabilities make multicluster management preferred for
  now.
</p>
<p>
  Networking is another significant factor. Kubernetes networking occurs within
  pods, between pods and in user-to-containerized resource connections.
  Kubernetes enables pods and nodes to communicate without address translation,
  allocating subnets as necessary. Lastly, IT admins working with Kubernetes
  should prepare to troubleshoot common container performance problems,
  including those caused by unavailable nodes and noisy neighbors, in an
  implementation.
</p>

</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/kubernetes/managing-containers</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/managing-containers</guid><pubDate>Wed, 06 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/70f4c7f444e8b3494ddc0fb955f86d40/docker.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Kubernetes Architecture 101]]></title><description><![CDATA[
import { Link } from "gatsby";
import ArchDiagram from "./kubernetes-highlevel-architecture.webp";
import { ResourcesWrapper } from "../../../Resources.style.js";

<ResourcesWrapper>

The way Kubernetes is architected is what makes it powerful. Kubernetes has a basic client and server architecture, but it goes way beyond that. Kubernetes has the ability to do rolling updates, it also adapts to additional workloads by auto scaling nodes if it needs to and it can also self-heal in the case of a pod meltdown. These innate abilities provide developers and operations teams with a huge advantage in that your applications will have little to no down time. In this section we provide a brief overview of the master and its worker nodes with a high level overview of how Kubernetes manages workloads.

<div className="right" >
<img src={ArchDiagram} alt="Simple Kubernetes Architecture Diagram" />
<i>Simple Kubernetes Architecture Diagram</i>
</div>

# Kubernetes Components

Let's dive into each of the Kubernetes components, starting with the Master node.

## Kubernetes Master

The Kubernetes master is the primary control unit for the cluster. The master is responsible for managing and scheduling the workloads in addition to the networking and communications across the entire cluster. The master node is responsible for the management of Kubernetes cluster. This is the entry point of all administrative tasks. The master node is the one taking care of orchestrating the worker nodes, where the actual services are running.

These are the components that run on the master:

### Etcd Storage
Etcd is an open-source key-value data store that can be accessed by all nodes in the cluster. It stores configuration data of the cluster’s state. etcd is a simple, distributed, consistent key-value store. It’s mainly used for shared configuration and service discovery.

It provides a REST API for CRUD operations as well as an interface to register watchers on specific nodes, which enables a reliable way to notify the rest of the cluster about configuration changes.

An example of data stored by Kubernetes in etcd is jobs being scheduled, created and deployed, pod/service details and state, namespaces and replication information, etc.

### Kube-API-Server 
Kube-API-Server manages requests from the worker nodes, and it receives REST requests for modifications, and serves as a front-end to control cluster. The API server is the entry points for all the REST commands used to control the cluster. It processes the REST requests, validates them, and executes the bound business logic. The result state has to be persisted somewhere, and that brings us to the next component of the master node.


### Kube-scheduler 
Kube-scheduler schedules the pods on nodes based on resource utilization and also decides where services are deployed. The deployment of configured pods and services onto the nodes happens thanks to the scheduler component. The scheduler has the information regarding resources available on the members of the cluster, as well as the ones required for the configured service to run and hence is able to decide where to deploy a specific service.

### Kube-controller-manager
Kube-controller-manager runs a number of distinct controller processes in the background to regulate the shared state of the cluster and perform routine tasks. When there is a change to a service, the controller recognizes the change and initiates an update to bring the cluster up to the desired state. Optionally you can run different kinds of controllers inside the master node. controller-manager is a daemon embedding those.

A controller uses apiserver to watch the shared state of the cluster and makes corrective changes to the current state to change it to the desired one.
An example of such a controller is the Replication controller, which takes care of the number of pods in the system. The replication factor is configured by the user, and it's the controller’s responsibility to recreate a failed pod or remove an extra-scheduled one. Other examples of controllers are endpoints controller, namespace controller, and serviceaccounts controller, but we will not dive into details here.

## Worker Nodes
These nodes run the workloads according the schedule provided by the master. The interaction between the master and worker nodes are what’s known as the control plane. The pods are run here, so the worker node contains all the necessary services to manage the networking between the containers, communicate with the master node, and assign resources to the containers scheduled.

### Kubelet
Kubelet ensures that all containers in the node are running and are in a healthy state.  If a node fails, a replication controller observes this change and launches pods on another healthy pod. Integrated into the kubelet binary is ‘cAdvisor` that auto-discovers all containers and collects CPU, memory, file system, and network usage statistics and also provides machine usage stats by analyzing the ‘root’ container. 

Kubelet gets the configuration of a pod from the apiserver and ensures that the described containers are up and running. This is the worker service that’s responsible for communicating with the master node. It also communicates with etcd, to get information about services and write the details about newly created ones.

### Kube Proxy
Kube Proxy acts as a network proxy and a load balancer for a service on a single worker node. . It takes care of the network routing for TCP and UDP packets. It forwards the request to the correct pods across isolated networks in a cluster. 

### Pods
A pod is the basic building block on Kubernetes. It represents the workloads that get deployed. Pods are generally collections of related containers, but a pod may also only have one container. A pod shares network/storage and also a specification for how to run the containers.

### Containers 
Containers are the lowest level of microservice. These are placed inside of the pods and need external IP addresses to view any outside processes. Docker is not the only supported container runtime, but is by far, the most popular. Docker runs on each of the worker nodes, and runs the configured pods. It takes care of downloading the images and starting the containers.

### kubectl
Kubectl is a command line tool to communicate with the API service and send commands to the master node. kubectl must be configured to communicate with your cluster. If you have multiple clusters, you might try using kubectx, which makes switching between contexts easy.


#### Managing objects with kubectl
You can divide a Kubernetes cluster into multiple environments by using namespaces (e.g., Dev1, Dev2, QA1, QA2, etc.), and each environment can be managed by a different user. One of the inconveniences of writing kubectl commands is that every time you write a command, you need the --namespace option at the end. People often forget this and end up creating objects (pods, services, deployments) in the wrong namespace. 

With this trick, you can set the namespace preference before running kubectl commands. Run the following command before executing the kubectl commands, and it will save the namespace for all subsequent kubectl commands for your current context:

```
kubectl config set-context $(kubectl config current-context) --namespace=mynamespace
```

</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/kubernetes/kubernetes-architecture-101</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/kubernetes-architecture-101</guid><pubDate>Tue, 05 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/9287b4a708bb510f64057ea305498b77/kubernetes.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[What is Multi-Cluster Kubernetes?]]></title><description><![CDATA[
import { Link } from "gatsby";

import { ResourcesWrapper } from "../../Resources.style.js";

Learn more about how to wrangle <Link to= "/blog/meshery/multi-cluster-kubernetes-management-with-meshery">Multiple Kubernetes clusters with Meshery.</Link>

<ResourcesWrapper>

Developers who work in fast-paced environments face the risk of infrastructure sprawl in their VMs or servers. Even with the rise in containerized deployments on Kubernetes and other platforms, admins still must determine how to efficiently manage hundreds and thousands of clusters for various projects.

Common concerns for an organization’s project deployments include how to run multiple workloads and whether a cluster is large enough to handle the work.

A Kubernetes multi-cluster setup can solve these problems. Multi-cluster architecture is a strategy for spinning up several clusters to achieve better isolation, availability, and scalability. In this type of implementation, an application’s infrastructure is distributed and maintained across multiple clusters. Because this strategy can also make cluster management more difficult, it needs to be handled properly.

## What Is a Kubernetes Multi-Cluster Setup?

Kubernetes works with clusters to efficiently run and manage workloads.

In Kubernetes multi-cluster orchestration, platforms such as managed services help you to run workloads across multiple clusters and environments. The multiple clusters can be configured within a single physical host, within multiple hosts in the same data center, or even in a single cloud provider across different regions. This allows you to provision your workloads in several clusters, rather than just one.

This type of deployment enables more scalability, availability, and isolation for your workloads and environments. It also enables you to better coordinate the planning, delivery, and management of these environments.

A key feature of multi-cluster Kubernetes architecture is that each cluster is highly independent, managing its internal state for maximum resource provisioning and service configuration.

## Why Use a Kubernetes Multi-Cluster Setup?

There are multiple use cases for a multi-cluster deployment. You can use it to deploy workloads spanning multiple regions for increased availability, eliminate cloud blast radius, prevent compliance issues, and enforce security around your clusters and tenants.

As your environment grows, so do the potential issues you need to solve in order to align your cluster maintenance with your business needs. Using a Kubernetes multi-cluster setup can help with the following concerns.

## Cluster Discovery and Tenant Isolation

It is common for projects to exist in dev, staging, and production environments. To achieve this kind of isolation, you require multiple Kubernetes environments.

Conventionally, using namespaces would be enough for discovery and isolation in a single cluster, but Kubernetes isn’t a direct multitenant system. Namespaces are also not great for isolation since any compromise in the namespace means that your cluster is also compromised. Additionally, badly configured applications in a namespace can consume more resources than expected, which impacts other applications in the cluster.

Kubernetes multi-cluster environments enable you to isolate users and projects by cluster, simplifying the process.

## Failover

Architecting multi-cluster workloads minimizes the downtime issues common within a single cluster, because you can freely transfer the workloads to other running clusters.

# Multi-Cluster, Multitenancy, or a Mix?

Kubernetes is a complex, high-level platform that offers multiple options for your deployments: single server, multitenant, or multi-cluster.

Multitenancy means a cluster is shared among several workloads, or tenants. Multiple users share the same cluster resources and control plane. Multitenant clusters require fair allocation of resources to the tenants as well as isolation of tenants from each other, in order to minimize the effects of a faulty tenant on other tenants and the overall cluster.

A multi-cluster setup, on the other hand, involves several clusters deployed across one or many data centers. This type of deployment can be used to separate development and production. It improves availability and enhances security around workloads.

The best choice for your organization depends on factors that include the technical expertise of your team, your infrastructure availability, and your budget. Many organizations separate their critical production services from non-critical services by placing them in separate tenants across tiers, teams, locations, or infrastructure providers. Projects that are time- and resource-dependent (where resources are spun up and down on the go) are, however, suitable for multi-cluster architecture.

# When to Use a Multi-Cluster Setup

To decide whether your projects would function best in a multi-cluster deployment, you first need to define your goals.

You should know the challenges you are trying to solve and how transitioning to a multi-cluster setup would help your organization. Projects that are performance-dependent with workloads that are sensitive to factors like latency can take advantage of the high availability and isolation available in multi-cluster setups. In other words, you can run workloads with intensive computations that don’t need to share resources.

You’ll need to collect workload data and other feedback from your various teams before making a decision. You should assess your teams’ expertise: are they well-versed in provisioning single clusters, even before transitioning to multi-clusters? You’ll also need to evaluate your business model and how such an infrastructure transition could affect your users or customers.

The following are some of the advantages of transitioning to a Kubernetes multi-cluster setup.

<ul>
<li>Tenant Isolation</li>

You might want to establish order while accommodating your development teams. The multi-cluster architecture allows workload isolation. For example, you could spin up separate clusters for staging and production.

With multiple clusters, any tenant configuration changes affect only that specific cluster. This way, cluster admins can easily identify issues, run new feature experiments, and carry out workload shifts without troubling other tenants and clusters.

<li>No Single Point of Failure</li>

Running a single cluster can expose your project to a single point of failure, in
which one malfunctioning component can bring down an entire system. Using a multi-cluster
environment enables you to shift your workloads between clusters so that your projects
continue to function if one cluster is down or even disappears entirely.

<li>No Vendor Lock-In</li>

There are multiple third-party cloud vendors available with varying resource offerings. Because of evolving resource pricing and models, organizations change their usage models over time as well. A Kubernetes multi-cluster setup ensures your workloads are cloud-agnostic so that you can safely use multiple vendors or move workloads from one cloud to another.
</ul>

Kubernetes provisions clusters that run and manage our workloads. Depending on the needs of an organization, Kubernetes deployments can be replicated to have the same workloads accessible across multiple nodes and environments. This concept is called Kubernetes multi-cluster orchestration. It’s simply provisioning your workloads in several Kubernetes clusters (going beyond a single cluster).

A Kubernetes multi-cluster defines deployment strategies to introduce scalability, availability, and isolation for your workloads and environments. A Kubernetes multi-cluster is fully embraced when an organization coordinates the planning, delivery, and management of several Kubernetes environments using appropriate tools and processes.

## Why Do You Need a Kubernetes Multi-Cluster?

In simple deployment cases, Kubernetes can spin workloads in a single cluster. However, some cases need advanced deployment models, and for such scenarios, a multi-cluster architecture is suitable and can improve the performance of your workloads.

Simply put, a development team may need a Kubernetes multi-cluster to handle workloads spanning regions, eliminate a cloud blast radius, manage compliance requirements, solve multi-tenancy conflicts, and enforce security around clusters and tenants.

### Cluster Upgrades and Security Management

Teams that rely heavily on Kubernetes for deployments need to plan for regular upgrades and patches on their environments for comprehensive security fixes.

Running cluster upgrades without due care or proper tools can break more things, and more so when dependent resources are overloaded. Tools like kOPs and Cluster APIs can therefore be used to apply upgrades to your running clusters.

The tools that you install to run your clusters depend entirely on the workloads that your clusters support. How you upgrade a cluster and its tools also depends on how you initially deployed and ran the Kubernetes cluster, that is, whether you’re using a hosted Kubernetes provider or some other means for deployment. Most hosted providers support and handle automatic upgrades, which relieves developers from manual upgrades and patching.

Upgrading a cluster and its toolset follows the approach of upgrading the control plane first, then the nodes in a cluster, followed by upgrading clients such as `kubectl`.

### Managing Kubernetes Multi-Cluster Complexity

The complexity of management tasks across multiple Kubernetes clusters greatly increases your the number of clusters increase. You need higher-level view and control as you manage workloads across clusters; need to be able to simply switch between clusters; you need a management plane.

<p><Link className="blog" to="/meshery">  Meshery </Link>is the open source, cloud native management plane that enables the adoption,
operation, and management of Kubernetes, any service mesh, and their workloads.</p>

MeshSync, a custom controller managed by Meshery Operator, uniquely contains cluster-wide details of all objects across any number of managed clusters separated by Kubernetes Cluster ID.

### Deprovisioning Clusters That Are No Longer Needed

When you deprovision a cluster, its running resources are also deleted. The control plane resources, the node instances, pods, and stored data are all deleted.

Different hosted Kubernetes providers have varying ways of deleting Kubernetes clusters. For instance, GKE supports deletion of clusters from the Google Cloud CLI and Cloud Console. Other tools for spinning Kubernetes clusters such as kOps and Amazon EKS also support the deletion from their CLIs and consoles.

Suppose you have provisioned your clusters with the Google Kubernetes Engine; you can run the following command in the gcloud CLI to deprovision your clusters that are no longer needed:

```
gcloud container clusters delete CLUSTER_NAME
```

At this point, you’ve seen the operations around managing a cluster lifecycle, that is, creation, deletion, and upgrading of clusters.

## Conclusion

Teams want working with clusters to be as easy as possible. This ease in operating clusters can be ensured by managing the cluster lifecycle. In this article, you learned what’s involved in managing a cluster lifecycle. You’ve seen how clusters are created at scale using various tools. You’ve also seen what cluster upgrades and security patch management involve while trying to maintain the health of your clusters.

The complexity of Kubernetes environments does present challenges, but setting clear goals and objectives for deploying your clusters can help you overcome any obstacles as your organization makes the transition.

Finally, multi-cluster deployments are a good choice for organizations that are building highly distributed systems, with geographic and regulatory control in check to help scale workloads beyond the limits of single clusters. Multi-cluster deployment and management is useful for minimizing exposure of production services, preventing access to sensitive data in environments like development and testing. Organizations are now opting to deploy their more critical workloads on separate multiple clusters from their less critical ones.

</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/kubernetes/what-is-multi-cluster-kubernetes</link><guid isPermaLink="false">https://layer5.io/resources/kubernetes/what-is-multi-cluster-kubernetes</guid><pubDate>Tue, 05 Jul 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/9287b4a708bb510f64057ea305498b77/kubernetes.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[TechStrong TV Interview]]></title><description><![CDATA[

import { ResourcesWrapper } from "../Resources.style.js";
import Button from "../../../reusecore/Button";
import { Link } from "gatsby";

<ResourcesWrapper>
<p>
TechStrong TV hosts a variety of live conversations and panel discussions with world’s leading technology experts and leaders at global tech events and user conferences. In this episode of TechStrong TV, straight out of <Link to="/community/events/open-source-summit-north-america-2022">Open Source Summit NA 2022</Link> , catch guest <Link to="https://layer5.io/community/members/lee-calcote">Lee Calcote</Link> from Layer5 and host Alan Shimel discuss the power of <Link to="/projects">Layer5 projects</Link> in managing service meshes, Kubernetes and the rest of your cloud native infrastucture. They also dive into some of the other network-centric CNCF projects like CoreDNS and gRPC. Tune in now! 
</p>
 <Button $primary $url="https://digitalanarchist.com/videos/open-source-summit-na-2022/lee-calcote-layer5" class="btn-center" >
 <h3>Check out the TechStrong TV Interview with Layer5!</h3>
 </Button>
</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/interview/techstrong-tv-interview</link><guid isPermaLink="false">https://layer5.io/resources/interview/techstrong-tv-interview</guid><pubDate>Sat, 25 Jun 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/25aeb82dc80044f21493d0295ca3b84b/techstrong.webp" length="0" type="image/webp"/><content:encoded></content:encoded></item><item><title><![CDATA[Istio Virtual Service]]></title><description><![CDATA[
import { Link } from "gatsby";
import { ResourcesWrapper } from "../../Resources.style.js";

<ResourcesWrapper>

<p>
  Istio Virtual Service defines a set of traffic routing rules to apply when host is addressed. Each routing rule defines standards for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service defined in the registry.
</p>

<p>
  The source of traffic can also be matched within a routing rule that allows routing to be customized for every specific client context.
</p>  

<div class="fact-left">
<p>
  The below example on Kubernetes routes all HTTP traffic by default to pods of the reviews service with the label “version: v1”. Additionally, HTTP requests with path starting with /wpcatalog/ or /consumercatalog/ will be rewritten to /newcatalog and sent to the pods with label “version: v2”.
</p>
</div>

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews-route
spec:
  hosts:
  - reviews.prod.svc.cluster.local
  http:
  - name: "reviews-v2-routes"
    match:
    - uri:
        prefix: "/wpcatalog"
    - uri:
        prefix: "/consumercatalog"
    rewrite:
      uri: "/newcatalog"
    route:
    - destination:
        host: reviews.prod.svc.cluster.local
        subset: v2
  - name: "reviews-v1-route"
    route:
    - destination:
        host: reviews.prod.svc.cluster.local
        subset: v1

```
<h2>Virtual Service Configuration Affecting Traffic Routing </h2>

<p>A single Virtual Service can be used to describe all the traffic properties of the hosts, including those for multiple HTTP and TCP ports.</p>

<div>
  <h3>Hosts</h3>
  <ul>
    <li>
      The application traffic created by hosts, clients, servers, and applications that use the network as a transport is contained in the physical network data plane (also known as the forwarding plane). 
      As a result, data plane traffic should never have source or destination IP addresses that are assigned to network elements like routers and switches; instead, it should be originated from and delivered to end devices like PCs and servers. To forward data plane traffic as swiftly as possible, routers and switches use hardware chips called application-specific integrated circuits (ASICs). A forwarding information base is referenced by the physical networking data plane (FIB). 
    </li>
    <li>
      The destination hosts to which traffic is being sent it could be a DNS name with wildcard prefix or an IP address depending on the platform.
    </li>
  </ul>
</div>

<div>
  <h3>Gateways</h3>
  <ul>
    <li>
      The names of gateways and sidecars that should apply all these routes. Gateways in other namespaces may be referred to by <code> gateway namespace>/gateway name </code>; specifying a gateway with no namespace qualifier is the same as specifying the VirtualService’s namespace.
    </li>
  </ul>
</div>

<div>
  <h3>HTTP</h3>
  <ul>
    <li>
      An ordered list of route rules for HTTP traffic. The HTTP routes will be applied to the platform service ports named <code>‘http-’/‘http2-’/‘grpc-*’, gateway ports with protocol HTTP/HTTP2/GRPC/ TLS-terminated-HTTPS </code> and service entry ports using HTTP/HTTP2/GRPC protocols.
    </li>
    <li>
      The first rule is matching an incoming request which is used.
    </li>
  </ul>
</div>

<div>
  <h3>TCP</h3>
  <ul>
    <li>  	
      An ordered list of all the routing rules for opaque TCP traffic. TCP routes will be applied to any of the port which is not a HTTP or TLS port. 
    </li>
  </ul>
</div>

<div>
  <h3>ExportTo</h3>
  <ul>
    <li>  	
      Exporting a virtual service allows it to be used by the sidecars and the gateways defined in other namespaces. 
    </li>
    <li>  	
      If no namespaces are specified then the virtual service is exported to all namespaces by default.
    </li>
  </ul>
</div>

<h2>
  Destination
</h2>

<p>
  A destination indicates that the network addressable service to which the request/connection will be sent. A DestinationRule defines policies that apply to traffic intended for a service after routing has occurred.
</p>

```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: reviews-destination
spec:
  host: reviews.prod.svc.cluster.local
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2

```
<div class="fact-left">
<p>A version of the route destination is identified with a reference to a named service subset which should be declared in a corresponding DestinationRule.</p>
</div>

</ResourcesWrapper>
]]></description><link>https://layer5.io/resources/service-mesh/istio-virtual-service</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/istio-virtual-service</guid><pubDate>Thu, 16 Jun 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/731763d720780a49c2ffdfede8c28f4b/istio.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Istio Authorization Policy]]></title><description><![CDATA[
import { Link } from "gatsby";
import istiosecurityarch from './istio-securityarch.svg'
import { ResourcesWrapper } from "../../Resources.style.js";

<ResourcesWrapper>
<div className="intro">
<p>
Istio is a massive project with a wide range of capabilities and deployment options. We will learn about the Istio’s authorization policy with an example .
</p>
</div>

<p>
    <h2>Let’s see Istio’s Security Architecture </h2> 
    </p>
<p>
    Before we directly jump into Istio's Authorization policies let's have a glance at Istio's Security architecture. The below diagram is directly referenced from Istio documentation. From the control plane, users can create things like authorization policies authentication policies, and policies will get translated into envoy config and streamed bent the varied proxies that form up the service mesh, on the information plane side there is east-west traffic from service b to c and also the actual communication takes place through sidecar proxies. If the traffic is entering it moves to the Ingress gateway and if it’s leaving it can attend the Egress gateway in between all this we will apply JWT enforcements.
</p>
<p>
  <img src={istiosecurityarch} align="center" alt="comparative spectrum" />
</p>
<h2> Istio includes a high-level architecture that involves multiple factors such as:</h2>

<p>
<ul>
    <li>  Certificate Authority for key and certificate management </li>
    <li> Sidecar and perimeter proxies work as Policy Enforcement Points to secure communication between the clients and servers. </li>
    <li> A set of Envoy proxy extensions is there to manage telemetry and auditing </li>

</ul>
</p>

<h2> Istio’s Authorization policies</h2>
<p>
    <ul>
    <li>  Workload-to-workload and end-user-to-workload authorization. </li>
    <li> A Simple API includes one single Authorization Policy, which is easy to use and maintain.</li>
    <li>Flexible semantics: operators can define custom conditions on Istio attributes, and use DENY and permit actions. </li>
    <li>  High performance: Istio authorization gets enforced natively on the Envoy. </li>
    <li> High compatibility: supports gRPC, HTTP, HTTPS, and HTTP2 natively, additionaly as well as any plain TCP protocols. </li>

</ul>
</p>

<h2>
    Example Authorization Policy
</h2>
<p>
In this example, we allow access to our service httpbin in namespace foo from any JWT (regardless of the principle) to use the GET method.
</p>

``` 
apiVersion: "security.istio.io/v1beta1"
kind: "AuthorizationPolicy"
metadata:
  name: "allow-reads"
  namespace: foo
spec:
  selector:
    matchLabels:
      app: httpbin
  rules:
  - from:
    - source:
        principals: ["*"]
    to:
    - operation:
        methods: ["GET"]

```
<h2>Access Flow with Auth Policies</h2>

<p>
    There is some logic behind how authorization is set given defined AuthorizationPolicies. Below is that the flow as taken directly from the Istio documentation.
    </p>
<ul>
    <li>If there are any CUSTOM policies that match the request, evaluate and deny the request if the evaluation result's is deny.</li>
    <li>If there are any DENY policies that match with the request, deny the request.</li>
    <li>If there are not any ALLOW policies for the workload, allow the request.</li>
    <li>If any of the ALLOW policies gets match with the request, allow the request.</li>
    <li>Deny the request.</li>
</ul>


</ResourcesWrapper>]]></description><link>https://layer5.io/resources/service-mesh/istio-authorization-policy</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/istio-authorization-policy</guid><pubDate>Wed, 01 Jun 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/731763d720780a49c2ffdfede8c28f4b/istio.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Istio Ingress Gateway in Kubernetes]]></title><description><![CDATA[
import { Link } from "gatsby";
import { ResourcesWrapper } from "../../Resources.style.js";
import Working from "./Istio Ingress Gateway.webp";
import Rep from "./Istio Ingress Gateway No Title.webp";


<ResourcesWrapper>

<p>
    Predominantly, Kubernetes has used an Ingress controller to handle the traffic that enters the cluster from the outside. 
    Istio has replaced all the familiar Ingress resource with new Gateway and VirtualServices resources.
    They work in sync to route all the traffic into the mesh.
    Inside the mesh there is no requirement for Gateways since the services can access each other by a cluster local service name.
</p>
<h3>Let’s understand the working with a representation</h3>
<p>
    <img src={Rep} align="center" alt="Istio Ingress Gateway in Kubernetes No Title" />
</p>
<p>
<ul>
<li>Firstly A request is made by a client on a specific port</li>
<li>Then a load balancer on this port listens and forwards the request to one of the workers in theh cluster on same or a new port</li>
<li>Inside the cluster the request is routed to the Istio Ingress Gateway which is listened on the port of the load balancer</li>
<li>The Service forwards the requestto an Istio Ingress Gateway Pod which is managed by a deployment</li>
<li>The Ingress Gateway Pod is configured by a Gateway and a VirtualService.</li>
<li>The Gateway configures all the ports, protocol, and certificates.</li>
<li>The Virtual Service configures all the routing information to find the correct Servicein it.</li>
<li>The Istio Ingress Gateway Pod routes the request to the application Service.</li>
<li>And lastly, the application Service routes the request to an application Pod which is managed by a deployment.</li>
</ul>
</p>
<ul>
</ul>

<h2>
   Ingress Gateway Service
</h2>

<p>
    The Ingress Gateway Service must listen to all the ports to be able to forward the traffic to the Ingress Gateway pods. 
    Here we will be using routing to bring all the port numbers back to their initial state.
</p>

<p>
    Note that a Kubernetes Service is not a real service, but, since we are using type: 
    "NodePort", all the request will be handled by the kube-proxy provided by Kubernetes and forwarded to a node with a current running pod. 
    Once on the node, an IP-tables is configured a request will be forwarded to the appropriate pod.
</p>

```yaml

# From the istio-ingressgateway service
  ports:
  - name: http2
    nodePort: 30000
    port: 80
    protocol: TCP
  - name: https
    nodePort: 30443
    port: 443
    protocol: TCP
  - name: mysql
    nodePort: 30306
    port: 3306
    protocol: TCP
```
<p>
    If we inspect the service, we will see that it defines more ports than we have describe above.
    So these ports will be used for all the internal Istio communication.
</p>

<h2>
    Ingress Gateway Deployment
</h2>

<p> 
It's a wrapper around the Envoy proxy and it is configured as the sidecars used inside the service mesh. 
When a Gateway or VirtualService gets changed,
they are detected by the Istio Pilot controller and converts this information to an Envoy configuration and sends it to all the proxies, including the Envoy inside the IngressGateway.
</p>

<p>
    Since container ports are not supposed to be declared in Kubernetes pods, we don't have to declare the ports in the Ingress Gateway Deployment.
    If we look inside the deployment we can see that there are a number of ports that are already declared anyway.
    We have to take care about the Ingress Gateway Deployment in SSL certificates. 
    To access the certificates inside the Gateway resources, make sure that we have mounted all the required certificates properly.
</p>

```yaml

# Example represents volume mounts
volumeMounts:
- mountPath: /etc/istio/ingressgateway-certs
  name: ingressgateway-certs
  readOnly: true
- mountPath: /etc/istio/ingressgateway-ca-certs
  name: ingressgateway-ca-certs
  readOnly: true

# Example represents volumes
volumes:
- name: ingressgateway-certs
  secret:
    defaultMode: 420
    optional: true
    secretName: istio-ingressgateway-certs
- name: ingressgateway-ca-certs
  secret:
    defaultMode: 420
    optional: true
    secretName: istio-ingressgateway-ca-certs
```

<h2>The Gateway</h2>

<p>
    The Gateway resources are used to configure the ports for Envoy and also support for the Kubernetes Ingress. 
    Since all the three ports are exposed with the servies, we need these ports to be handled by the Envoy. 
    It can be handled by declaring one or more Gateways.
</p>

```yaml

apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: default-gateway
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:

  - hosts:
    - '*'
    port:
      name: http
      number: 80
      protocol: HTTP

  - hosts:
    - '*'
    port:
      name: https
      number: 443
      protocol: HTTPS
    tls:
      mode: SIMPLE
      privateKey: /etc/istio/ingressgateway-certs/tls.key
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt

  - hosts: # For all the TCP routing this fields will be ignored, but it will be matched
    - '*'  # with the VirtualService, We use * since it will match anything.
    port:
      name: mysql
      number: 3306
      protocol: TCP
```

<h2>VirtualService</h2>
<p>
    The last interesting resource we have is the VirtualService, it used in concert with the Gateway to configure Envoy. 
</p>
<p>
    A general configuration for an HTTP(s) service
</p>

```yaml

apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: counter
spec:
  gateways:
  - default-gateway.istio-system.svc.cluster.local
  hosts:
  - counter.lab.example.com
  http:
  - match:
    - uri:
      prefix: /
    route:
    - destination:
        host: counter
        port:
          number: 80

```
<h2>Application Service and Deployment</h2>
<p>
    The request have now reached the application service and deployment. These are normal Kubernetes resources.
</p>

<h2>Extras:</h2>

<h3>Debugging Istio Gateway</h3>
<p>
     First we will use istioctl to check the configuration status of Istio Ingress Gateway:
</p>

```yaml

# istioctl proxy-status istio-ingressgateway-5586f47659-r64lb.istio-system
Clusters Match
Listeners Match
Routes Match

```
<p>
    If anything does not get synced with it, try restarting the ingress gateway pod once - it may be possible that it somehow an update got missed.
    If RDS looked good, we can check access logs of it. 
</p>

```yaml

#kubectl get configmap istio -n istio-system -o yaml | grep "accessLogFile: "
disable access log.\naccessLogFile: \"/dev/stdout\"\n\n# If accessLogEncoding

```
<p>
    Once all the access logs are enabled, we can try torequest a few more times and check the logs on the Ingress Gateway:
</p>

```yaml

# kubectl logs -n istio-system istio-ingressgateway-5586f47659-r64lb | grep -v deprecated

```

</ResourcesWrapper>]]></description><link>https://layer5.io/resources/service-mesh/istio-ingress-gateway-in-kubernetes</link><guid isPermaLink="false">https://layer5.io/resources/service-mesh/istio-ingress-gateway-in-kubernetes</guid><pubDate>Fri, 01 Apr 2022 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/eec5644b77e71f1ff68a36b674aea59c/Istio Ingress Gateway.webp" length="0" type="image/webp"/><content:encoded></content:encoded></item><item><title><![CDATA[An Introduction to Meshery (Webinar-on-Demand)]]></title><description><![CDATA[
import { Link } from "gatsby";
import { ResourcesWrapper } from "../Resources.style.js";

<ResourcesWrapper>
<p>
<Link to="/cloud-native-management/meshery">Meshery</Link> is an open source, vendor-neutral, extensible management plane that enables service mesh users to overcome the challenges of complex virtual networking, empowers them to design and apply patterns containing tried and true best practices, benchmarks the performance of your service mesh deployments and enables developers, operators, and product managers to understand and manage their cloud native services with confidence. 
</p>

<h3>
Let’s learn how to manage service meshes with confidence with the extensible service mesh manager, <Link to="/cloud-native-management/meshery">Meshery</Link>.
</h3>

<div class="iframe-container">
<iframe width="460" height="215" src="https://www.youtube.com/embed/mU8qHUGYsk8" loading="lazy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>

</ResourcesWrapper>]]></description><link>https://layer5.io/resources/meshery/an-introduction-to-meshery-webinar-on-demand</link><guid isPermaLink="false">https://layer5.io/resources/meshery/an-introduction-to-meshery-webinar-on-demand</guid><pubDate>Sun, 07 Nov 2021 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/2f4a78ada287ae5dcc2e75fad653f671/meshery-logo-light-text.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Service Mesh FAQs]]></title><description><![CDATA[
import { ResourcesWrapper } from "../Resources.style.js";
import FAQ from "../../../sections/General/Faq";

<ResourcesWrapper>
  <div>
    <FAQ category={["Service Mesh"]} />
  </div>
</ResourcesWrapper>
;
]]></description><link>https://layer5.io/resources/faq/service-mesh-faqs</link><guid isPermaLink="false">https://layer5.io/resources/faq/service-mesh-faqs</guid><pubDate>Fri, 05 Nov 2021 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/cb310234b6631abcabb632a85974a3dd/service-mesh.svg" length="0" type="image/svg+xml"/><content:encoded></content:encoded></item><item><title><![CDATA[Meshery FAQs]]></title><description><![CDATA[
import { ResourcesWrapper } from "../Resources.style.js";
import FAQ from "../../../sections/General/Faq";

<ResourcesWrapper>
  <div>
    <FAQ category={["Meshery"]} />
  </div>
</ResourcesWrapper>
;
]]></description><link>https://layer5.io/resources/faq/meshery-faqs</link><guid isPermaLink="false">https://layer5.io/resources/faq/meshery-faqs</guid><pubDate>Wed, 03 Nov 2021 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/2db8d89cdff9c95760d3235b380b39ea/meshery-logo-dark-text.webp" length="0" type="image/webp"/><content:encoded></content:encoded></item></channel></rss>