<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Layer5 Contributor Feed]]></title><description><![CDATA[Expect more from your infrastructure. Cloud native, open source software for your cloud native infrastructure and applications. Allowing developers to focus on business logic, not infrastructure concerns. Empowering operators to confidently run modern infrastructure.]]></description><link>https://layer5.io</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 11 Aug 2025 08:27:06 GMT</lastBuildDate><item><title><![CDATA[Meet the Maintainer: Ian Whitney]]></title><description><![CDATA[Meet the Maintainer series with open source maintainer, Ian Whitney]]></description><link>https://layer5.io/blog/open-source/meet-the-maintainer-ian-whitney</link><guid isPermaLink="false">https://layer5.io/blog/open-source/meet-the-maintainer-ian-whitney</guid><dc:creator><![CDATA[Vivek Vishal]]></dc:creator><pubDate>Thu, 29 May 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/4d40eb2ec567887432dc7cab5eee5af7/ian-whitney-meshery-maintainer.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;div class=&quot;MeetTheMaintainerstyle__MeetTheMaintainer-sc-cimr9h-0 cVWUJX&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Continuing in our Meet the Maintainer series, we have &lt;a href=&quot;/community/members/ian-whitney&quot;&gt;Ian Whitney&lt;/a&gt;. Ian is a maintainer of &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt;Meshery UI&lt;/a&gt;. In this interview, we get to know Ian a little better and learn about his journey as an open source project maintainer and with Layer5 community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Ian, thank you for joining me today. Many people inside and outside of the Layer5 Community have seen the effects of your contributions, but may not know the backstory as to who Ian is and how you arrived at your maintainer role. Indulge us. How did you discover the Layer5 community? What inspired you to stay?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;My journey into Layer5 began with a desire to contribute to an open source project during Hacktoberfest 2024.  I was looking for a welcoming community that was working on meaningful problems - somewhere I could operate at the intersection of what I already knew and where I wanted to grow. That&amp;#x27;s when I found Meshery and Layer5. After joining a newcomers meeting, I immediately felt it was the right fit as a place I could explore deeper cloud native topics, a new programming language like golang and contribute to the UI. What inspired me to stay was the emphasis on collaboration and developer growth. Before I knew it, I was actively contributing to Meshery and eventually invited to become a maintainer.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;You’re a &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt;Meshery UI&lt;/a&gt; maintainer. What does being a Meshery maintainer mean to you?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;After being a contributor for some time, I looked to the maintainers as leaders that were champions of culture to guide and lift others up to contribute and grow their technical skills and leadership. I hope to be able to continue with this emphasis of helping others grow and develop.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Have you worked with any other open source project? How does Layer5 compare?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;I have not contributed to any other open source projects. Meshery was my first open source experience!&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;&lt;a href=&quot;/projects&quot;&gt;Layer5 projects&lt;/a&gt; have a number of active, open source projects. You’ve been consistently contributing to a few of them. Which one(s) are you currently focusing on?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;I am currently focused on Meshery UI client and the supporting backend and ecosystems.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What’s the coolest Meshery demo you have done/seen?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;The coolest demos I have seen always pertain to Meshery Kanvas. The interactive and intuitive UI are really cool to watch.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What is your favorite feature or aspect of UI in this project, and why?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;The best feature of the project is the community and being able to explore technical areas with guidance of seasoned contributors.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What is your hot tip for working with Meshery that others may not know?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;My hot tip is to just start and develop a habit to keep showing up.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Where do you see opportunities for contributors to get involved within Meshery and Layer5 community?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;There are alot of good first issues in the UI repo. We are currently trying to mature our testing efforts around Meshery UI client with playwright. This space is a great space to learn the product and improve the user flows while increasing test coverage.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Let&amp;#x27;s get to know you a bit better with some quick questions:  What&amp;#x27;s the emoji you use most often? Do you prefer movies or books? Would you consider yourself a morning person or a night owl? Over the past year, what&amp;#x27;s a project or accomplishment you&amp;#x27;re particularly proud of?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;😀&lt;p&gt;My favorite emoji is 😀. I prefer books - the more technical the better 😀. I&amp;#x27;m a morning person with a cup of coffee. Over the past year, I feel most accomplished by trying to give back as much as I&amp;#x27;ve gained. This includes efforts through code contributions, mentoring, or sharing what I&amp;#x27;ve learned along the way. Being part of this community has shown me the value of collaboration, and I&amp;#x27;ve made it a point to support new contributors, answer questions, and help others contribute. It&amp;#x27;s incredibly rewarding to see someone you helped gain confidence and start making meaningful contributions of their own.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Do you have any advice for individuals hopeful to become Layer5 contributors or potentially maintainers?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Ian:&lt;/span&gt;&lt;p&gt;Try to build a habit of learning, contributing and helping others.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;note&quot;&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/forklift.81115a37.svg&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;p&gt;The Meshery project moves at an impressive pace thanks to maintainers like Ian. Be like Ian. Join the &lt;a href=&quot;https://slack.layer5.io&quot;&gt;Layer5 Slack&lt;/a&gt; and say “hi&amp;quot;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Docker Model Runner: Engineering Summary & Future Horizons]]></title><link>https://layer5.io/blog/docker/docker-model-runner-engineering-summary-future-horizons</link><guid isPermaLink="false">https://layer5.io/blog/docker/docker-model-runner-engineering-summary-future-horizons</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Tue, 20 May 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/079679ec12e22e5eaead3990ab81da4f/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;Over the course of &lt;a href=&quot;/blog/category/docker&quot;&gt;this series&lt;/a&gt;, we&amp;#x27;ve embarked on a deep technical dive into Docker Model Runner, moving beyond surface-level descriptions to uncover the engineering principles and practical implications of this innovative toolkit. From its foundational architecture to its integration with the broader developer ecosystem, Model Runner presents a compelling vision for the future of local AI development. In this concluding post, we&amp;#x27;ll synthesize the key engineering takeaways and explore the promising horizons as Docker Model Runner matures.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Key Engineering Takeaways: A Recap&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Our journey has illuminated several critical aspects that define Docker Model Runner&amp;#x27;s value proposition for engineers:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;OCI for Robust Model Management:&lt;/strong&gt; Model Runner&amp;#x27;s strategic adoption of the Open Container Initiative (OCI) standard for packaging and distributing AI models is transformative. It brings DevOps-like rigor to model lifecycle management, enabling versioning, provenance, and the use of existing container registries and CI/CD pipelines for AI models.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Performance via Host-Native Execution:&lt;/strong&gt; The decision to run inference engines (like llama.cpp) as host-native processes, with direct GPU access (especially Metal API on Apple Silicon), prioritizes local performance. This minimizes latency and provides a responsive experience crucial for iterative development.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;OpenAI-Compatible API for Seamless Integration:&lt;/strong&gt; By offering an API compatible with OpenAI&amp;#x27;s standards, Model Runner drastically lowers the barrier to entry. Engineers can leverage existing SDKs, tools like LangChain and LlamaIndex, and familiar coding patterns with minimal friction.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Docker Compose for Orchestrated AI Stacks:&lt;/strong&gt; The introduction of the provider service type in Docker Compose allows AI models to be declared and managed as integral components of multi-service applications, simplifying the orchestration of complex local AI development environments.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Ecosystem Synergy (e.g., Spring AI):&lt;/strong&gt; Integrations with frameworks like Spring AI demonstrate Model Runner&amp;#x27;s ability to seamlessly fit into established development ecosystems, enabling Java developers, for instance, to easily incorporate local LLMs.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Advanced Local Workflows &amp;amp; Fine-Grained Control:&lt;/strong&gt; Model Runner empowers engineers to execute sophisticated, multi-stage AI pipelines locally. The ability to dynamically tune model parameters for specific tasks without API costs fosters deep experimentation and accelerates the development of nuanced AI features.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Collectively, these features address core engineering challenges in local AI development: cost, privacy, iteration speed, complexity, and environmental control.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Future Horizons: From Beta to Mainstream&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;As Docker Model Runner evolves beyond its Beta phase, several key developments will shape its impact:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;API Stability and Maturation:&lt;br/&gt;A crucial step will be the stabilization of its APIs. As noted during its Beta, APIs were subject to change. A stable API will provide the confidence developers need to build more robust and long-lasting integrations.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Expanded Platform and Hardware Support:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Windows GPU Acceleration:&lt;/strong&gt; The full realization of performant GPU acceleration on Windows (especially for NVIDIA GPUs) will be a significant milestone, broadening its accessibility to a large segment of the developer community.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Linux Enhancements:&lt;/strong&gt; While a Docker Engine plugin exists, further enhancements for Linux environments, potentially with more streamlined management features akin to Docker Desktop, will be important for server-side local development or specialized Linux-based AI workstations.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Comprehensive Custom Model Management:&lt;br/&gt;The ability for users to easily package, docker model push their own custom or fine-tuned models to any OCI-compliant registry, and then docker model pull and run them seamlessly is paramount. This will unlock Model Runner&amp;#x27;s full potential for organizations with bespoke AI needs, moving beyond curated public models.  &lt;/li&gt;&lt;li&gt;Deeper Ecosystem Integrations:&lt;br/&gt;Expect continued and deeper integrations with:  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;MLOps Tools:&lt;/strong&gt; Tighter connections with MLOps platforms for experiment tracking, model monitoring (even locally), and smoother transitions from local development to production deployment pipelines.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;IDEs:&lt;/strong&gt; More direct integrations within popular Integrated Development Environments for an even more fluid &amp;quot;inner loop&amp;quot; experience.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;More Inference Engines:&lt;/strong&gt; While llama.cpp is a strong start, the potential for a pluggable engine architecture could see Model Runner supporting a wider array of inference backends optimized for different model types or hardware.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Enhanced Observability and Debugging:&lt;br/&gt;As local AI workflows become more complex, improved tools for observing model behavior, debugging inference issues, and monitoring resource consumption locally will become increasingly valuable.&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;strong&gt;The Enduring Impact: Local AI as a Standard Engineering Practice&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Docker Model Runner is more than just a feature; it represents a significant step towards making local AI development a standard, accessible, and efficient engineering practice. By integrating AI model execution directly into the familiar and powerful Docker ecosystem, it lowers barriers, fosters innovation, and empowers developers to build the next generation of AI-powered applications with greater speed, control, and confidence.&lt;br/&gt;
The journey from Beta to a fully mature product will undoubtedly bring further refinements and capabilities. However, the foundational principles and architectural choices already evident in Docker Model Runner signal a bright future for local-first AI development, driven by the needs and workflows of engineers.  &lt;/p&gt;&lt;p&gt;&lt;em&gt;This blog post series has been based on information available about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change as the product evolves.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Spring AI: Streamlining Local LLM Integration for Java Developers]]></title><link>https://layer5.io/blog/docker/spring-ai-streamlining-local-llm-integration-for-java-developers</link><guid isPermaLink="false">https://layer5.io/blog/docker/spring-ai-streamlining-local-llm-integration-for-java-developers</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Wed, 14 May 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/7a5be1548e7efed21a2ef4929e78a5ab/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;In our &lt;a href=&quot;/blog/category/docker&quot;&gt;ongoing exploration&lt;/a&gt; of Docker Model Runner, we&amp;#x27;ve covered its OCI-based model management, performance architecture, OpenAI-compatible API, and Docker Compose integration. Now, we turn to a specific, yet highly impactful, synergy: how Docker Model Runner empowers &lt;strong&gt;Java developers using the Spring AI framework&lt;/strong&gt; to seamlessly incorporate local Large Language Models (LLMs) into their applications.&lt;br/&gt;
For Java engineers vested in the Spring ecosystem, Spring AI offers a familiar and powerful abstraction layer for interacting with various AI models. Docker Model Runner&amp;#x27;s compatibility provides a straightforward path to leverage these local models without stepping outside the conventional Spring development paradigm.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Spring AI: Simplifying AI for Java Applications&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Before diving into the integration, it&amp;#x27;s worth briefly understanding Spring AI&amp;#x27;s mission. Spring AI aims to apply core Spring principles—such as autoconfiguration, dependency injection, and portable service abstractions—to the domain of artificial intelligence. It provides Java developers with:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Consistent APIs:&lt;/strong&gt; A unified API for interacting with different AI models (both local and remote), reducing the need to learn multiple vendor-specific SDKs.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Abstraction Layers:&lt;/strong&gt; Components like ChatClient, EmbeddingClient, and ImageClient abstract away the underlying model provider.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Integration with Spring Boot:&lt;/strong&gt; Easy setup and configuration within Spring Boot applications.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Docker Model Runner as a Local &amp;quot;Ollama&amp;quot; for Spring AI&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Spring AI supports various AI model providers, including commercial cloud services (like OpenAI, Azure OpenAI) and self-hosted solutions (like Ollama). From Spring AI&amp;#x27;s perspective, Docker Model Runner, with its OpenAI-compatible API, effectively acts like a local, easily manageable Ollama-style endpoint.&lt;br/&gt;
When Docker Model Runner is active and serving a model (e.g., Llama 3, Gemma) with its API endpoint accessible (typically http://localhost:12434 or &lt;a href=&quot;http://model-runner.docker.internal&quot;&gt;http://model-runner.docker.internal&lt;/a&gt; if accessed from another container), Spring AI can be configured to point to it.&lt;br/&gt;
Here&amp;#x27;s how a Java engineer benefits:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simplified Configuration in Spring Boot&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Spring AI&amp;#x27;s autoconfiguration can often detect and set up the necessary beans to interact with an OpenAI-compatible endpoint. For Docker Model Runner, this typically involves setting a few properties in your application.properties or application.yml file:  &lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-java&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;\# For Spring AI 0.8.x (or similar versions)  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;spring.ai.openai.chat.base-url=http://localhost:12434/engines/v1 &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;\# Or your specific DMR endpoint  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;spring.ai.openai.chat.options.model=ai/llama3.2:1B-Q8\_0 &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;\# The model you want to use  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;use  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;spring.ai.openai.api-key=YOUR\_DUMMY\_API\_KEY\_OR\_EMPTY&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;\# Potentially disable API key if DMR doesn&amp;#x27;t require it strictly for local&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;(Note: The exact property names and structure might vary slightly based on the Spring AI version and whether you&amp;#x27;re configuring a generic OpenAI client or a more specific Ollama-like client type if Spring AI introduces more direct DMR support.)&lt;/em&gt;  &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Leveraging Spring AI&amp;#x27;s ChatClient and EmbeddingClient&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Once configured, developers can inject and use Spring AI&amp;#x27;s standard clients without needing to know that the underlying provider is Docker Model Runner. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-java&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;import org.springframework.ai.chat.ChatClient;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;   import org.springframework.ai.chat.prompt.Prompt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;   import org.springframework.beans.factory.annotation.Autowired;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;   import org.springframework.stereotype.Service;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;   @Service  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;   public class MyAiService {&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;       private final ChatClient chatClient;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;       @Autowired  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;       public MyAiService(ChatClient chatClient) {  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;           this.chatClient \= chatClient;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;       }&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;       public String getJokeAbout(String topic) {  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;           Prompt prompt \= new Prompt(&amp;quot;Tell me a short joke about &amp;quot; \+ topic);  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;           return chatClient.call(prompt).getResult().getOutput().getContent();  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;       }  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;   }&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;   This code remains the same whether Spring AI is talking to OpenAI&amp;#x27;s cloud API, a self-hosted Ollama instance, or Docker Model Runner serving a local model. This portability is a huge win.  &lt;/p&gt;&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Seamless Local Development and Testing&lt;/strong&gt;
Engineers can develop and test AI-driven features entirely locally using their preferred Java tools and the Spring framework. Docker Model Runner handles the model serving, and Spring AI provides the clean Java interface. This speeds up iteration cycles and reduces reliance on potentially costly cloud APIs during development.  &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Consistency with Production (Potentially)&lt;/strong&gt;&lt;br/&gt;
While Docker Model Runner is primarily for local development, the abstraction provided by Spring AI means that switching to a production-grade, potentially cloud-hosted model provider for deployment can be achieved mainly through configuration changes, without altering the core application logic.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;strong&gt;The Bigger Picture: Local AI in Enterprise Java&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The integration with Spring AI is significant because it brings the ease of local LLM experimentation directly into the robust, enterprise-focused Java and Spring ecosystem. It allows Java teams to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Prototype AI features rapidly.&lt;/strong&gt;  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Upskill on AI concepts using familiar tools.&lt;/strong&gt;  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Conduct local, private testing of AI interactions with business data.&lt;/strong&gt;  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Integrate AI into existing Spring Boot applications with minimal friction.&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Docker&amp;#x27;s collaboration with Spring AI (as noted in some announcements) underscores a shared vision of making AI more accessible and developer-friendly across different programming environments. By ensuring Docker Model Runner presents an API that Spring AI can readily consume, both platforms contribute to lowering the barrier to entry for sophisticated AI development.&lt;br/&gt;
For Java engineers, this means Docker Model Runner isn&amp;#x27;t just another tool; it&amp;#x27;s a key enabler for leveraging the power of local LLMs within the comfort and productivity of the Spring framework.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Next, we&amp;#x27;ll delve into some practical, task-specific configurations and advanced use cases you can explore with Docker Model Runner, moving beyond basic chat completions.&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change. Configuration details for Spring AI may vary based on specific versions.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Docker Compose: Orchestrating Multi-Service AI Applications Locally]]></title><link>https://layer5.io/blog/docker/docker-compose-orchestrating-multi-service-ai-applications-locally</link><guid isPermaLink="false">https://layer5.io/blog/docker/docker-compose-orchestrating-multi-service-ai-applications-locally</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Thu, 24 Apr 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/97d66df31989169322ece96b70bfd7a8/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;So far in our &lt;a href=&quot;/blog/category/docker&quot;&gt;series on Docker Model Runner&lt;/a&gt;, we&amp;#x27;ve dissected its OCI-based model management, its performance-optimized execution architecture, and its OpenAI-compatible API. Now, we explore a feature that truly elevates its utility for engineers building complex systems: &lt;strong&gt;deep integration with Docker Compose via a novel provider service type.&lt;/strong&gt;  &lt;/p&gt;&lt;p&gt;For engineers, Docker Compose is the go-to tool for defining and running multi-container Docker applications. The introduction of the provider service type specifically for Model Runner bridges the gap between local AI model execution and the broader application stack, allowing you to define and manage AI models as integral components of your local development environment declaratively.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Beyond CLI: Models as First-Class Services in Compose&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;While docker model run is handy for quick tests, real-world applications often involve multiple interacting services—a web frontend, a backend API, a database, and now, an AI model. Docker Model Runner&amp;#x27;s Compose integration allows you to define the AI model itself as a service within your &lt;code&gt;docker-compose.yml&lt;/code&gt; file.  &lt;/p&gt;&lt;p&gt;The key innovation here is the provider attribute within a service definition. Here&amp;#x27;s a conceptual example based on Docker&amp;#x27;s documentation:&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-yaml&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;model\_provider\_service&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; \&lt;/span&gt;&lt;span class=&quot;token comment&quot; style=&quot;color:rgb(99, 119, 119);font-style:italic&quot;&gt;# You can name this service as you like  &lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;provider&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; model        \&lt;/span&gt;&lt;span class=&quot;token comment&quot; style=&quot;color:rgb(99, 119, 119);font-style:italic&quot;&gt;# Specifies this is a model provider  &lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; ai/llama3.2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;1B&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;Q8\_0 \&lt;/span&gt;&lt;span class=&quot;token comment&quot; style=&quot;color:rgb(99, 119, 119);font-style:italic&quot;&gt;# The OCI image for the model  &lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    \&lt;/span&gt;&lt;span class=&quot;token comment&quot; style=&quot;color:rgb(99, 119, 119);font-style:italic&quot;&gt;# No &amp;#x27;build&amp;#x27; or &amp;#x27;image&amp;#x27; directives here in the traditional sense for the provider&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;my\_app\_service&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; ./app  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      \&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; &lt;/span&gt;&lt;span class=&quot;token string&quot; style=&quot;color:rgb(173, 219, 103)&quot;&gt;&amp;quot;8080:80&amp;quot;&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;depends\_on&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      \&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; model\_provider\_service \&lt;/span&gt;&lt;span class=&quot;token comment&quot; style=&quot;color:rgb(99, 119, 119);font-style:italic&quot;&gt;# Ensures model is ready before the app starts  &lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;environment&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      \&lt;/span&gt;&lt;span class=&quot;token comment&quot; style=&quot;color:rgb(99, 119, 119);font-style:italic&quot;&gt;# Environment variables will be injected here (see below)  &lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;MODEL\_NAME&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; $&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;MODEL\_PROVIDER\_SERVICE\_MODEL&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;  &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;token key atrule&quot;&gt;MODEL\_URL&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt; $&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;MODEL\_PROVIDER\_SERVICE\_URL&lt;/span&gt;&lt;span class=&quot;token punctuation&quot; style=&quot;color:rgb(199, 146, 234)&quot;&gt;}&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;In this setup:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;model_provider_service doesn&amp;#x27;t run a traditional container in the same way my_app_service does. Instead, it instructs Docker Compose to leverage Docker Model Runner.  &lt;/li&gt;&lt;li&gt;Docker Model Runner, when processing this provider service, will ensure the specified image (the AI model) is pulled and made available via its host-native inference engine.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Automatic Model Provisioning and Service Discovery&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;This Compose integration brings significant benefits for engineers:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Declarative Model Dependencies:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;You declare your AI model dependency directly in your docker-compose.yml. Docker Model Runner handles the provisioning (pulling and preparing the model if needed) when you run docker compose up.  &lt;/li&gt;&lt;li&gt;This is a stark improvement over manual docker model run commands or custom scripts to manage model lifecycle alongside your application stack.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Automated Service Discovery via Environment Variables:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;This is a crucial feature for seamless integration. When my_app_service starts (after model_provider_service is ready), Docker Compose automatically injects environment variables into my_app_service.  &lt;/li&gt;&lt;li&gt;These variables typically follow the pattern: PROVIDER_SERVICE_NAME_MODEL and PROVIDER_SERVICE_NAME_URL.  &lt;ul&gt;&lt;li&gt;MODEL_PROVIDER_SERVICE_MODEL: Contains the name/tag of the model being served (e.g., ai/llama3.2:1B-Q8_0).  &lt;/li&gt;&lt;li&gt;MODEL_PROVIDER_SERVICE_URL: Provides the URL your application should use to access the Model Runner&amp;#x27;s API endpoint for this model. This would often point to the internal DNS &lt;a href=&quot;http://model-runner.docker.internal&quot;&gt;http://model-runner.docker.internal&lt;/a&gt; or a host-accessible TCP port if configured.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Your application code can then dynamically use these environment variables to configure its AI client, making the connection to the local model effortless and portable.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Simplified depends_on for Startup Order:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;Using depends_on ensures that your application services only start after Model Runner has signaled that the model provider is ready. This prevents your application from trying to connect to a model that isn&amp;#x27;t yet available.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;strong&gt;Engineering Benefits for Complex AI Applications&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;This declarative, integrated approach offers tangible advantages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Reproducible AI Development Environments:&lt;/strong&gt; Your entire local stack, including the specific AI model version, is defined in code (docker-compose.yml), making it easy to share, version control, and ensure consistency across development team members.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Simplified Onboarding:&lt;/strong&gt; New developers can get a complex AI-powered application stack running locally with a single docker compose up command.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Streamlined Local Testing of AI Features:&lt;/strong&gt; Test end-to-end flows involving your application logic and AI model interactions in a fully integrated local environment that mirrors how services would interact.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Foundation for Local MLOps Loops:&lt;/strong&gt; While focused on local development, this pattern lays a conceptual foundation for how AI models can be treated as manageable dependencies within larger application architectures, aligning with MLOps principles.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By treating AI models as discoverable services managed by Compose, Docker Model Runner significantly lowers the barrier to building and iterating on sophisticated multi-service applications that leverage local AI capabilities. This moves beyond simply running a model in isolation to truly integrating AI into your development workflow.&lt;br/&gt;
Next up, we&amp;#x27;ll explore how Docker Model Runner specifically caters to Java developers through its integration with frameworks like Spring AI, further simplifying the adoption of local AI.  &lt;/p&gt;&lt;p&gt;&lt;em&gt;This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Host-Native Execution & GPU Deep Dive]]></title><link>https://layer5.io/blog/docker/host-native-execution-gpu-deep-dive</link><guid isPermaLink="false">https://layer5.io/blog/docker/host-native-execution-gpu-deep-dive</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Tue, 15 Apr 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/a0d502a06407981844fc859ba51cbc91/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;In our series on &lt;a href=&quot;/blog/category/docker&quot;&gt;Docker Model Runner&lt;/a&gt;, we&amp;#x27;ve explored Docker Model Runner&amp;#x27;s role in simplifying local AI development and its strategic use of OCI artifacts for model management. Now, we peel back another layer to examine a critical aspect for any engineer working with Large Language Models (LLMs): &lt;strong&gt;performance&lt;/strong&gt;. How does Docker Model Runner achieve the responsiveness needed for an efficient local development loop? The answers lie in its architectural choices, particularly its embrace of host-native execution and direct GPU access.  &lt;/p&gt;&lt;p&gt;For engineers, &amp;quot;local&amp;quot; often implies a trade-off: convenience versus raw power. Docker Model Runner attempts to bridge this gap, and understanding its performance model is key to leveraging it effectively.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Architectural Pivot: Why docker model run Isn&amp;#x27;t docker container run&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;One of the most crucial, and perhaps initially counter-intuitive, aspects of Docker Model Runner is how it executes AI models. Seasoned Docker users might expect docker model run some-model to spin up an isolated Docker container housing the model and its inference engine. However, Model Runner takes a more direct path to prioritize local performance.  &lt;/p&gt;&lt;p&gt;As detailed in multiple technical breakdowns and official documentation, when you execute &lt;code&gt;docker model run&lt;/code&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;No Traditional Container for Inference:&lt;/strong&gt; The command doesn&amp;#x27;t launch a standard Docker container for the core inference task.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Host-Native Inference Server:&lt;/strong&gt; Instead, it interacts with an inference server (initially built on the efficient llama.cpp engine) that runs as a &lt;strong&gt;native process directly on your host machine&lt;/strong&gt;. This server is managed as part of Docker Desktop or the Model Runner plugin.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is a deliberate engineering decision. Docker&amp;#x27;s own statements reveal that this approach was chosen to &amp;quot;significantly improve performance by eliminating containerization overhead for resource-intensive AI workloads&amp;quot; and to avoid the &amp;quot;performance limitations of running models inside virtual machines.&amp;quot; While Docker&amp;#x27;s traditional strength lies in containerization for isolation and portability, for the demanding task of LLM inference locally, the raw performance gains from direct host execution were deemed paramount.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Unlocking Hardware: Direct GPU Acceleration&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;A major bottleneck for LLM performance is often GPU access. Docker Model Runner addresses this head-on:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Optimized for Apple Silicon (Metal API):&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;For developers on macOS with Apple Silicon (M-series chips), Model Runner&amp;#x27;s host-native inference engine is designed to &lt;strong&gt;directly access Apple&amp;#x27;s Metal API&lt;/strong&gt;. This provides a highly optimized path to the GPU, bypassing virtualization layers that can throttle performance. This direct access can offer a noticeable speed advantage compared to running models within a container that has to go through more layers to reach the GPU.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Windows GPU Support on the Roadmap:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;Recognizing the diverse hardware landscape, Docker has explicitly included support for GPU acceleration on Windows platforms (primarily targeting NVIDIA GPUs) in its development plans. This is a critical feature for broadening Model Runner&amp;#x27;s appeal and utility.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This strategy of direct hardware access, especially for GPUs, is a pragmatic choice. It acknowledges that for the &amp;quot;inner loop&amp;quot; of local AI development—where rapid iteration and experimentation are key—minimizing inference latency is crucial.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Intelligent Resource Management: Efficiency Under the Hood&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Beyond raw execution speed, Docker Model Runner incorporates features for efficient resource utilization:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;On-Demand Model Loading:&lt;/strong&gt; Models are not kept in memory at all times. When you make a request to a specific model (e.g., via an API call from your application or a docker model run command), Model Runner loads it into memory &amp;quot;on-demand,&amp;quot; provided the model files have already been pulled locally. This means you don&amp;#x27;t necessarily have to issue a docker model run before your application can start interacting with a model.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Memory Caching with Inactivity Timeout:&lt;/strong&gt; Once loaded, a model remains in memory to serve subsequent requests quickly. However, to conserve system resources, models are automatically unloaded if they remain inactive for a predefined period. This inactivity timeout is &lt;strong&gt;currently set to 5 minutes&lt;/strong&gt;. This is a practical detail that impacts how long a model stays &amp;quot;warm&amp;quot; and ready for immediate use during an interactive development session.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This combination of on-demand loading and inactivity-based unloading helps balance responsiveness with efficient use of your local machine&amp;#x27;s memory.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Engineering Trade-Off: Performance vs. Isolation&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The decision to run the inference engine as a host-native process is a clear trade-off: Docker is prioritizing local inference speed and direct hardware access over the complete process isolation typically provided by containers &lt;em&gt;for the inference step itself&lt;/em&gt;. While the applications &lt;em&gt;using&lt;/em&gt; the model can still be containerized and benefit from Docker&amp;#x27;s isolation, the model execution core operates closer to the metal.&lt;br/&gt;
This architectural choice highlights Docker&amp;#x27;s commitment to making the local AI development experience as smooth and fast as possible, even if it means deviating slightly from its traditional container-centric execution model for this specific, performance-sensitive component.&lt;br/&gt;
Understanding this performance architecture—host-native execution, direct GPU access, and smart resource management—allows engineers to better anticipate Model Runner&amp;#x27;s behavior, optimize their local AI workflows, and appreciate the engineering decisions aimed at making local LLM development more practical and efficient.  &lt;/p&gt;&lt;p&gt;In our next post, we&amp;#x27;ll explore the API architecture of Docker Model Runner, focusing on its OpenAI compatibility and the various ways you can connect your applications to the local inference engine.  &lt;/p&gt;&lt;p&gt;&lt;em&gt;This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[API Architecture, OpenAI Compatibility, and Connection Strategies]]></title><link>https://layer5.io/blog/docker/api-architecture-openai-compatibility-and-connection-strategies</link><guid isPermaLink="false">https://layer5.io/blog/docker/api-architecture-openai-compatibility-and-connection-strategies</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/7ad71d9ddffb4f0135947f5b528e0c93/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;In our last &lt;a href=&quot;/blog/category/docker&quot;&gt;post in this series&lt;/a&gt;, explored Docker Model Runner&amp;#x27;s OCI-based model management and its performance-centric execution model, we now turn our attention to another critical area for engineers: its &lt;strong&gt;API architecture and connectivity options&lt;/strong&gt;. How do your applications actually &lt;em&gt;talk&lt;/em&gt; to the models running locally via Model Runner? The answer lies in a thoughtfully designed API layer, with OpenAI compatibility at its core, and flexible connection methods to suit diverse development scenarios.  &lt;/p&gt;&lt;p&gt;For engineers, a well-defined and accessible API is paramount. It dictates the ease of integration, the reusability of existing code, and the overall developer experience when building AI-powered applications.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Heart of the Engine: llama.cpp and a Pluggable Future&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;In its initial Beta release, Docker Model Runner&amp;#x27;s inference capabilities are powered by an integrated engine built on llama.cpp. This open-source project is renowned for its efficient execution of LLMs across various hardware, making it a solid foundation for local inference.  &lt;/p&gt;&lt;p&gt;When you interact with Model Runner, you&amp;#x27;re essentially communicating with this llama.cpp-based server, which runs as a native host process. The API paths often reflect this underlying engine, for example, with endpoints structured under /engines/llama.cpp/v1/... or a more generalized &lt;code&gt;/engines/v1/...&lt;/code&gt;.&lt;br/&gt;
While llama.cpp provides a robust initial backbone, the API path structure (e.g., &lt;code&gt;/engines/...&lt;/code&gt;) hints at a potentially pluggable architecture. This is a common design pattern that could allow Docker to integrate other inference engines or model serving technologies in the future. This foresight means Model Runner could evolve to support a wider array of model types, quantization methods, or hardware acceleration frameworks without requiring a fundamental redesign of its API interaction model.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The &amp;quot;Superpower&amp;quot;: OpenAI-Compatible API&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Perhaps the most strategically significant aspect of Model Runner&amp;#x27;s API is its &lt;strong&gt;OpenAI compatibility&lt;/strong&gt;. This is a game-changer for several reasons:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Leverage Existing SDKs and Tools:&lt;/strong&gt; Engineers can use their existing OpenAI SDKs (Python, Node.js, etc.) and a vast ecosystem of compatible tools like LangChain or LlamaIndex with minimal, if any, code changes. This dramatically lowers the barrier to adoption.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Simplified Migration:&lt;/strong&gt; If you&amp;#x27;ve been developing against OpenAI&amp;#x27;s cloud APIs, transitioning to local models with Model Runner can often be as simple as changing the baseURL in your client configuration. This seamless switch accelerates local development and testing.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Reduced Learning Curve:&lt;/strong&gt; There&amp;#x27;s no need to learn a new, proprietary API. The familiar OpenAI request/response structures for tasks like chat completions (&lt;code&gt;/chat/completions&lt;/code&gt;) or embeddings (&lt;code&gt;/embeddings&lt;/code&gt;) remain consistent. &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This adherence to a de facto industry standard API is a deliberate choice by Docker to maximize interoperability and ease of integration, allowing developers to focus on application logic rather than wrestling with new API paradigms.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Connecting Your Applications: A Multi-Pronged Approach&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Docker Model Runner offers several ways for your applications and tools to connect to the local inference engine, providing flexibility for different development setups:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Internal DNS for Containerized Applications (model-runner.docker.internal):&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;How it works:&lt;/strong&gt; For applications running as Docker containers themselves (e.g., a backend API service), Model Runner provides a stable internal DNS name: &lt;a href=&quot;http://model-runner.docker.internal.&quot;&gt;http://model-runner.docker.internal.&lt;/a&gt;  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Benefit for Engineers:&lt;/strong&gt; This is incredibly convenient. Your containerized service can simply target this DNS name to reach the Model Runner API, without needing to know the host&amp;#x27;s IP address or worry about dynamic port mappings. It simplifies network configuration within your Docker environment.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Endpoint Example:&lt;/strong&gt; &lt;a href=&quot;http://model-runner.docker.internal/engines/v1/chat/completions&quot;&gt;http://model-runner.docker.internal/engines/v1/chat/completions&lt;/a&gt;  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Host TCP Port for Direct Access:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;How it works:&lt;/strong&gt; You can configure Model Runner to listen on a specific TCP port on your host machine. This is typically done via a Docker Desktop setting or a command like docker desktop enable model-runner --tcp \&amp;lt;port&amp;gt; (e.g., port 12434).  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Benefit for Engineers:&lt;/strong&gt; This allows applications running directly on your host (outside of Docker containers)—such as IDEs, local scripts, or standalone Java applications using Spring AI—to connect to the Model Runner.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Endpoint Example:&lt;/strong&gt; http://localhost:12434/engines/v1/chat/completions  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Docker Socket (Advanced/CLI Use):&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;How it works:&lt;/strong&gt; For direct interactions via the Docker API or for certain CLI scripting scenarios, the Docker socket (/var/run/docker.sock on Linux/macOS) can be used. API calls through the socket might have a specific path prefix (e.g., &lt;code&gt;/exp/vDD4.40/...&lt;/code&gt; as seen in early versions).  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Benefit for Engineers:&lt;/strong&gt; This offers a lower-level interface, useful for automation scripts or tools that integrate deeply with the Docker daemon.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This multi-faceted approach to connectivity ensures that whether your application is containerized, running natively on the host, or interacting via CLI tools, there&amp;#x27;s a clear and supported path to communicate with the local AI models managed by Docker Model Runner.  &lt;/p&gt;&lt;p&gt;Understanding these API mechanics and connection options is crucial for effectively integrating Docker Model Runner into your development workflows. It allows you to choose the most appropriate method for your specific application architecture and leverage the power of local AI models with ease.&lt;br/&gt;
In our next post, we&amp;#x27;ll explore how Docker Model Runner integrates with Docker Compose, enabling the orchestration of complex, multi-service AI applications locally.  &lt;/p&gt;&lt;p&gt;&lt;em&gt;This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Taming the Wild West of AI Model Management]]></title><link>https://layer5.io/blog/docker/taming-the-wild-west-of-ai-model-management</link><guid isPermaLink="false">https://layer5.io/blog/docker/taming-the-wild-west-of-ai-model-management</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/ad894700277bdd8c181ccbffc7dc4e16/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;In our [previous post](https://layer5.io/blog/docker/docker-model-runner), we introduced Docker Model Runner as a promising new toolkit for simplifying local AI development. Now, let&amp;#x27;s delve into one of its foundational—and perhaps most strategically significant—aspects: its deep reliance on the Open Container Initiative (OCI) standard for managing AI models.&lt;p&gt;If you&amp;#x27;ve wrestled with AI models, you know the &amp;quot;messy landscape&amp;quot; of model distribution. Models often arrive as loose files, tucked behind proprietary download tools, or lacking clear versioning. This fragmentation makes standardization, reproducibility, and integration into automated workflows a real headache for engineers. Docker Model Runner aims to bring order to this chaos by treating AI models as OCI artifacts, and this decision has profound implications for how you, as an engineer, can manage the entire lifecycle of your AI models.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;OCI: More Than Just docker model pull&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;You might see docker model pull &lt;code&gt;ai/llama3.2:1B-Q8_0&lt;/code&gt; and think it&amp;#x27;s just a convenient way to download models. But packaging models as OCI artifacts is a strategic move by Docker that goes far deeper. It aligns AI model management with the mature, robust ecosystem already built around OCI for container images.  &lt;/p&gt;&lt;p&gt;Essentially, Docker is working to make AI models &lt;strong&gt;first-class citizens within the Docker ecosystem&lt;/strong&gt;. This means the same trusted registries and workflows you use for your application containers can now, in principle, be applied to your AI models. Imagine the possibilities:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Unified Workflows:&lt;/strong&gt; Manage, version, and distribute your AI models using the same tools and processes you already use for your containerized applications. No more separate, bespoke systems for model management.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Leveraging Existing Infrastructure:&lt;/strong&gt; Your existing private container registries (like Docker Hub, Artifactory, Harbor, etc.) can become repositories for your AI models. This allows you to apply the same security scanning, access control policies, and auditing mechanisms you trust for your containers directly to your AI assets.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Engineering Benefits: What OCI Brings to Your AI Model Lifecycle&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Adopting OCI for models isn&amp;#x27;t just about tidiness; it brings tangible engineering benefits:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Robust Versioning &amp;amp; Provenance:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;How you benefit:&lt;/strong&gt; OCI&amp;#x27;s tagging system (e.g., :1B-Q8_0, :latest, :v2.1-finetuned) provides robust version control for your models. This is critical for reproducibility in experiments and ensuring stability in deployments. You can track exactly which model version was used for a particular result or release.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Immutability:&lt;/strong&gt; Like container images, OCI artifacts can be treated as immutable. Once a version is tagged and pushed, it remains consistent, preventing accidental modifications and ensuring that when you pull my-model:v1.0, you always get the same bits.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Streamlined CI/CD for ML Models:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;How you benefit:&lt;/strong&gt; This is a big one. Your existing CI/CD pipelines, likely already geared to handle OCI artifacts for application builds and deployments, can be extended to manage your AI models.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Think about it:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;Automated testing and validation of new model versions.  &lt;/li&gt;&lt;li&gt;Triggering model deployments based on updates in your model training repositories.  &lt;/li&gt;&lt;li&gt;Integrating model security scanning into your pipeline.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;This moves you closer to comprehensive MLOps automation by leveraging familiar tools and processes.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Enhanced Governance and Security:&lt;/strong&gt;  &lt;ul&gt;&lt;li&gt;&lt;strong&gt;How you benefit:&lt;/strong&gt; By storing models in your existing OCI-compliant registries, you can apply consistent governance. Use the same tools for vulnerability scanning on your models as you do for your container images. Enforce role-based access control (RBAC) to determine who can pull or push specific models or versions.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;strong&gt;The Future is Custom: Pushing Your Own Models&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;While Docker Model Runner currently provides access to curated models from Docker Hub (often under the ai/ namespace or from partners like Hugging Face via hf.co/), the real power of OCI will unlock when you can easily manage your &lt;em&gt;own&lt;/em&gt; custom models.  &lt;/p&gt;&lt;p&gt;The inclusion of commands like docker model push and docker model tag in the CLI strongly signals this future direction. Imagine:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Training or fine-tuning a model for your specific needs.  &lt;/li&gt;&lt;li&gt;Packaging it as an OCI artifact.  &lt;/li&gt;&lt;li&gt;Pushing it to your private or public OCI registry.  &lt;/li&gt;&lt;li&gt;Seamlessly pulling and running it with docker model pull your-namespace/your-custom-model:v1 and docker model run ....&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This capability will be transformative, allowing you to integrate bespoke AI directly into your standardized Docker workflows, free from vendor lock-in for model storage and distribution.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;A More Cohesive AI Development World&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;By embracing OCI, Docker Model Runner isn&amp;#x27;t just offering a new command; it&amp;#x27;s paving the way for a more unified and manageable AI development landscape. As an engineer, this means you can apply familiar, battle-tested DevOps principles and tools to your AI models, reducing complexity and accelerating your path from experimentation to production. This strategic choice for an open standard also offers a degree of future-proofing. As the AI ecosystem evolves, models packaged as OCI artifacts will likely be manageable by an ever-expanding array of tools and platforms that support this widely adopted standard.  &lt;/p&gt;&lt;p&gt;In our next post, we&amp;#x27;ll shift gears and look under the hood at Docker Model Runner&amp;#x27;s performance architecture, particularly its use of host-native execution and GPU acceleration.  &lt;/p&gt;&lt;p&gt;&lt;em&gt;This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Docker Model Runner]]></title><link>https://layer5.io/blog/docker/docker-model-runner</link><guid isPermaLink="false">https://layer5.io/blog/docker/docker-model-runner</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Thu, 27 Mar 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/dfd3200061f7dd06b9c2ae3283e13437/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;The shift towards local-first AI development is undeniable, driven by engineers seeking to overcome the practical hurdles of cloud-centric model interaction. Escalating API costs, data privacy concerns when handling sensitive information, network latency impacting iteration speed, and the desire for finer-grained control over execution environments have all highlighted the need for robust local solutions. Docker Model Runner is Docker&amp;#x27;s response to these engineering challenges, aiming to significantly streamline how we develop and test AI models locally.  &lt;/p&gt;&lt;p&gt;This post, the first in a series, will dissect Docker Model Runner from an engineering perspective. We&amp;#x27;ll explore its core technical value propositions and how you can leverage this new toolkit to enhance your AI development workflows.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;The Engineering Case for Local AI Development&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;For engineers, the &amp;quot;local-first&amp;quot; approach to AI isn&amp;#x27;t just a trend; it&amp;#x27;s a pragmatic choice offering tangible benefits:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Reduced Iteration Costs:&lt;/strong&gt; Experimenting with prompts, parameters, and model variations can lead to substantial API expenses. Local execution eliminates these costs during the crucial development and debugging phases.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Enhanced Data Privacy &amp;amp; Security:&lt;/strong&gt; Working with proprietary or sensitive datasets locally mitigates the risks associated with transmitting data to external services, a critical consideration for many enterprise applications.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Accelerated Development Cycles:&lt;/strong&gt; Eliminating network latency allows for near-instantaneous feedback, dramatically speeding up iterative tasks like prompt engineering, parameter tuning, and debugging model behavior.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Granular Environmental Control:&lt;/strong&gt; Local execution provides engineers with complete control over the model&amp;#x27;s runtime environment, dependencies, and specific configurations, facilitating reproducible experiments and precise debugging.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Docker Model Runner: Key Technical Capabilities for Engineers&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Docker Model Runner aims to integrate local AI model execution seamlessly into the familiar Docker ecosystem. Here are some of its core technical aspects beneficial for engineers:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Simplified Local Inference Setup:&lt;br/&gt;While the &amp;quot;Docker&amp;quot; name might imply traditional containerization for the model itself, Model Runner takes a different architectural path for performance. It facilitates running models like ai/llama3.2:1B-Q8_0 or hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF via commands such as docker model pull and docker model run. The key is that the inference itself often runs as a host-native process (initially leveraging llama.cpp), interacting with Docker Desktop or a Model Runner plugin. This design choice, which we&amp;#x27;ll explore in detail later, prioritizes direct hardware access.  &lt;/li&gt;&lt;li&gt;Performance through Host-Native Execution &amp;amp; GPU Access:&lt;br/&gt;To tackle the performance demands of LLMs, Model Runner enables the inference engine to directly access host resources. For macOS users with Apple Silicon, this means direct Metal API utilization for GPU acceleration. Windows GPU support is also on the roadmap. This approach aims to minimize the overhead often associated with virtualized GPU access in containerized environments, offering a potential speed advantage for local development.  &lt;/li&gt;&lt;li&gt;OpenAPI-Compatible API for Seamless Integration:&lt;br/&gt;One of the most significant engineering benefits is the provision of an OpenAI-compatible API. This allows you to reuse existing codebases, SDKs (like LangChain or LlamaIndex), and tools with minimal, if any, modification. For many, transitioning to a local model might be as simple as changing an API endpoint URL, drastically reducing the integration effort and learning curve.  &lt;/li&gt;&lt;li&gt;Standardized Model Management with OCI Artifacts:&lt;br/&gt;Docker Model Runner treats AI models as Open Container Initiative (OCI) artifacts. This is a strategic move towards standardizing model distribution, versioning, and management, aligning it with the mature ecosystem already in place for container images. This opens the door to leveraging existing container registries and CI/CD pipelines for models, a crucial step towards robust MLOps practices. We&amp;#x27;ll dedicate our next post to a deep dive into this OCI integration.&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;strong&gt;Beyond Single Invocations: The Potential for Local AI Pipelines&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;While running individual models is a core function, the architecture of Docker Model Runner also supports the local orchestration of more complex, multi-stage AI workflows. As detailed in examples like the Gemma 3 Comment Processing System, engineers can design and debug entire pipelines—involving synthetic data generation, categorization, embedding generation, feature extraction, and response generation—all on their local machines. This capability for end-to-end local development of AI-driven features is invaluable.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Engineering the Future of Local AI&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Docker Model Runner, even in its Beta phase (introduced with Docker Desktop 4.40, with APIs still evolving), presents a compelling toolkit for engineers looking to overcome the traditional challenges of local AI development. It offers a pathway to faster iteration, greater control, enhanced privacy, and reduced costs.&lt;br/&gt;
In our next post, we will delve into the technical specifics of how Docker Model Runner&amp;#x27;s use of &lt;strong&gt;OCI artifacts is set to revolutionize AI model management&lt;/strong&gt;, bringing DevOps principles to your MLOps workflows.&lt;br/&gt;
&lt;em&gt;This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Supercharge Your Git Workflow with Powerful Aliases]]></title><description><![CDATA[Git command line aliases and git shortcuts]]></description><link>https://layer5.io/blog/engineering/supercharge-your-git-workflow-with-powerful-aliases</link><guid isPermaLink="false">https://layer5.io/blog/engineering/supercharge-your-git-workflow-with-powerful-aliases</guid><dc:creator><![CDATA[Layer5 Team]]></dc:creator><pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/566411427e09e80368249dd7537f1ba8/hero-image.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;&lt;strong&gt;Tired of typing long Git commands?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Git is an incredibly powerful tool, but its command-line interface can sometimes feel cumbersome. Fortunately, Git allows you to create custom aliases to simplify your workflow. By assigning short, easy-to-remember names to frequently used commands, you can significantly boost your productivity and reduce the time spent on repetitive tasks.&lt;/p&gt;&lt;div&gt;&lt;iframe width=&quot;800&quot; height=&quot;490&quot; src=&quot;https://www.youtube.com/embed/vkk2jHUgbNQ?si=ohL-fnpZJDkwHO6w&quot; title=&quot;YouTube video player&quot; frameBorder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerPolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;p&gt;Video: `git lg` alias for a more visually appealing log&lt;/p&gt;&lt;/div&gt;&lt;h3&gt;Why Use Git Aliases?&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt;  Quickly execute complex commands with a single keystroke.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Consistency:&lt;/strong&gt; Reduce the risk of typos and errors in your Git commands.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Personalization:&lt;/strong&gt; Tailor your Git experience to your specific needs and preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Essential Git Aliases&lt;/h3&gt;&lt;p&gt;Here are some essential Git aliases that can revolutionize your workflow:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Navigation and Branching:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;git co:&lt;/strong&gt;  Quickly switch branches.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.co checkout&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git br:&lt;/strong&gt; List all branches.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.br branch&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git new:&lt;/strong&gt; Create a new branch and switch to it.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.new &amp;#x27;!git checkout -b&amp;#x27;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Staging and Committing:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;git a:&lt;/strong&gt; Stage all changes.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.a add&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git cm:&lt;/strong&gt; Commit with a message.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.cm commit -m&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git cam:&lt;/strong&gt; Amend the last commit.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.cam commit --amend -m&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git ca:&lt;/strong&gt; Stage all and commit with a message.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.ca &amp;#x27;!git add -A &amp;amp;&amp;amp; git commit -m&amp;#x27;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Viewing and Comparing:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;git st:&lt;/strong&gt; Check the state of your repository.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.st status&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git lg:&lt;/strong&gt; View a more visually appealing log.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.lg &amp;quot;log --color --graph --pretty=format:&amp;#x27;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&amp;lt;%an&amp;gt;%Creset&amp;#x27; --abbrev-commit&amp;quot;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git df:&lt;/strong&gt; Show the diff of unstaged changes.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.df diff&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git dc:&lt;/strong&gt; Show the diff of staged changes.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.dc diff --cached&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Undoing Changes:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;git undo:&lt;/strong&gt; Reset the last commit, keeping your changes.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.undo &amp;#x27;reset HEAD^&amp;#x27;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Remote Interactions:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;git fch:&lt;/strong&gt; Fetch all changes from remotes.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.fch fetch&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git pl:&lt;/strong&gt; Pull the latest changes from the current branch&amp;#x27;s remote.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.pl pull&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;git ps:&lt;/strong&gt; Push your local changes to the remote branch.&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.ps push&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Setting Up Git Aliases&lt;/h3&gt;&lt;p&gt;To set up these aliases, you can edit your global Git configuration file:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Open your &lt;code&gt;.gitconfig&lt;/code&gt; file:&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Global:&lt;/strong&gt; &lt;code&gt;~/.gitconfig&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Local:&lt;/strong&gt; &lt;code&gt;.git/config&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Add the aliases:&lt;/strong&gt; Use the &lt;code&gt;git config&lt;/code&gt; command to add each alias. For example:&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;git config --global alias.co checkout&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Streamline your cloud native workflow (just like git aliases)&lt;/h3&gt;&lt;p&gt;Just as git aliases simplify your development workflow, &lt;a href=&quot;/meshery&quot;&gt;Meshery&lt;/a&gt; streamlines the management of your cloud native infrastructure.  This CNCF project provides a unified platform to wrangle Kubernetes and other cloud native tools, so you can focus on building and deploying amazing applications.&lt;/p&gt;&lt;p&gt;By incorporating these Git aliases into your workflow, you can streamline your development process, reduce errors, and ultimately become a more efficient developer. Experiment with different aliases to find the perfect combination that suits your needs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Happy Git-ing!&lt;/strong&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Layer5 Launches Kanvas: A Collaborative Platform for Cloud Native Infrastructure]]></title><description><![CDATA[Layer5 announces Kanvas, a collaboration platform for engineering teams managing cloud native infrastructure. Built on Meshery, Kanvas provides an intuitive design suite for engineers to visualize, manage, and collaboratively design multi-cloud and Kubernetes-native infrastructure.]]></description><link>https://layer5.io/company/news/layer5-launches-kanvas-a-collaborative-platform-for-cloud-native-infrastructure</link><guid isPermaLink="false">https://layer5.io/company/news/layer5-launches-kanvas-a-collaborative-platform-for-cloud-native-infrastructure</guid><dc:creator><![CDATA[The Newsroom]]></dc:creator><pubDate>Fri, 15 Nov 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/d183a1a25111a497fe92979cfea33f09/kanvas-stacked-color.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Newsstyle__NewsWrapper-sc-12r6uiw-0 llCBzu&quot;&gt;&lt;p&gt;[Salt Lake City, UT] [KubeCon + CloudNativeCon] - November 14th, 2024 – &lt;a aria-current=&quot;page&quot; class=&quot;&quot; href=&quot;/&quot;&gt;Layer5&lt;/a&gt;, the open source company behind the popular &lt;a href=&quot;https://meshery.io&quot;&gt;Meshery&lt;/a&gt; project, announces Kanvas, a new collaboration platform that is like Google Workspace, but designed for engineering teams.&lt;/p&gt;&lt;a href=&quot;/static/layer5-kanvas-designer-4c521709f369987eecb50e3072328e9e.webp&quot;&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/layer5-kanvas-designer-4c521709f369987eecb50e3072328e9e.webp&quot; alt=&quot;Layer5 Kanvas Designer&quot; width=&quot;100%&quot; class=&quot;block-display align-center&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot;/&gt;&lt;/div&gt;&lt;/a&gt;&lt;p&gt;&lt;a href=&quot;https://kanvas.new/&quot;&gt;Kanvas&lt;/a&gt; is a multi-modal collaboration suite built atop one of the Cloud Native Computing Foundation’s highest velocity open source projects: Meshery. Kanvas’s two modes, Designer and Operator, offer declarative and imperative DevOps workflows, respectively. Both modes provide a visual interface for creating and managing complex cloud native infrastructure, expediting collaborative problem-solving, brainstorming and innovation, engineer onboarding, and auto-documented infrastructure. Importantly, Kanvas helps teams avoid finger-pointing and the blame-game by allowing them to be on the same page - literally.&lt;/p&gt;&lt;div class=&quot;blockquotestyle__BlockquoteStyle-sc-1yeq4hm-0 gRBWBw blockquote&quot; quote=&quot;Partnering on Dapr’s integration with Meshery has been eye-opening. Kanvas promotes collaboration in the design and discussion of cloud native applications, eliminating misunderstandings between teams. It is a game-changer for how we navigate the complexities of relationships between system components&quot; person=&quot;Mauricio Salatino&quot; title=&quot;Software Engineer at Diagrid and author of Platform Engineering on Kubernetes&quot;&gt;&lt;div class=&quot;blockquote-wrapper&quot;&gt;&lt;div class=&quot;blockquote-container&quot;&gt;&lt;h1 class=&quot;blockquote-quote&quot;&gt;Partnering on Dapr’s integration with Meshery has been eye-opening. Kanvas promotes collaboration in the design and discussion of cloud native applications, eliminating misunderstandings between teams. It is a game-changer for how we navigate the complexities of relationships between system components&lt;/h1&gt;&lt;h4 class=&quot;blockquote-person&quot;&gt;—Mauricio Salatino&lt;/h4&gt;&lt;h5 class=&quot;blockquote-title&quot;&gt;Software Engineer at Diagrid and author of Platform Engineering on Kubernetes&lt;/h5&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;As an extensible, self-service engineering platform with hundreds of technology integrations, supporting multi-cloud and Kubernetes native infrastructure, Meshery is the ideal management platform upon which to build Kanvas’ novel collaboration experience. Meshery has thousands of pre-built components supporting Kubernetes and Cloud services and with over 2,000 contributors, Meshery is the 9th fastest growing CNCF (out of 200+ projects).&lt;/p&gt;&lt;div class=&quot;blockquotestyle__BlockquoteStyle-sc-9kzfnh-0 cRwgvu blockquote&quot; quote=&quot;Internal developer platforms, like Meshery, are rising in popularity, because engineering teams remain siloed with disparate tooling, inconsistent workflow, lack of collaboration and shared process. A lack of collaboration plagues engineering teams with 83% of IT organizations implementing DevOps practices. 62% of organizations are stuck mid-DevOps evolution, teams tightly coupled and responsibilities diffused. Engineering teams desperately need to collaborate, but lack tooling specifically designed for cloud native infrastructure.&quot; person=&quot;Lee Calcote&quot; title=&quot;Layer5 founder&quot;&gt;&lt;div class=&quot;blockquote-wrapper&quot;&gt;&lt;div class=&quot;blockquote&quot;&gt;&lt;h4&gt;Internal developer platforms, like Meshery, are rising in popularity, because engineering teams remain siloed with disparate tooling, inconsistent workflow, lack of collaboration and shared process. A lack of collaboration plagues engineering teams with 83% of IT organizations implementing DevOps practices. 62% of organizations are stuck mid-DevOps evolution, teams tightly coupled and responsibilities diffused. Engineering teams desperately need to collaborate, but lack tooling specifically designed for cloud native infrastructure.&lt;/h4&gt;&lt;h5 class=&quot;person&quot;&gt;Lee Calcote&lt;/h5&gt;&lt;h5 class=&quot;title&quot;&gt;Layer5 founder&lt;/h5&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;Like Figma for engineers, Kanvas users can access Kanvas from any computer with an internet connection and a web browser.&lt;/p&gt;&lt;h4&gt;Feature Highlights:&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Infrastructure as Design&lt;/strong&gt;: Intuitive drag-and-drop interface for designing and visualizing cloud native infrastructure and general architecture diagrams. Supports Kubernetes and multi-cloud services.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Self-Service DevOps&lt;/strong&gt;: Empowers engineers to create, share, and manage their own environments on demand.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Greenfields and Brownfield Infrastructure&lt;/strong&gt;: Import existing cloud environments to visualize current infrastructure or create a new design from scratch.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;GitOps Integration&lt;/strong&gt;: Pull request integration for infrastructure design reviews.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Model-Driven&lt;/strong&gt; characterization of both semantic and non-semantic infrastructure as design components.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Policy-driven intelligent inference&lt;/strong&gt; of infrastructure components and their relationships.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Real-Time Collaboration&lt;/strong&gt;: Work with others on your designs in real-time, making it easier to collaborate and share ideas, while all changes are saved automatically. &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Design Patterns&lt;/strong&gt;: A catalog full of ready-made blueprints for common infrastructure and application architectures.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Kanvas Spaces&lt;/strong&gt;: provide a collaborative environment similar to Google Shared Drive, but specifically tailored for cloud-native infrastructure management.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Design Reviews&lt;/strong&gt;: Collaboratively review and provide feedback on designs and prototypes.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; style=&quot;margin-right:1.5rem;margin-left:1.5rem&quot; src=&quot;https://www.youtube.com/embed/4WcofErPTx4?si=UfouUV7mhADg3zkk&quot; title=&quot;YouTube video player&quot; frameBorder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerPolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;p style=&quot;font-style:italic;font-size:1rem;margin-left:1rem&quot;&gt;Birth of Kanvas from Meshery&lt;/p&gt;&lt;/div&gt;&lt;h4&gt;Kanvas caters to a wide range of users, including:&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Teams and engineering managers for brainstorming, diagramming, wireframing, and interviewing.&lt;/li&gt;&lt;li&gt;Platform engineers for underpinning self-service and developer empowerment.&lt;/li&gt;&lt;li&gt;Site reliability engineers for curating a catalog of design patterns as a center of excellence.&lt;/li&gt;&lt;li&gt;Operators for managing and visualizing infrastructure components.&lt;/li&gt;&lt;li&gt;Solution architects designing infrastructure across multiple cloud providers from a single canvas.&lt;/li&gt;&lt;li&gt;Developer advocates and educators for facilitating real-time exploration and asynchronous study of any cloud native technology.&lt;/li&gt;&lt;li&gt;Developers and product engineers for ease of understanding and design of their application infrastructure.&lt;/li&gt;&lt;li&gt;System integrators and consultants for a service provider-grade organization hierarchy, multi-tenant, white-labelable, highly extensible delivery platform.&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;blockquotestyle__BlockquoteStyle-sc-1yeq4hm-0 gRBWBw blockquote&quot; quote=&quot;Layer5 Kanvas is revolutionizing our approach to infrastructure design. With its collaborative environment, we&amp;#x27;re seeing significant gains in efficiency and cost-effectiveness, streamlining our workflows and fostering a shared understanding between our customers and partners. Any team that needs a shared space to collaborate and visualize ideas can benefit from using Kanvas.&quot; person=&quot;Yogi Porla&quot; title=&quot; CTO of Deeplineage&quot;&gt;&lt;div class=&quot;blockquote-wrapper&quot;&gt;&lt;div class=&quot;blockquote-container&quot;&gt;&lt;h1 class=&quot;blockquote-quote&quot;&gt;Layer5 Kanvas is revolutionizing our approach to infrastructure design. With its collaborative environment, we&amp;#x27;re seeing significant gains in efficiency and cost-effectiveness, streamlining our workflows and fostering a shared understanding between our customers and partners. Any team that needs a shared space to collaborate and visualize ideas can benefit from using Kanvas.&lt;/h1&gt;&lt;h4 class=&quot;blockquote-person&quot;&gt;—Yogi Porla&lt;/h4&gt;&lt;h5 class=&quot;blockquote-title&quot;&gt; CTO of Deeplineage&lt;/h5&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;Kanvas Designer is available now in beta as a service or self-hosted solution. Kanvas Operator will be available early next year. Try dragging and dropping your Kubernetes manifest into &lt;a href=&quot;https://kanvas.new&quot;&gt;https://kanvas.new&lt;/a&gt; today.&lt;/p&gt;&lt;h4&gt;Resources&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Kanvas application: &lt;a href=&quot;https://kanvas.new&quot;&gt;https://kanvas.new&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Design Catalog: &lt;a href=&quot;https://cloud.layer5.io/catalog&quot;&gt;https://cloud.layer5.io/catalog&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Kanvas website: &lt;a href=&quot;/cloud-native-management/kanvas&quot;&gt;https://layer5.io/cloud-native-management/kanvas&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Kanvas documentation: &lt;a href=&quot;https://docs.layer5.io/kanvas/&quot;&gt;https://docs.layer5.io/kanvas/&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Attend &lt;a href=&quot;https://layer5.io/community/events/kubecon-cloudnativecon-na-salt-lake-city-utah-2024&quot;&gt;KubeCon NA 2024 Session: Visualizing Kubernetes Resource Relationships with Meshery&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;h5&gt;About Layer5, Inc.&lt;/h5&gt;&lt;p style=&quot;font-size:1rem&quot;&gt;Our open source and commercial products empower organizations to embrace the power of cloud native with confidence. Layer5&amp;#x27;s mission is to simplify the adoption and operation of cloud native infrastructure, enabling organizations to innovate faster and engineers to do so collaboratively. Layer5’s award-winning open source community has over 10,000 members. For more information, visit &lt;a aria-current=&quot;page&quot; class=&quot;&quot; href=&quot;/&quot;&gt;https://layer5.io&lt;/a&gt;&lt;/p&gt;&lt;h5&gt;About Kanvas&lt;/h5&gt;&lt;p style=&quot;font-size:1rem&quot;&gt;Kanvas is a web-based collaboration tool that allows you to create, review, and operate highly-detailed  architecture diagrams of your cloud and cloud infrastructure using a drag-and-drop interface. Kanvas is popular with site reliability engineers, platform engineers, architects, operators, and developers as an enabler of productive, collaborative infrastructure management. Try Kanvas at &lt;a href=&quot;https://kanvas.new&quot;&gt;https://kanvas.new&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Meshery at KubeCon + CloudNativeCon NA 2024]]></title><description><![CDATA[Meshery sessions at KubeCon + CloudNativeCon NA 2024]]></description><link>https://layer5.io/blog/events/meshery-at-kubecon-cloudnativecon-na-2024</link><guid isPermaLink="false">https://layer5.io/blog/events/meshery-at-kubecon-cloudnativecon-na-2024</guid><dc:creator><![CDATA[Layer5 Team]]></dc:creator><pubDate>Sun, 10 Nov 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/4ee4ed80c9c20f5efdb8445ba92a90e3/kubeconna-2024.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;Join Layer5 at KubeCon + CloudNativeCon NA, Salt Lake City, Utah!&lt;/p&gt;&lt;p&gt;Join the Meshery project at KubeCon NA 2024 from November 11th to November 16th, 2024 and get introduced to collaborative cloud native management and Meshery open source maintainers.&lt;/p&gt;&lt;h3&gt;Session: Meshery - Visualizing Kubernetes Resource Relationships with Meshery&lt;/h3&gt;&lt;p&gt;Meshery and its extensions empower you to navigate cloud native infrastructure in complex environments. This lighting talk delves into the human-computer interaction (HCI) principles that underpin MeshMap&amp;#x27;s intuitive visualization of Kubernetes resources and the various forms of inter/relationships with other CNCF projects&amp;#x27; resources.&lt;p&gt;Human-Computer Interaction Principles in Meshery:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Cognitive Load: How Meshery reduces cognitive load by presenting complex information in a structured and visually digestible manner.&lt;/li&gt;&lt;li&gt;Mental Models: How Meshery aligns with users&amp;#x27; mental models of Kubernetes environments, facilitating comprehension and navigation.&lt;/li&gt;&lt;li&gt;Visual Perception: How Meshery leverages visual cues, colors, and layout to guide users&amp;#x27; attention and highlight critical information.&lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;&lt;div class=&quot;flex-row&quot; style=&quot;margin-bottom:2rem&quot;&gt;&lt;p&gt;Date: November 12, 2024&lt;br/&gt;Time: 3:04pm - 3:09pm MST&lt;/p&gt;&lt;a href=&quot;https://sched.co/1iWA9&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;button class=&quot;btnstyle__ButtonStyle-sc-mhxpaj-0 cOPfMh appion__btn&quot; title=&quot;See Details&quot;&gt; See Details&lt;/button&gt;&lt;/a&gt;&lt;/div&gt;&lt;h3&gt;Session: CNCF TAG Network - Intro &amp;amp; Deep Dive&lt;/h3&gt;&lt;p&gt;“It’s the network!” is the cry of every engineer. With the increased prevalence of microservices and distributed systems, it’s true - networking as a discipline has never been more critical in the well-architected design and efficient operation of modern infrastructure. Join this talk for an intro to the TAG, its charter and a deeper discussion of current cloud native networking topics being advanced in this TAG.&lt;/p&gt;&lt;div class=&quot;flex-row&quot; style=&quot;margin-bottom:2rem&quot;&gt;&lt;p&gt;Date: November 14, 2024 &lt;br/&gt;Time: 11:55am - 12:30pm MST&lt;/p&gt;&lt;a href=&quot; https://sched.co/1howx&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;button class=&quot;btnstyle__ButtonStyle-sc-mhxpaj-0 cOPfMh appion__btn&quot; title=&quot;See Details&quot;&gt; See Details&lt;/button&gt;&lt;/a&gt;&lt;/div&gt;&lt;h3&gt;Session: Contribfest - Meshery Contribfest: Extending the Cloud Native Manager&lt;/h3&gt;&lt;p&gt;Join the Meshery maintainers and community in improving the leading cloud native management plane. This is your chance to get hands-on with the tools shaping the future of collaborative cloud native management. Opportunities: Work on core functionality in the Server (Golang) or UI (React) or extend Meshery by building your own plugin. Contribute to the Meshery documentation by incorporating your own examples of cloud native solution architectures using Meshery Designer.&lt;/p&gt;&lt;p&gt;Why Contribute to Meshery? - Gain experience with cloud native technologies, including essentially every CNCF project and open source development practices. As is the 10th fastest growing CNCF project, Meshery has a vibrant community. Work alongside passionate maintainers and contributors. No Prior Experience Needed: We welcome contributions from all levels of experience. Join us at Meshery Contribfest and be part of the growing community shaping the future of collaborative cloud native management.&lt;/p&gt;&lt;div class=&quot;flex-row&quot; style=&quot;margin-bottom:2rem&quot;&gt;&lt;p&gt;Date: November 14, 2024 &lt;br/&gt;Time: 4:30pm - 6:00pm MST&lt;/p&gt;&lt;a href=&quot;https://kccncna2024.sched.com/event/1hoxN/contribfest-meshery-contribfest-extending-the-cloud-native-manager&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;button class=&quot;btnstyle__ButtonStyle-sc-mhxpaj-0 cOPfMh appion__btn&quot; title=&quot;See Details&quot;&gt; See Details&lt;/button&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[What is the Kanvas Catalog?]]></title><description><![CDATA[A comprehensive guide to the Kanvas Catalog, a collection of curated, pre-built, and ready-to-use cloud and cloud native infrastructure configurations.]]></description><link>https://layer5.io/blog/kanvas/what-is-the-kanvas-catalog</link><guid isPermaLink="false">https://layer5.io/blog/kanvas/what-is-the-kanvas-catalog</guid><dc:creator><![CDATA[Layer5 Team]]></dc:creator><pubDate>Tue, 05 Nov 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/e8dae3ed176ff95fd81e69c814a30306/catalog2.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;h3&gt;What is the Kanvas Catalog?&lt;/h3&gt;&lt;a href=&quot;/cloud-native-management/catalog&quot;&gt;Kanvas Catalog&lt;/a&gt; is a hub for sharing and discovering best practices, reusable templates, and operational patterns for Kubernetes and cloud-native infrastructure. It&amp;#x27;s like a marketplace where you can find and contribute pre-built infrastructure configurations and operational views. The Catalog is a part of the Kanvas platform, which is a comprehensive suite of tools for managing cloud-native infrastructure.&lt;div class=&quot;note&quot;&gt; &lt;a href=&quot;https://cloud.layer5.io/catalog&quot;&gt;Explore the catalog&lt;/a&gt;&lt;/div&gt;&lt;h3&gt;What can you find in the Catalog?&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Design Patterns: Ready-made blueprints for common infrastructure and application architectures. These patterns can save you significant time and effort in designing your deployments.  &lt;/li&gt;&lt;li&gt;Filters and Applications: Pre-configured filters for Envoy proxies, WebAssembly filters, and complete application deployments.&lt;/li&gt;&lt;li&gt;Meshery Designs: Share and reuse your own Meshery configurations, making it easier to collaborate and standardize your deployments.  &lt;/li&gt;&lt;li&gt;Meshery Models: Share and reuse your own Meshery models, making it easier to collaborate and standardize your component library.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Why is the Catalog useful?&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Accelerated Development: Leverage existing patterns to jumpstart your projects and avoid reinventing the wheel.&lt;/li&gt;&lt;li&gt;Community Knowledge: Benefit from the collective experience of the Layer5 community and industry best practices.  &lt;/li&gt;&lt;li&gt;Standardization: Promote consistency and reduce errors by using predefined configurations.&lt;/li&gt;&lt;li&gt;Collaboration: Share your own designs and contribute to the growing collection of patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;How can you contribute to the Catalog?&lt;/h3&gt;&lt;p&gt;You can contribute to the Catalog by creating high-quality starter templates and publishing designs for the community to use in various ways. You can also climb the leaderboard by having your designs cloned, downloaded, or viewed the most. Follow the instructions below to get started with your designs.&lt;/p&gt;&lt;h4&gt;Create or Import a Design&lt;/h4&gt;&lt;p&gt;Begin by creating a new design from scratch, using existing design patterns and templates from catalog:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From Scratch&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Open the Designs panel, Select and arrange components from the Designer Dock on the Kanvas, and customize with connections, labels, and properties.&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/start-from-scratch-21b5915673335277d79cd425627e8fe5.gif&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;From a Template&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Start from a pre-built template or clone an existing design from the Catalog. This allows you to build on established designs for a quicker start.&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/catalog-d6740356d3f91ec18d50c83e0d7af6d0.gif&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Import a Design&lt;/strong&gt;: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;Access the Kanvas Designer and select the &amp;quot;Import&amp;quot; button in the left Designs panel. Import your own designs from local filesystem or from a remote URL directly into the Catalog. Upload a file or provide a URL for Docker Compose, Helm Charts, Meshery Designs or Kubernetes Manifests. Choose to either import as new or merge into current design that you have open in Kanvas.&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/import-design-gif-9a9fa6ea76258783aa31d776bd5f2557.gif&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Kanvas will convert these into a usable design based on their configurations.&lt;/p&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/rendered-design-e00309ed95ec6b886d639defa806f939.png&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;h4&gt;Publish a Design&lt;/h4&gt;&lt;p&gt;Make your designs accessible to others by publishing them in the Catalog:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In the designs panel, locate your design and hover over it to access quick actions. Select the info button (marked with an &amp;quot;i&amp;quot;) and add any necessary details for the review process, such as relevant technologies, descriptions, and considerations and click Publish button. Once approved by the Maintainers, your design becomes available to the broader community in Kanvas catalog.&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/publish-to-catalog-ad82c7ecd418bd328187784f3605ad95.gif&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Share your designs&lt;/h4&gt;&lt;p&gt;Share your designs with your team members and effortlessly collaborate on designing and operating multi-cloud and Kubernetes native infrastrcutre with a seamless, built-in review mechanism.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Meet the Maintainer: Yash Sharma]]></title><description><![CDATA[Meet the Maintainer series with open source maintainer, Yash Sharma]]></description><link>https://layer5.io/blog/open-source/meet-the-maintainer-yash-sharma</link><guid isPermaLink="false">https://layer5.io/blog/open-source/meet-the-maintainer-yash-sharma</guid><dc:creator><![CDATA[Hargun Kaur]]></dc:creator><pubDate>Fri, 01 Nov 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/22186effa82abcb1fc7f888151e9e429/yash-sharma-meshery-maintainer.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;div class=&quot;MeetTheMaintainerstyle__MeetTheMaintainer-sc-cimr9h-0 cVWUJX&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Continuing in our Meet the Maintainer series, we have &lt;a href=&quot;/community/members/yash-sharma&quot;&gt;Yash Sharma&lt;/a&gt;. Yash is a maintainer of &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt;Meshery UI&lt;/a&gt;. In this interview, we get to know Yash a little better and learn about his journey as an open source project maintainer and with Layer5 community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;Yash, thank you for joining me today. Many people inside and outside of the Layer5 Community have seen the effects of your contributions, but may not know the backstory as to who Yash is and how you arrived at your maintainer role. Indulge us. How did you discover the Layer5 community? What made you stay?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;Thanks for having me, Hargun, I got into open source about two years ago, not really expecting to get so wrapped up in it. I found Layer5 and the community through a Twitter post and thought, &amp;quot;Why not?&amp;quot; So, I jumped in. The more I got involved, the more I saw how pumped the team and maintainers were about Meshery and helping others, and it just got me hooked. I started with some smaller, more routine issues but quickly moved on to bigger challenges and started getting recognized. One time, during a meeting, Lee said something that stuck with me, it’s become one of my go-to quotes. He was like, &amp;quot;You don&amp;#x27;t start working like a maintainer after becoming one, you act like a maintainer and do the work before you&amp;#x27;re promoted.&amp;quot; That really clicked for me. So I started thinking like a maintainer, doing the stuff that isn’t always glamorous but really matters. Eventually, I took on more responsibilities, and before I knew it, I became a maintainer.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;You’re a &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt;Meshery&lt;/a&gt; maintainer and have been for some long time now. What does being a Meshery maintainer mean to you?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;Honestly, being a maintainer is about taking responsibility for a big part of the project with taking care of small things too at the same time, and I really enjoy that. One of my favourite things about it is that it lets me think more broadly about what we’re doing. As a contributor, you often focus on the details, but as a maintainer, you get to step back and see the bigger picture, which is super rewarding.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;Have you worked with any other open source project? How does Layer5 compare?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;I’ve contributed to other open source projects, but I didn’t really get a chance to get involved deeply. I think it’s mostly because a lot of those projects don’t have an active community, which makes it tough to get involved. However, when I joined Layer5, everything changed. I quickly figured out how to start contributing, and the community managers were super helpful and always around to give a hand. You just don’t see that kind of support in a lot of other open source projects.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;&lt;a href=&quot;/projects&quot;&gt;Layer5 projects&lt;/a&gt; have a number of active, open source projects. You’ve been consistently contributing to a few of them. Which one(s) are you currently focusing on?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;Absolutely, Layer5 has a number of active open source projects, and I’ve contributed to quite a few of them. But my main focus has been on Meshkit, Sistent, Meshery Extensions, and, of course, Meshery itself.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;What’s the coolest Meshery demo you have done/seen?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;Haha good one, this year I got a chance to represent Meshery at KubeCon which was really an amazing experience for me. I gave a demo of Kanvas and during representation and during that I was collaborating with another user which really showed Meshery’s capability.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;What is your favorite feature in Meshery UI?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;&lt;a href=&quot;https://meshery.io/&quot;&gt;Meshery&lt;/a&gt; is an extensible platform and I like this feature of it, it gives Meshery a whole new powerful capability to interact with various technologies in the ecosystem and use it as an extension. My favourite one is Kanvas, though.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;What is your hot tip for working with Meshery that others may not know?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;I definitely encourage you to join the meetings. They’re a great way to connect with the maintainers in real time. Text chats just don’t do it justice when it comes to letting them really get to know you. Video calls are way better for that and give you a chance to show yourself to the project leaders and maintainers.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;Where do you see opportunities for contributors to get involved within Meshery and Layer5 community?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;Meshery is growing very fast, it is the 9th fastest growing project under CNCF. There are many opportunities open for you to contribute. Start with joining community, introducing yourself and sharing your skill. This will help community managers to put you in right direction based on your skills. All areas are consistently in need of help, including best at this time to contribute like Meshery&amp;#x27;s CLI client, Meshery Server and digital marketing.&lt;br/&gt;Out of a 100+ repos that the Layer5 community stewards, two are currently closed: Kanvas and Cloud. Both of these projects were created and are actively being developed by open source contributors, though. There are ~15 open source contributors who working on it now, who meet daily. New contributors in the community routinely express interest and are invited to participate, and are extremely well supported as they do. In other words, the source is currently closed (subject to change), but any community member here that shows interest, demonstrates competence and consistency of participation is readily invited to join in.&lt;br/&gt;The core team working on these two projects meets Mon, Tue, Wed, Fri at 9:00am Central / 7:30pm IST on &lt;a href=&quot;https://meet.layer5.io/team&quot;&gt;meet.layer5.io/team&lt;/a&gt;. You are most encouraged and welcome to partake.&lt;br/&gt;Between the two projects, ~100 different contributors have helped create these two project to date.&lt;a href=&quot;https://layer5.io/community/handbook/repository-overview&quot;&gt; This&lt;/a&gt; list of repositories and the expandable note on the page offers some additional context as well.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;Your most often used emoji? Your preference: movie or book? Morning person or night owl? What have you worked on in the past six months that you’re particularly proud of?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;I’d definitely recommend reading &amp;#x27;Six Easy Pieces&amp;#x27; by Richard Feynman. It’s a fantastic book, especially if you’re a science lover like me, haha.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;Do you have any advice for individuals hopeful to become Layer5 contributors or potentially maintainers?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;Good question, I’d say think about the long-term. People are more likely to invest in you if they see you’re committed for the long term. If you seem like you’re just looking for a quick win, it sends a message to the maintainers that they might not want to put too much into you. And of course try to make high-quality contributions; it doesn&amp;#x27;t matter if it’s small or big.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Hargun:&lt;/span&gt;&lt;p&gt;In other words, whether your contribution is big or small, it sounds like aiming for high-quality contributions that add value to the projects is key.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Yash:&lt;/span&gt;&lt;p&gt;You summarized it perfectly, yes I agree with you Hargun.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;note&quot;&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/forklift.81115a37.svg&quot; height=&quot;100px&quot; width=&quot;100%&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;p&gt;The Meshery project moves at an impressive pace thanks to maintainers like Yash. Be like Yash. Join the &lt;a href=&quot;https://slack.layer5.io&quot;&gt;Layer5 Slack&lt;/a&gt; and say “hi&amp;quot;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Docker Build Check Failures]]></title><description><![CDATA[Linting Dockerfiles. Docker build fails with warning: FromAsCasing: ‘as’ and ‘FROM’ keywords’ casing do not match. Learn how to fix this and other build check warnings.]]></description><link>https://layer5.io/blog/docker/docker-build-check-failures</link><guid isPermaLink="false">https://layer5.io/blog/docker/docker-build-check-failures</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Wed, 09 Oct 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/2a7dde65e579f386ecbed3f2fc4606ac/docker-extension-meshery-logo.webp" length="0" type="image/webp"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;While building a Docker image for one of &lt;a href=&quot;/projects&quot;&gt;Layer5&amp;#x27;s open source projects&lt;/a&gt;, I ran into a build warning in one of the multi-stage Dockerfiles. Up to this point, while creating container images, I hadn&amp;#x27;t paid attention to a particular feature of `docker build`: &lt;i&gt;&lt;a href=&quot;https://docs.docker.com/reference/build-checks/&quot;&gt;build checks&lt;/a&gt;&lt;/i&gt;. I had otherwise overlooked warnings and informational notices that were lost among the fray of docker build log lines. I&amp;#x27;ve stopped ignoring them, however. In this post, I&amp;#x27;ll touch upon a handful of the more common build warnings, delve into the importance of these checks, and explore how and why to address each. But, first...&lt;div class=&quot;blockquotestyle__BlockquoteStyle-sc-1yeq4hm-0 gRBWBw blockquote&quot; style=&quot;width:100%&quot; person=&quot;Lee Calcote&quot; quote=&quot;Build checks are your Dockerfile linter.&quot;&gt;&lt;div class=&quot;blockquote-wrapper&quot;&gt;&lt;div class=&quot;blockquote-container&quot;&gt;&lt;h1 class=&quot;blockquote-quote&quot;&gt;Build checks are your Dockerfile linter.&lt;/h1&gt;&lt;h4 class=&quot;blockquote-person&quot;&gt;—Lee Calcote&lt;/h4&gt;&lt;h5 class=&quot;blockquote-title&quot;&gt;&lt;/h5&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;h3&gt;Why Use Docker Build Checks?&lt;/h3&gt;&lt;p&gt;Now, of course we&amp;#x27;re aware that &lt;code&gt;docker build&lt;/code&gt; is a bread and butter command (very commonplace), however, if you&amp;#x27;re like me, you might have overlooked the &lt;code&gt;--check&lt;/code&gt; flag. Think of build checks as a linter for your Dockerfile, and as it turns out, build checks are a valuable tool for validating your Dockerfile and build options (flags) against established best practices. Build checks help you identify potential issues early on, too, which I&amp;#x27;ve come to appreciate. In fact, whereas am occassionally annoyed by overzealous linting rules, in general, I&amp;#x27;ve turned the corner with lint checks as when you&amp;#x27;re maintaining large codebases with a couple thousand contributors, it becomes essential to enforce some uniformity and idiomatic coding. &lt;/p&gt;&lt;p&gt;They analyze your build configuration and provide feedback on potential problems, such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Inefficiencies:&lt;/strong&gt;  Checks can identify unnecessary layers or overly large images, helping you optimize for size and performance.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Security risks:&lt;/strong&gt;  They can flag potential security vulnerabilities, like using outdated base images or insecure commands.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Maintainability issues:&lt;/strong&gt; Checks can highlight inconsistencies and deviations from best practices, improving the readability and maintainability of your Dockerfiles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Under the Hood of Build Checks&lt;/h3&gt;&lt;p&gt;Introduced in Dockerfile 1.8, build checks are integrated into the build process itself. When you invoke a build with the &lt;code&gt;--check&lt;/code&gt; flag, Docker analyzes your Dockerfile and build options &lt;em&gt;before&lt;/em&gt; executing any build steps. This pre-emptive analysis allows for early detection of potential issues, saving you time and resources.&lt;/p&gt;&lt;p&gt;Docker employs a rule-based system to perform these checks. Each rule targets a specific aspect of your Dockerfile, such as image size, security practices, or Dockerfile syntax. These rules are continuously updated to reflect evolving best practices and address new vulnerabilities.&lt;/p&gt;&lt;p&gt;As you familiarize yourself with build checks, you&amp;#x27;ll notice that they are categorized into three levels, which are pretty self-evident, but worth noting, so that you can prioritize your remediation efforts. These levels are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Errors:&lt;/strong&gt;  These are critical issues that prevent the build from completing successfully. Errors must be resolved before you can proceed with the build.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Warnings:&lt;/strong&gt;  Warnings indicate potential problems that don&amp;#x27;t halt the build but should be addressed to ensure best practices.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Info:&lt;/strong&gt;  Informational messages provide additional context or suggestions for improving your Dockerfile. While not critical, acting on these messages can enhance your Dockerfile&amp;#x27;s quality.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Common Warnings and Remediation&lt;/h2&gt;&lt;p&gt;Before we dive into some common warnings and how to address them, let&amp;#x27;s review how to run build checks on a Dockerfile.&lt;/p&gt;&lt;h4&gt;Running Build Checks&lt;/h4&gt;&lt;p&gt;To run build checks, simply use the &lt;code&gt;--check&lt;/code&gt; flag with your &lt;code&gt;docker build&lt;/code&gt; command:&lt;/p&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-bash&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;docker build --check .&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;Yes, quite straightforward. This command triggers build checks for the Dockerfile in the current directory. If any issues are detected, Docker will display the corresponding warnings or errors.&lt;/p&gt;&lt;p&gt;Now, we&amp;#x27;re ready to explore some frequently encountered warnings and how to address them. Here are some of the warnings that I&amp;#x27;ve run into, what they mean, and how to fix them:&lt;/p&gt;&lt;h3&gt;1. FromAsCasing&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;WARN: FromAsCasing: &amp;#x27;as&amp;#x27; and &amp;#x27;FROM&amp;#x27; keywords&amp;#x27; casing do not match&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; This warning occurs when the &lt;code&gt;FROM&lt;/code&gt; and &lt;code&gt;AS&lt;/code&gt; keywords in a multi-stage build have different casing (e.g., &lt;code&gt;FROM&lt;/code&gt; and &lt;code&gt;as&lt;/code&gt;). While Docker allows both uppercase and lowercase, mixing casing can hinder readability. Consistency in casing enhances readability and maintainability, making your Dockerfile easier to understand for both yourself and collaborators.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Remediation:&lt;/strong&gt;  Ensure consistent casing for &lt;code&gt;FROM&lt;/code&gt; and &lt;code&gt;AS&lt;/code&gt; keywords throughout your Dockerfile.&lt;/li&gt;&lt;/ul&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-docker&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Incorrect&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;FROM ubuntu:latest as builder&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Correct&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;FROM ubuntu:latest AS builder&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h3&gt;2. LatestType&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;WARN: LatestType: &amp;#x27;latest&amp;#x27; tag used for base image&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; Using the &lt;code&gt;latest&lt;/code&gt; tag can lead to unpredictable builds since the base image may change unexpectedly. Pinning to specific tags ensures reproducible builds, preventing unexpected behavior due to changes in the base image. This is crucial for production environments where stability is paramount.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Remediation:&lt;/strong&gt;  Specify a specific tag for your base image to ensure consistency and reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-dockerfile&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Incorrect&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;FROM ubuntu:latest&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Correct&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;FROM ubuntu:22.04&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h3&gt;3. AptGetNoInstallRecommends&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;WARN: AptGetNoInstallRecommends: &amp;#x27;apt-get install&amp;#x27; with no &amp;#x27; --no-install-recommends&amp;#x27; flag&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Explanation:&lt;/strong&gt;  Installing recommended packages can increase image size and potentially introduce unnecessary dependencies. Recommended packages often include extra dependencies that bloat your image size. This can lead to increased storage costs, slower downloads, and potentially longer build times.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Remediation:&lt;/strong&gt;  Use the &lt;code&gt;--no-install-recommends&lt;/code&gt; flag with &lt;code&gt;apt-get install&lt;/code&gt; to avoid installing recommended packages.&lt;/li&gt;&lt;/ul&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-dockerfile&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Incorrect&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y package-name&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Correct&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends package-name&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h3&gt;4. RunCommandWithNoExecForm&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;WARN: RunCommandWithNoExecForm: &amp;#x27;RUN&amp;#x27; command with no &amp;#x27;exec&amp;#x27; form&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; Using the exec form (&lt;code&gt;RUN [&amp;quot;executable&amp;quot;, &amp;quot;param1&amp;quot;, &amp;quot;param2&amp;quot;]&lt;/code&gt;) for RUN commands is generally more efficient and avoids potential issues with shell interpretation. The exec form provides better performance and avoids potential shell injection vulnerabilities by directly executing the command without shell processing.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Remediation:&lt;/strong&gt; Use the exec form for &lt;code&gt;RUN&lt;/code&gt; commands whenever possible.&lt;/li&gt;&lt;/ul&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-dockerfile&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Incorrect&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y package-name&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Correct&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN [&amp;quot;apt-get&amp;quot;, &amp;quot;update&amp;quot;]&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN [&amp;quot;apt-get&amp;quot;, &amp;quot;install&amp;quot;, &amp;quot;-y&amp;quot;, &amp;quot;--no-install-recommends&amp;quot;, &amp;quot;package-name&amp;quot;]&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;h3&gt;5. RedunantAptUpdate&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;WARN: RedundantAptUpdate: &amp;#x27;apt-get update&amp;#x27; found in multiple RUN instructions&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; Running &lt;code&gt;apt-get update&lt;/code&gt; multiple times within a Dockerfile is inefficient. Each &lt;code&gt;apt-get update&lt;/code&gt; fetches package information from repositories. Repeating this unnecessarily consumes bandwidth and increases build time.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Remediation:&lt;/strong&gt;  Combine &lt;code&gt;apt-get update&lt;/code&gt; with the subsequent &lt;code&gt;apt-get install&lt;/code&gt; command in a single &lt;code&gt;RUN&lt;/code&gt; instruction.&lt;/li&gt;&lt;/ul&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde&quot;&gt;&lt;button class=&quot;CodeBlock__CopyCode-sc-4qx7vm-2 TeiOI&quot;&gt;Copy&lt;/button&gt;&lt;pre class=&quot;CodeBlock__Pre-sc-4qx7vm-0 fvvcde prism-code language-dockerfile&quot; style=&quot;color:#d6deeb;background-color:#011627&quot;&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Incorrect&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN apt-get update&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN apt-get install -y package-name&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token plain&quot; style=&quot;display:inline-block&quot;&gt;
&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;# Correct&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;token-line&quot; style=&quot;color:#d6deeb&quot;&gt;&lt;span class=&quot;CodeBlock__LineNo-sc-4qx7vm-1 iWbVnM&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;token plain&quot;&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y package-name&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;p&gt;These are just the checks that I ran into. See the full list of available checks by running &lt;code&gt;docker build --help&lt;/code&gt; or visiting the &lt;a href=&quot;https://docs.docker.com/reference/build-checks/&quot;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;Beyond the Build Check Basics&lt;/h2&gt;&lt;p&gt;Now, that you&amp;#x27;ve gotten started with Dockerfile optimatization, you can further customize build checks to suit your needs. Know that you&amp;#x27;re not limited to just enabling or disabling all checks, but that Docker offers granular control, allowing you to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Skip specific checks:&lt;/strong&gt;  If a particular check isn&amp;#x27;t relevant to your use case, you can skip it using the &lt;code&gt;--check=rule1,rule2,!rule3&lt;/code&gt; syntax. This enables &lt;code&gt;rule1&lt;/code&gt; and &lt;code&gt;rule2&lt;/code&gt; while skipping &lt;code&gt;rule3&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Fail on warnings:&lt;/strong&gt; By default, warnings don&amp;#x27;t halt the build process. However, you can enforce stricter compliance by using the &lt;code&gt;--fail-on=warn&lt;/code&gt; flag, ensuring any warning triggers a build failure.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Explore experimental checks:&lt;/strong&gt;  Push the boundaries with experimental checks by using the &lt;code&gt;--check=experimental&lt;/code&gt; flag. These checks offer a glimpse into future best practices and can help you stay ahead of the curve.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Build checks are not just about fixing warnings; they are about proactively adopting best practices. Up your Dockerfile game with these additional tips:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Embrace multi-stage builds:&lt;/strong&gt;  Leverage multi-stage builds to create smaller, more efficient images by separating build-time dependencies from runtime essentials.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Utilize a &lt;code&gt;.dockerignore&lt;/code&gt; file:&lt;/strong&gt; Exclude unnecessary files from your build context to reduce image size and speed up builds.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Order your instructions strategically:&lt;/strong&gt; Place frequently changing instructions towards the end of your Dockerfile to take advantage of Docker&amp;#x27;s layer caching.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By understanding the mechanics of build checks and actively addressing the warnings, you can unlock the full potential of Docker and build robust, efficient, and secure containerized applications.&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Meet the Maintainer: Aisuko Li]]></title><description><![CDATA[Meet the Maintainer series with open source maintainer, Aisuko Li]]></description><link>https://layer5.io/blog/open-source/meet-the-maintainer-aisuko-li</link><guid isPermaLink="false">https://layer5.io/blog/open-source/meet-the-maintainer-aisuko-li</guid><dc:creator><![CDATA[Anita Ihuman]]></dc:creator><pubDate>Fri, 27 Sep 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/efc06b9b581cfec422e3ea89fb87f7e3/aisuko-li-layer5-maintainer.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;div class=&quot;MeetTheMaintainerstyle__MeetTheMaintainer-sc-cimr9h-0 cVWUJX&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Continuing in our Meet the Maintainer series, we have &lt;a href=&quot;/community/members/aisuko-li&quot;&gt;Aisuko Li&lt;/a&gt;. Aisuko is a maintainer of the &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt; Meshery Adapters&lt;/a&gt; project. In this interview, we get to know Aisuko a little better and learn about his journey as an open source project maintainer and with Layer5 community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;Aisuko, thank you for joining me today. Many people inside and outside of the Layer5 Community have seen the effects of your contributions, but may not know the backstory as to who Aisuko is and how you arrived at your maintainer role. Indulge us. How did you discover the Layer5 community? What made you stay?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;Thanks for having me here. Actually, Aisuko is my code name. My real name is Bowen Li. I love both of I used to work for RancherLabs for a while, and I worked to maintain the official Helm (a third-party management tool for Kubernetes manifests) charts repo. These experiences helped me contribute to creating and maintaining Meshery Helm charts.&lt;br/&gt;I like open source software, and I love contributing to the community. The more you contribute, the more permission you get to help the community grow and improve.&lt;br/&gt;The Layer5 community is a true open source community. Everyone here can find a comfortable role. I have been here since 2019 (a long time ago). I’ve seen new members join and some leave. It&amp;#x27;s great to see people work together without any other conditions. This is one of the ways I have fun.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;You’ve been consistently contributing to a large number of Layer5 projects (Meshery adapters, mesheryctl, SMI, SMP). Layer5 has a large collection of active projects. Which one are you currently focusing on? &lt;i&gt;Psst.&lt;/i&gt; Also, which one’s your favorite? I won’t tell.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;Actually, the Meshery project in 2021-2022 has changed a lot. More skilled and talented contributors joined the community. They are so professional and active, and their hard work has made Meshery more powerful than before. For instance, projects like `meshkit` and `meshsync` have grown significantly. It’s great to have such a strong team working together.&lt;br/&gt;Right now, I am primarily focusing on the `meshery-operator` project and `meshery-linkerd`, along with fixing bugs across all the projects. I always aim to make all the projects more controllable and maintain high code quality.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;Have you worked with any other open source projects? How does Layer5 compare?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;I was active in the Rancher community and the Helm charts project, where I owned three charts. I’m also still a maintainer of the GNU Hurd. Recently, I’ve been working on contributions to Kubernetes community projects as well.&lt;br/&gt;Compared to the Layer5 community, the Kubernetes community is much larger. Many members are not very active, so it can be difficult to get feedback on PRs and issues from inactive members.&lt;br/&gt;The GNU Hurd project is unique, so there’s no need to compare it with others. In the Layer5 community, we have a warm welcome for new contributors, and most projects have active reviewers who provide feedback quickly.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;Fascinating. Why did you pick service meshes specifically, though?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;I have worked with many middle- and small-sized companies that wanted to migrate to the cloud. It’s easy to move to Kubernetes, but it’s hard to ensure everything runs smoothly. You have limited visibility into what’s happening in the cluster, and service mesh solves that problem by showing real-time traffic.&lt;br/&gt;Service mesh provides direct insights into traffic flows, which is its most useful feature.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;Haha. Leading on from that, what should Meshery dream about next? What can we hope to contribute to the service mesh landscape in your opinion?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;I once talked to Lee, the founder of Layer5. Due to time zone differences, we don’t get many chances to discuss things directly. But I believe we don’t need to create a new service mesh.&lt;br/&gt;What we should do is provide third-party performance tools for existing service mesh projects. We should give the choice back to the users, letting them pick the service mesh that best suits their needs.&lt;br/&gt;We should contribute to SMI and CNCF projects, helping to define performance standards for the cloud-native industry. That’s why I’m keen on joining these communities.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;Interesting. Do expand on that. What do you think Meshery could offer, in addition to what it already does?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;I believe we can offer a CNCF-standard performance tool for all service mesh applications. We can collaborate with service mesh maintainers to define these standards, which would be beneficial for end-users. It’s similar to what we did with SMI.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;What are today&amp;#x27;s challenges when working with service meshes?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;Even though service mesh has many features, it&amp;#x27;s still not always stable in production. I remember that even Istio (v1.1x) couldn’t be upgraded to newer versions easily.&lt;br/&gt;Additionally, we don’t often get test results from real production environments. Right now, the focus is on multi-cluster service mesh capabilities, which brings new challenges.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;That’s good to hear. What do you think we should look forward to with respect to service mesh development?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;I’ve worked with service mesh applications like Linkerd2, Istio, and OSM in development environments. OSM is my preference because it’s modular and has a simpler architecture compared to others.&lt;br/&gt;From my experience, I believe that not all environments need all the features of a service mesh. Some middle or small companies may only need visibility into traffic flows without complex features like mTLS.&lt;br/&gt;So, we should focus on simple architecture and features. For example, integrating traffic visualization with Ingress, so users don’t need to create new custom resources to track traffic.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;Ah, while I have you here, let me get more reading recommendations lined up. Cloud Native and especially the field of service meshes is evolving exceptionally fast. Keeping up with all the developments can be challenging. Which resources do you use to stay up-to-date?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;People are always interested in new technology, but we are limited by time. I believe that continuing to contribute to the CNCF open source community is the best way to stay updated.&lt;br/&gt;Articles and news may include the author’s personal opinions, and we don&amp;#x27;t always know if they have strong relationships with the community. We should maintain critical thinking and focus on solving real-world problems. The best way to learn is through hands-on experience.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;What does being a Meshery maintainer mean to you? How has being a maintainer impacted your full-time role?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;It’s an honor to be a maintainer of the Meshery community. The membership is a reward for contributing to the community. Being a maintainer has made me more enthusiastic about contributing to open source projects. It has also given me confidence to contribute to other projects.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Anita:&lt;/span&gt;&lt;p&gt;Do you have any advice for individuals hoping to become Layer5 contributors or potentially maintainers?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aisuko:&lt;/span&gt;&lt;p&gt;The Layer5 and Meshery communities are always welcoming to everyone. New features are great, but there’s more to contributing than just code. For example, writing unit tests and code comments is just as important as adding new features.&lt;br/&gt;One of our goals is to provide an opportunity for everyone who wants to contribute to open source projects, so we need to maintain a high level of code quality.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;note&quot;&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/forklift.81115a37.svg&quot; height=&quot;100px&quot; width=&quot;100%&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;p&gt;The Meshery project moves at an impressive pace thanks to maintainers like Aisuko. Be like Aisuko. Join the &lt;a href=&quot;https://slack.layer5.io&quot;&gt;Layer5 Slack&lt;/a&gt; and say “hi&amp;quot;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Meet the Maintainer: Hussaina Begum]]></title><description><![CDATA[Meet the Maintainer series with open source maintainer, Hussaina Begum]]></description><link>https://layer5.io/blog/open-source/meet-the-maintainer-hussaina-begum</link><guid isPermaLink="false">https://layer5.io/blog/open-source/meet-the-maintainer-hussaina-begum</guid><dc:creator><![CDATA[Vivek Vishal]]></dc:creator><pubDate>Sun, 08 Sep 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/f8cde22decf106caae36d310f02e1552/hussaina-begum-meshery-maintainer.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;div class=&quot;MeetTheMaintainerstyle__MeetTheMaintainer-sc-cimr9h-0 cVWUJX&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Continuing in our Meet the Maintainer series, we have &lt;a href=&quot;/community/members/hussaina-begum&quot;&gt;Hussaina Begum&lt;/a&gt;, Staff Engineer at VMware. Hussaina is a maintainer of &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt;Meshery CLI&lt;/a&gt;. In this interview, we get to know Hussaina a little better and learn about her journey as an open source project maintainer and with Layer5 community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Hussaina, thank you for joining me today. Many people inside and outside of the Layer5 Community have seen the effects of your contributions, but may not know the backstory as to who Hussaina is and how you arrived at your maintainer role. Indulge us. How did you discover the Layer5 community? What made you stay?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;I wanted to contribute to open source for a long time and finally made up my mind to get into the world of open source during Hacktoberfest 2020. Meshery was one of the projects I contributed to and it had a well defined structure and meetings for everyone - Newcomers call, Developers call, Websites (Frontend development) call, CI/CD call and Community call, etc. Also, the MeshMate program where the existing Meshery maintainers help the newest members to get started. Meshery project aligned with my networking background and my goal towards backend development using golang. Last but not the least, lighthearted jokes from Lee during the development call.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;You’re a Meshery Maintainer and have been for some long time now. What does being a Meshery maintainer mean to you?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;Lots of learning. Though I started out small in Meshery with a documentation PR, Meshery community had so much knowledge to share. With their support, I could start on `mesheryctl` UX improvements. I have seen many bright engineers implementing great features in Meshery. I draw inspiration from them and try to pay it forward by helping new members of the community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;&lt;a href=&quot;/projects&quot;&gt;Layer5 projects&lt;/a&gt; has a number of active, open source projects. You’ve been consistently contributing to a few of them. Which one(s) are you currently focusing on?&lt;i&gt;Psst.&lt;/i&gt; Also, which one’s your favorite? I won’t tell.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;I have contributed to mesheryctl, meshkit and meshery.io projects so far, It&amp;#x27;s hard to pick one.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What’s the coolest Meshery demo you have done/seen?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;The &lt;a href=&quot;https://playground.meshery.io&quot;&gt;Meshery Playground&lt;/a&gt; is an awesome system and a something of a hidden gem in cloud native ecosystem. I also remember the `mesheryctl perf` CLI demo I have done on the Meshery office hours from one of the Kubecon events and am proud of this. Kudos to all the contributors of `mesheryctl` command development and refactors.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What do you anticipate will be Meshery’s biggest announcement this year?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;Meshery project moving to Incubating status! 🙂&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What is your favorite Meshery CLI command?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;mesheryctl system start&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What is your hot tip for working with Meshery that others may not know?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;Use Slack effectively and leverage discuss forum, talk to MeshMates and maintainers, they are all ears to the issues you face while starting out.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What are some personal goals for the next year with respect to the Meshery and the Layer5 community?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;Work on the long pending `system config` CLI improvements and contribute to other Meshery projects.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Interesting. Do expand on that: What do you think Meshery could offer in addition to what it already does?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;Labs designed in a way that they work with local resources.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Your most often used emoji? Your preference: movie or book? Morning person or night owl? What have you worked on in the past six months that you’re particularly proud of?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;eyes emoji. My preference is a book compared to a movie. I am a Night Owl w.r.t. work, however, I use my early mornings for my fitness goals. I have not been active lately, however removing dependency on CLIs in `mesheryctl system config` would be something I would consider a good milestone item for me.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;The cloud native ecosystem moves quickly. Keeping up with all the developments can be challenging. How do you stay up-to-date?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;I like listening to kubernetes podcasts and I follow the internal slack channels for upstream activities at VMware. Also, attending the local KCD events helped me understand the latest developments across various k8s projects. Being a program committee member has given me good understanding of developments across a specific technology area, let it be security and identity area or the world of service meshes.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Do you have any advice for individuals hopeful to become Layer5 contributors or potentially maintainers?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Hussaina:&lt;/span&gt;&lt;p&gt;Show up and attend the meetings which align with your expertise and areas you would like to learn or contribute to. Make use of the Meshmates and Meshery slack and discuss forums are pretty active and everyone is eager to help.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;note&quot;&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/forklift.81115a37.svg&quot; height=&quot;100px&quot; width=&quot;100%&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;p&gt;The Meshery project moves at an impressive pace thanks to maintainers like Hussaina. Be like Hussaina. Join the &lt;a href=&quot;https://slack.layer5.io&quot;&gt;Layer5 Slack&lt;/a&gt; and say “hi&amp;quot;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Meet the Maintainer: Aadhitya Amarendiran]]></title><description><![CDATA[Meet the Maintainer series with open source maintainer, Aadhitya Amarendiran]]></description><link>https://layer5.io/blog/open-source/meet-the-maintainer-aadhitya-amarendiran</link><guid isPermaLink="false">https://layer5.io/blog/open-source/meet-the-maintainer-aadhitya-amarendiran</guid><dc:creator><![CDATA[Vivek Vishal]]></dc:creator><pubDate>Sun, 08 Sep 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/436e06d306ef7e35d44324c48612eebf/aadhitya-amarendiran-layer5-maintainer.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;div class=&quot;MeetTheMaintainerstyle__MeetTheMaintainer-sc-cimr9h-0 cVWUJX&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Continuing in our Meet the Maintainer series, we have &lt;a href=&quot;/community/members/aadhitya-amarendiran&quot;&gt;Aadhitya Amarendiran&lt;/a&gt;. Aadhitya is a maintainer of the &lt;a href=&quot;/cloud-native-management/meshery&quot;&gt; Meshery CLI&lt;/a&gt; project. In this interview, we get to know Aadhitya a little better and learn about his journey as an open source project maintainer and with Layer5 community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Aadhitya, thank you for joining me today. Many people inside and outside of the Layer5 Community have seen the effects of your contributions, but may not know the backstory as to who Aadhitya is and how you arrived at your maintainer role. Indulge us. How did you discover the Layer5 community? What made you stay?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;It was around the year 2020 when I was a sophomore and explored many things in the field of open source. I got to know about Meshery via the LFX program and tried applying for it, though I was a newcomer at that time. Later, I learned more about Layer5, started attending the community call, and met the community. The community members helped me a lot wherever I got stuck as a newcomer, which made me learn new things and involve myself in the project, which later helped me grow. Oh, of course, great learning sessions from Lee during development and community calls.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;You’re a Meshery Maintainer and have been for some long time now. What does being a Meshery maintainer mean to you?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;Three things come into my mind: Learning lots of new things, Challenging yourself to your limits and being a helpful navigator for contributors. During my time as a newcomer, I started out with a simple readme fix PR in the Meshery project, which I thought would cause less impact. But the maintainers accepted my PR though it’s a very small one. That instilled a feeling in me that I should give back to the community by helping newcomers and contributors whenever they are stuck in work.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Have you worked with any other open source project? How does Layer5 compare?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;Not a lot, but I worked on quite a few open source projects. Layer5 is one of the best places to start if you are new to open source. By being involved in the community, you will feel and understand the spirit of open source.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;&lt;a href=&quot;/projects&quot;&gt;Layer5 projects&lt;/a&gt; have a number of active, open source projects. You’ve been consistently contributing to a few of them. Which one(s) are you currently focusing on?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;I currently work on Meshery, as it piqued my interest during my initial days. I also work on the Meshery-SMP GitHub action project as well.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What’s the coolest Meshery demo you have done/seen?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;I’ve seen a lot of demos but the coolest one for me is the Meshery Docker extension where you can start and use Meshery right from DockerHub!&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What is your favorite Meshery CLI Command?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;Oof! That’s a tricky one. But my favorite one is definitely &lt;code&gt;mesheryctl perf&lt;/code&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;What is your hot tip for working with Meshery that others may not know?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;If you’re starting out with Meshery, make sure to use the Meshery Playground if you want to get hands-on for the first time without the need to deploy Meshery in your system. After you get the basics right, install Meshery and log in to your deployed instance. You’ll see that your designs, performance test results and configurations remain intact in your instance as if they are present exactly the same in the Playground. There’s no need to start from scratch. Just continue where you left off!&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Where do you see opportunities for contributors to get involved within Meshery and Layer5 community?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;Considering the fact that Meshery is now a part of CNCF (especially the fact that we are aiming for the Incubation status as well!), I feel that Meshery has a wide range of scope for contributors to be involved in. Whether you’re an expert or a newbie, Meshery has lots of subdomains to contribute. Documentation, Frontend, Backend, Adapters… the list goes on.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Your most often used emoji? Your preference: movie or book? Morning person or night owl? What have you worked on in the past six months that you’re particularly proud of?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;Most used emoji: 😎. I’m a morning person usually and sleep early, but sometimes I’m a night owl when it comes to intense work. I’d prefer movies compared to books as for some they clearly adapt from books. I’ve worked on refactoring the &lt;code&gt;mesheryctl pattern&lt;/code&gt; command to &lt;code&gt;mesheryctl design&lt;/code&gt; without losing the core features present. This took me a bit of time as I had to balance my current work as well which caused a bit of inactivity. But I managed to complete it, and I’m proud of doing such great work!&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;Do you have any advice for individuals hopeful to become Layer5 contributors or potentially maintainers?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;Make your presence stand out from the crowd even if you are a beginner, and learn as much as you can. Seek MeshMates and maintainers if you get stuck in something. Ask questions during meets or in Slack, and get feedback on your PRs, doesn’t matter if it’s big or small. Incorporate feedback and improvise. Remember, communication is the key, and be active!&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Vivek:&lt;/span&gt;&lt;p&gt;In other words, whether your contribution is big or small, it sounds like aiming for high-quality contributions that add value to the projects is key.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Aadhitya:&lt;/span&gt;&lt;p&gt;Yes, you got it right! Even the smallest contribution which creates a good impact in a project becomes a great factor in Open source. All that matters is perseverance, challenging yourself to limits and learning. Do these things right and you’ll find yourself growing in the community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;note&quot;&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/forklift.81115a37.svg&quot; height=&quot;100px&quot; width=&quot;100%&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;p&gt;The Meshery project moves at an impressive pace thanks to maintainers like Aadhitya. Be like Aadhitya. Join the &lt;a href=&quot;https://slack.layer5.io&quot;&gt;Layer5 Slack&lt;/a&gt; and say “hi&amp;quot;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Ways to Contribute at Layer5]]></title><description><![CDATA[The State of Open Source]]></description><link>https://layer5.io/blog/open-source/ways-to-contribute-at-layer5</link><guid isPermaLink="false">https://layer5.io/blog/open-source/ways-to-contribute-at-layer5</guid><dc:creator><![CDATA[Lee Calcote]]></dc:creator><pubDate>Sat, 07 Sep 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/b7b6c399b06ecc47f667b497c05df8e6/ways-to-contribute.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;p&gt;The Layer5 community grows on a daily basis, which means that each day &lt;a href=&quot;/community/community-managers&quot;&gt;community managers&lt;/a&gt;, &lt;a href=&quot;/community/meshmates&quot;&gt;MeshMates&lt;/a&gt;, and other existing &lt;a href=&quot;/community/members&quot;&gt;community members&lt;/a&gt; field the question of &amp;quot;how can I contribute to your project&amp;quot; any number of times everyday. The good news is that there are many what is to engage - all of which are important to the health of both the projects and the community.&lt;/p&gt;&lt;p&gt;Upon joining the community Slack workspace, individuals are warmly greeted and encouraged to browse the &lt;a href=&quot;/community/handbook&quot;&gt;community handbook&lt;/a&gt;, join the weekly &lt;a href=&quot;/community/newcomers&quot;&gt;newcomers meeting&lt;/a&gt;, and to begin familiarizing our &lt;a href=&quot;/projects&quot;&gt;open source projects&lt;/a&gt;. We have heavily invest in each individual that shows up to participate as we consider their success, our success. Part of doing so is ensuring that everyone understands the variety of ways depending in which they can contribute. Any and all of the possible ways are encouraged. &lt;/p&gt;&lt;h3&gt;Community Engagement &amp;amp; Management&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;As a &lt;a href=&quot;/community/community-managers&quot;&gt;Community Manager&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Foster discussions and interactions on community platforms (Slack, forums, social media).&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;/newcomers&quot;&gt;Onboard new members&lt;/a&gt;, answer queries, and organize community events.&lt;/li&gt;&lt;li&gt;Curate and share community-generated content, news, and project updates. Help draft and send newsletters, announcements, and other community updates.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://badges.layer5.io&quot;&gt;Recognize Contributors&lt;/a&gt;:&lt;/strong&gt; Assign badges, recognize achievements, and celebrate milestones.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Moderate social media:&lt;/strong&gt; Monitor and respond to comments and messages on Layer5&amp;#x27;s or it&amp;#x27;s project&amp;#x27;s social media channels &lt;a href=&quot;https://x.com/layer5&quot;&gt;@layer5&lt;/a&gt;, &lt;a href=&quot;https://x.com/mesheryio&quot;&gt;@meshery&lt;/a&gt;, &lt;a href=&quot;https://x.com/smp_spec&quot;&gt;@smp_spec&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/company/layer5&quot;&gt;Layer5&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/showcase/meshery&quot;&gt;Meshery&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/showcase/service-mesh-performance&quot;&gt;Performance&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/channel/UCFL1af7_wdnhHXL1InzaMvA?sub_confirmation=1&quot;&gt;YouTube&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Curate content:&lt;/strong&gt; Share relevant &lt;a href=&quot;/company/news&quot;&gt;news&lt;/a&gt;, &lt;a href=&quot;/resources&quot;&gt;articles&lt;/a&gt;, and &lt;a href=&quot;/projects&quot;&gt;project&lt;/a&gt; updates on social media.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a &lt;a href=&quot;https://discuss.layer5.io&quot;&gt;Discussion Forum&lt;/a&gt; Moderator:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Ensure that discussions remain constructive, respectful, and on-topic.&lt;/li&gt;&lt;li&gt;Moderate comments and posts, addressing any inappropriate or offensive content.&lt;/li&gt;&lt;li&gt;Encourage participation and create a welcoming environment for all users.&lt;/li&gt;&lt;li&gt;Guide discussions and help resolve conflicts if they arise.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;/community/calendar&quot;&gt;Meeting Host&lt;/a&gt; or &lt;a href=&quot;/community/events&quot;&gt;Event Organizer&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Plan, schedule, and facilitate community meetings, workshops, or webinars.&lt;/li&gt;&lt;li&gt;Prepare agendas, moderate discussions, and ensure smooth execution.&lt;/li&gt;&lt;li&gt;Record and share meeting notes or summaries for those unable to attend.&lt;/li&gt;&lt;li&gt;Help organize and coordinate Layer5 events, whether online or in-person.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a &lt;a href=&quot;https://layer5.io/community/meshmates&quot;&gt;MeshMate&lt;/a&gt; (a mentor):&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Mentor new contributors:&lt;/strong&gt;  Guide newcomers through the contribution process, answer their questions, and help them get started.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Review code contributions:&lt;/strong&gt;  Provide constructive feedback on pull requests, ensuring code quality and adherence to project guidelines.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Triage issues:&lt;/strong&gt;  Help identify and prioritize issues in the project&amp;#x27;s issue tracker.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Technical Contributions&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;As a Maintainer:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Take ownership of specific projects or components.&lt;/li&gt;&lt;li&gt;Review and merge code contributions, ensuring quality and adherence to standards.&lt;/li&gt;&lt;li&gt;Triage and address issues promptly.&lt;/li&gt;&lt;li&gt;Guide and mentor other contributors.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a &lt;a href=&quot;/community/handbook/repository-overview&quot;&gt;Code Contributor&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Fix bugs:&lt;/strong&gt; Address reported issues and bugs in the codebase.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Implement new features:&lt;/strong&gt;  Contribute code to add new functionalities or enhancements.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Triage Issues&lt;/strong&gt;: Ensure timely validation of new issues, ongoing assignment, and removal of incidental issue squatters.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Offer Peer Review&lt;/strong&gt;: Review and comment on pull requests. Collaborate with other contributors to improve the project.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a Performance or &lt;a href=&quot;https://docs.meshery.io/project/contributing/contributing-ui-tests&quot;&gt;Test Engineer&lt;/a&gt;:&lt;/strong&gt;
&lt;strong&gt;Optimize performance:&lt;/strong&gt; Identify and implement optimizations to improve the performance of the projects, like &lt;a href=&quot;https://github.com/layer5io/layer5/issues?q=is%3Aopen+is%3Aissue+label%3Atype%2Fperformance&quot;&gt;layer5.io performance issues&lt;/a&gt;.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;**End-to-End Testing: Write, review, and test code contributions, following project guidelines and best practices.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a &lt;a href=&quot;https://docs.meshery.io/project/contributing/build-and-release&quot;&gt;Build and Release Lead&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Manage the build and release processes, including versioning, packaging, and deployment.&lt;/li&gt;&lt;li&gt;Ensure smooth and timely releases, addressing any build or dependency issues.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a &lt;a href=&quot;https://github.com/issues?q=is%3Aopen+is%3Aissue+archived%3Afalse+org%3Alayer5io+org%3Ameshery+org%3Aservice-mesh-performance+org%3Aservice-mesh-patterns+org%3Alayer5labs&quot;&gt;Bug Hunter&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Actively identify and report bugs or issues in the software.&lt;/li&gt;&lt;li&gt;Provide detailed information to help developers reproduce and fix the problems.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a Technical Writer:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Create clear and comprehensive documentation, tutorials, and guides.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://docs.layer5.io&quot;&gt;Improve documentation&lt;/a&gt;:&lt;/strong&gt; Update and enhance project documentation to make it easier for others to understand and use.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Translate&lt;/strong&gt; project documentation or user interfaces into different languages, broadening the project&amp;#x27;s reach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Project Engagement and Advocacy&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;As a User Ambassador:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://badges.layer5.io&quot;&gt;Showcase your achievements&lt;/a&gt;: As a user, this is how you demonstrate your prowess, milestones, and meaningful engagement.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Share your enthusiasm:&lt;/strong&gt; Share your Layer5 project experiences on social media platforms like Twitter, LinkedIn, Reddit, or relevant forums. Highlight your favorite features, use cases, or success stories.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Create content:&lt;/strong&gt; Write blog posts, record videos, or create tutorials about Layer5 projects.  Share them on your personal channels and within the Layer5 community.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/community/events&quot;&gt;Attend and present&lt;/a&gt;&lt;/strong&gt; Participate in conferences, meetups, or webinars related to cloud-native technologies. Represent Layer5 and share your knowledge.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Engage in online communities:&lt;/strong&gt;  Answer questions on Stack Overflow, participate in discussions on GitHub, or contribute to relevant online forums.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a &lt;a href=&quot;/community/handbook/writing-program&quot;&gt;Writer&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/community/adventures-of-five-and-friends&quot;&gt;Craft Five&amp;#x27;s Adventures&lt;/a&gt;:&lt;/strong&gt; Contribute to the ongoing story of Five, the Layer5 mascot, by writing engaging short stories.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://docs.meshery.io/guides/tutorials&quot;&gt;Develop tutorials&lt;/a&gt; and &lt;a href=&quot;/learn/learning-paths&quot;&gt;Learning Paths&lt;/a&gt;:&lt;/strong&gt; Create clear and concise written tutorials or guides to help users understand and use Layer5 projects.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/blog&quot;&gt;Write blog posts&lt;/a&gt;:&lt;/strong&gt;  Share your insights, experiences, and knowledge about Layer5 projects through informative blog posts. Have an idea? Pitch it by opening &lt;a href=&quot;https://github.com/layer5io/layer5/issues/new?assignees=&amp;amp;labels=area%2Fwriting%2C+help+wanted%2C+language%2Fenglish&amp;amp;projects=&amp;amp;template=writing.md&amp;amp;title=%5BWriting%5D&quot;&gt;a writing issue&lt;/a&gt;. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As a Designer:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/company/brand&quot;&gt;Design brand materials&lt;/a&gt;:&lt;/strong&gt;  Contribute to the design of project logos, website layouts, social media graphics, or presentation slides.&lt;/li&gt;&lt;li&gt;Create wireframes and mockups for any of our &lt;a href=&quot;/community/handbook/repository-overview&quot;&gt;frontend projects&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Improve user interfaces:&lt;/strong&gt; Leave your UX mark and help refine the user interfaces of Layer5 projects to make them more intuitive and user-friendly &lt;a href=&quot;https://www.figma.com/team_invite/redeem/qJy1c95qirjgWQODApilR9&quot;&gt;(open Figma invite)&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/community/adventures-of-five-and-friends&quot;&gt;Illustrate Five&amp;#x27;s World&lt;/a&gt;:&lt;/strong&gt; Create captivating visuals for the Five&amp;#x27;s Adventures stories.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;As an &lt;a href=&quot;/careers/programs&quot;&gt;Intern&lt;/a&gt; or &lt;a href=&quot;/careers&quot;&gt;Employee&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Meshery is the #1 most popular Linux Foundation internship, offering invaluable hands-on experience in the cloud native space.&lt;/li&gt;&lt;li&gt;Layer5 is continuously offering internships, providing opportunities to learn and grow alongside a passionate community.&lt;/li&gt;&lt;/ul&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/number-one-most-popular-project-df51a301be87272a1ad6598f7eb95c2c.png&quot; alt=&quot;Meshery is the most popular LFX project&quot; class=&quot;image-center-shadow&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot;/&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Remember:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;All contributions are valuable and every skill set has a place in open source.  Don&amp;#x27;t hesitate to ask questions or reach out for help.  Most importantly, have fun and enjoy being part of the community!&lt;/p&gt;&lt;p&gt;&lt;em&gt;Every contribution, big or small, is valuable and helps strengthen the Layer5 community and its projects.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Meet the Maintainer: Xin Huang]]></title><description><![CDATA[Meet the Maintainer series with open source maintainer, Xin Huang]]></description><link>https://layer5.io/blog/open-source/meet-the-maintainer-xin-huang</link><guid isPermaLink="false">https://layer5.io/blog/open-source/meet-the-maintainer-xin-huang</guid><dc:creator><![CDATA[Layer5 Team]]></dc:creator><pubDate>Mon, 01 Jul 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/f061d9779b8971811daa9135d76ca7be/xin-huang-layer5-maintainer.png" length="0" type="image/png"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;div class=&quot;MeetTheMaintainerstyle__MeetTheMaintainer-sc-cimr9h-0 cVWUJX&quot;&gt;&lt;div class=&quot;intro&quot;&gt;&lt;p&gt;Continuing in our Meet the Maintainer series, we have &lt;a href=&quot;/community/members/xin-huang&quot;&gt;Xin Huang&lt;/a&gt;. Xin is a maintainer of the &lt;a href=&quot;/projects/service-mesh-performance&quot;&gt;Cloud Native Performance&lt;/a&gt; project. In this interview, we get to know Xin a little better and learn about his journey as an open source project maintainer and with Layer5 community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;Xin, thank you for joining me today. Many people inside and outside of the Layer5 Community have seen the effects of your contributions, but may not know the backstory as to who Xin  is and how you arrived at your maintainer role. Indulge us. How did you discover the Layer5 community? What made you stay?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;I’m working around Service Mesh Acceleration in Intel, and noticed the Service Mesh Performance project. So I attended the community meeting, the community is very active and welcoming and they hope for feedback and contribution from me. It’s the main reason that made me stay here and contribute continuously.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;Have you worked with any other open source project? How does &lt;a href=&quot;/community&quot;&gt;Layer5 community&lt;/a&gt; compare?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;Yes, I’ve worked with some open source projects. Layer5 community knows how to cultivate and  fulfill the sprint of open source. If you are a newcomer to open source, you can get a big leg up from the Layer5 community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;What is so fascinating about service meshes? Why are you focused on this technology specifically?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;First, I’m working on Service Mesh Acceleration at Intel. Second, service mesh is a hot area this year, more and more people or enterprises are trying service mesh in their production.&lt;/p&gt;&lt;p&gt;Service Mesh is the bridge between developers and infrastructure, how fascinating it is to build such a thing that benefits everyone.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;&lt;a href=&quot;/projects&quot;&gt;Layer5 projects&lt;/a&gt; has a number of active, open source projects. You’ve been consistently contributing to a few of them. Which one(s) are you currently focusing on?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;I’m focusing on the SMP project, because it’s related to my working scope. I also use Meshery to manage our service mesh environment and show our effort on Service Mesh acceleration. Every project has its specific goal, I can’t tell which one is the best.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;What is your favorite Meshery CLI command?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;Having worked on and been a user of Meshery&amp;#x27;s performance characterization features, I&amp;#x27;m biased. I like the `mesheryctl perf` command the most.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;Where do you see Cloud Native Performance, Nighthawk, and Meshery heading?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;Meshery has a great dream to manage all cloud and cloud native technologies. It&amp;#x27;s a great idea for infrastructure and platform engineers who are confused with so many different projects to try and understand their differences.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;What is your hot tip for working with Service Mesh Performance, Nighthawk, or Meshery that others may not know?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;We have a distributed performance characterization feature upcoming in Meshery in which &lt;a href=&quot;/project/nighthawk&quot;&gt;Nighthawk&lt;/a&gt; will be externalized. Meshery manages the lifecycle of Nighthawk, using Nighthawk as one of its load generators to run performance benchmarks. As Nighthawk supports an adaptive load control as a feature, Meshery can leverage this to build a custom load controller to run performance benchmarks.&lt;/p&gt;&lt;p&gt;By default, the adaptive load controller in Nighthawk runs benchmarks with different RPS values and based on the latency, it adjusts the RPS value. But, with its custom plugin ability, we can bring in our own inputs as well as metrics to measure.&lt;/p&gt;&lt;p&gt;With the metrics from these tests, Meshery should also adjust the resiliency characteristics of the mesh automatically so as to improve these metrics and in turn improve performance. Meshery allows users to generate traffic load tests using Nighthawk. The tests are controlled and provisioned using meshery-nighthawk, a Meshery component supporting multiple load generators. This is a very powerful feature that is coming up in Meshery.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt; Do you have any advice for individuals hopeful to become Layer5 contributors or potentially maintainers?&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;Yes! I think that being a maintainer requires a combination of technical expertise, community engagement, and leadership skills. Start by being a consistent contributor yourself. Demonstrate your commitment by making regular and valuable contributions to the projects. You&amp;#x27;ll need to build a strong understanding of the technologies and the codebase. In order to demonstrate your technical leadership, you&amp;#x27;ll need to show initiative, take ownership of tasks, and often do the menial work that other contributors may not want to do.&lt;/p&gt;&lt;p&gt;One aspect of being a maintainer is actively participating in the Layer5 community, helping newcomers, and engaging in helping advance project discussions. Guiding and supporting other contributors is an expecation of a maintainer, so it&amp;#x27;s important to have a good understanding of the project and the technologies involved.&lt;/p&gt;&lt;p&gt;Finally, I wholeheartedly suggest that you are not afraid to ask questions. The Layer5 community is welcoming and supportive. The existing maintainers are individuals who have given their time and demonstrated their dedication to the betterment of the projects. They are a valuable resource for you to learn from, too, so when you engage with them, be respectful of their time - be sure to have done your homework before asking questions.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewer&quot;&gt;&lt;span&gt;Layer5 Team:&lt;/span&gt;&lt;p&gt;In other words, whether your contribution is big or small, it sounds like aiming for high-quality contributions that add value to the projects is key.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;interviewee&quot;&gt;&lt;span&gt;Xin:&lt;/span&gt;&lt;p&gt;Yes, that&amp;#x27;s right. It&amp;#x27;s important to remember that the Layer5 community is a diverse group of individuals with different backgrounds, experiences, and perspectives. Remember to have fun and enjoy the process of learning, contributing, and collaborating with the community.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;note&quot;&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/forklift.81115a37.svg&quot; height=&quot;100px&quot; width=&quot;100%&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot; alt=&quot;Blog content image&quot;/&gt;&lt;/div&gt;&lt;p&gt;The Meshery project moves at an impressive pace thanks to maintainers like Xin. Be like Xin. Join the &lt;a href=&quot;https://slack.layer5.io&quot;&gt;Layer5 Slack&lt;/a&gt; and say “hi&amp;quot;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Meshery's 5,000 Star Milestone]]></title><description><![CDATA[Celebrating Meshery's 5,000 GitHub Star Milestone]]></description><link>https://layer5.io/blog/announcements/mesherys-5000-star-milestone</link><guid isPermaLink="false">https://layer5.io/blog/announcements/mesherys-5000-star-milestone</guid><dc:creator><![CDATA[Sandra Ashipala]]></dc:creator><pubDate>Thu, 20 Jun 2024 00:00:00 GMT</pubDate><enclosure url="https://layer5.io/static/edf0277ea6903daba9480dabf3be6c9f/banner.jpeg" length="0" type="image/jpeg"/><content:encoded>&lt;div class=&quot;Blogstyle__BlogWrapper-sc-di69nl-0 fUgVTq&quot;&gt;&lt;h2&gt;A Journey of Growth and Community&lt;/h2&gt;&lt;p&gt;Achieving key milestones in open source reflects the combined efforts of passionate individuals and forward-thinking development. Today, we are thrilled to announce that Meshery, a leading project under the Layer5 umbrella, has reached a significant milestone: &lt;strong&gt;&lt;a href=&quot;https://github.com/layer5io/meshery&quot;&gt; 5,000 stars 🌟 on GitHub! &lt;/a&gt;&lt;/strong&gt; This achievement is more than just a number; it represents the collective efforts of our vibrant community, the dedication of our contributors, and the transformative journey that Meshery has embarked upon.&lt;/p&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/star-meshery-10118a491ba0e6e3a8210d541270e122.png&quot; alt=&quot;Five Thousand Stars on GitHub&quot; style=&quot;object-fit:contain;margin:20px 0px;width:60%&quot; width=&quot;100%&quot; height=&quot;auto&quot; loading=&quot;lazy&quot;/&gt;&lt;/div&gt;&lt;p&gt;Let&amp;#x27;s take a moment to reflect on this journey, celebrate the people behind it, and look forward to the exciting future ahead.&lt;/p&gt;&lt;h2&gt;The Genesis of Meshery&lt;/h2&gt;&lt;p&gt;Meshery was born out of a simple yet ambitious idea: to create an open source, cloud native management platform that simplifies and enhances the experience of managing Kubernetes environments. The vision was to build a tool that integrates seamlessly with all CNCF projects, providing users with unparalleled control over their multi-cluster Kubernetes deployments.&lt;/p&gt;&lt;p&gt;From its inception, Meshery was designed with the user in mind. It aimed to provide a robust and intuitive interface for managing service meshes, enabling users to optimize their monitoring, CI/CD, and security solutions effortlessly. With a focus on user experience and community-driven development, Meshery quickly gained traction within the open-source community.&lt;/p&gt;&lt;h2&gt;Who is Meshery For?&lt;/h2&gt;&lt;p&gt;Meshery is for everyone passionate about cloud-native technologies. Whether you&amp;#x27;re a DevOps engineer looking to streamline your Kubernetes management, a developer seeking to optimize your CI/CD pipelines, or a cloud enthusiast eager to explore the intricacies of service meshes, Meshery offers something valuable for you.&lt;/p&gt;&lt;p&gt;Our diverse community includes individuals from all walks of life—seasoned professionals, enthusiastic beginners, and everyone in between. This diversity is our strength, fostering a rich environment where ideas flourish, and innovation thrives.&lt;/p&gt;&lt;h2&gt;Celebrating the Community&lt;/h2&gt;&lt;p&gt;Reaching 5,000 stars on GitHub is a testament to the incredible contributions from our community. Each star, each pull request, and each issue raised has played a part in shaping Meshery into the powerful tool it is today.&lt;/p&gt;&lt;h4&gt;A Heartfelt Thank You&lt;/h4&gt;&lt;p&gt;To our contributors, thank you for your unwavering dedication. Your code contributions, documentation improvements, bug reports, and feature requests have been instrumental in Meshery&amp;#x27;s growth.&lt;/p&gt;&lt;div class=&quot;blockquotestyle__BlockquoteStyle-sc-9kzfnh-0 cRwgvu blockquote&quot; style=&quot;css-float:left;width:60%&quot; quote=&quot;It was an intuitive experience to visually place and configure various components saving the time of going through 10 different YAML files&quot; person=&quot;Deepak Dinesh, Meshery User&quot;&gt;&lt;div class=&quot;blockquote-wrapper&quot;&gt;&lt;div class=&quot;blockquote&quot;&gt;&lt;h4&gt;It was an intuitive experience to visually place and configure various components saving the time of going through 10 different YAML files&lt;/h4&gt;&lt;h5 class=&quot;person&quot;&gt;Deepak Dinesh, Meshery User&lt;/h5&gt;&lt;h5 class=&quot;title&quot;&gt;&lt;/h5&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;We extend our deepest gratitude to each of you.&lt;/p&gt;&lt;h4&gt;Acknowledging the CNCF&lt;/h4&gt;&lt;p&gt;We are also proud to share that Meshery is the 10th fastest-growing project within the Cloud Native Computing Foundation (CNCF) by contributor count, among over 200 projects. This recognition is a significant milestone and a validation of our vision and efforts. Meshery has been proposed for incubation within CNCF, marking another exciting chapter in our journey.&lt;/p&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/meshery-stats-b07da176492c8dbde68a93bc2133f19a.png&quot; alt=&quot;Tenth fastest growing project&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot;/&gt;&lt;/div&gt;&lt;h2&gt;Meshery: Leading in Open Source&lt;/h2&gt;&lt;p&gt;Meshery has garnered significant attention within the open-source community and stands out as a cornerstone program under the Linux Foundation. Its popularity stems from its ability to simplify and enhance cloud-native management, integrating seamlessly with CNCF projects for optimized Kubernetes environment management.&lt;/p&gt;&lt;h4&gt;Why Meshery Stands Out&lt;/h4&gt;&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;&lt;strong&gt;Integration and Compatibility:&lt;/strong&gt; Meshery&amp;#x27;s seamless integration with CNCF projects ensures comprehensive management capabilities across multi-cluster Kubernetes deployments, enhancing operational efficiency and scalability.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Community-Driven Innovation:&lt;/strong&gt; Embracing community-driven development, Meshery empowers a global network of contributors to collaborate on enhancing functionality, reliability, and user experience, fostering continuous innovation.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Educational Leadership:&lt;/strong&gt; Meshery&amp;#x27;s educational initiatives, including workshops and webinars, cultivate industry best practices and empower users to maximize their cloud-native potential, contributing to a skilled workforce.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Prestigious Position:&lt;/strong&gt; As the most popular Linux Foundation program, Meshery&amp;#x27;s leadership within CNCF and its widespread adoption underscore its pivotal role in advancing cloud-native technologies and standards.&lt;/li&gt;&lt;/ol&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/lfx-mentorship-8ca17d8626fd0218ff03b6c547699ced.png&quot; alt=&quot;LFX Mentorship&quot; class=&quot;image-right&quot; width=&quot;100%&quot; height=&quot;auto&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot;/&gt;&lt;/div&gt;&lt;h2&gt;The Road Ahead: Inviting New Contributors&lt;/h2&gt;&lt;p&gt;As we celebrate this milestone, we also recognize that this is just the beginning. The journey ahead is filled with opportunities for growth, learning, and innovation. We invite you—yes, you!—to join us in this exciting adventure.&lt;/p&gt;&lt;h4&gt;How to Get Started&lt;/h4&gt;&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;&lt;strong&gt; Explore the Website:&lt;/strong&gt; Visit the &lt;a href=&quot;https://meshery.io&quot;&gt;Meshery website&lt;/a&gt;  to understand its features and capabilities. The comprehensive documentation, tutorials, and resources are great starting points.&lt;/li&gt;&lt;li&gt;&lt;strong&gt; Join the Community:&lt;/strong&gt; Engage with fellow Meshery enthusiasts on our community forums, mailing lists, and social media channels. Join the conversations in the #meshery Slack channel, ask questions, and share your experiences.&lt;/li&gt;&lt;li&gt;&lt;strong&gt; Contribute to the Repository:&lt;/strong&gt; There are numerous opportunities to contribute, regardless of your experience level. Start by familiarizing yourself with the &lt;a href=&quot;https://github.com/layer5io/meshery/blob/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; guidelines. Look for issues labeled &amp;quot;good-first-issue&amp;quot; or &amp;quot;beginner-friendly,&amp;quot; and start small.&lt;/li&gt;&lt;li&gt;&lt;strong&gt; Attend Events:&lt;/strong&gt; Participate in our online development meetings, and don&amp;#x27;t miss the Layer5 Newcomers&amp;#x27; meet. Sync the &lt;a href=&quot;https://layer5.io/community/calendar&quot;&gt;community calendar&lt;/a&gt; to get reminders for these events.&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;Reflections on the Journey&lt;/h4&gt;&lt;p&gt;For many, contributing to Meshery has been more than a technical endeavor; it has been a personal and professional growth journey.&lt;/p&gt; &lt;div class=&quot;blockquotestyle__BlockquoteStyle-sc-9kzfnh-0 cRwgvu blockquote&quot; style=&quot;width:60%&quot; quote=&quot;Meshery has been a fantastic platform for honing my technical skills and learning from an incredibly supportive community.&quot; person=&quot;Alex Johnson, Contributor&quot;&gt;&lt;div class=&quot;blockquote-wrapper&quot;&gt;&lt;div class=&quot;blockquote&quot;&gt;&lt;h4&gt;Meshery has been a fantastic platform for honing my technical skills and learning from an incredibly supportive community.&lt;/h4&gt;&lt;h5 class=&quot;person&quot;&gt;Alex Johnson, Contributor&lt;/h5&gt;&lt;h5 class=&quot;title&quot;&gt;&lt;/h5&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Each contribution, big or small, has been a stepping stone, enhancing technical prowess and fostering essential soft skills like communication and collaboration.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;The Journey Has Just Begun&lt;/h2&gt;&lt;p&gt;As we look back on our achievements, we are filled with gratitude and excitement for the future.&lt;/p&gt;&lt;div style=&quot;width:100%;height:auto&quot;&gt;&lt;img src=&quot;/static/star-history-7f36f275a4c463ee341a3a867b2db8ac.png&quot; alt=&quot;Star history&quot; width=&quot;70%&quot; height=&quot;25%&quot; style=&quot;object-fit:contain;margin:20px 0px&quot; loading=&quot;lazy&quot;/&gt;&lt;/div&gt;&lt;p&gt;With new features, enhancements, and community engagements on the horizon, there are endless opportunities to make a meaningful impact. We invite you to be a part of this journey. Join us in shaping the future of cloud-native management with Meshery. Together, we can achieve great things. &lt;/p&gt;&lt;p&gt;Thank you for your support, and happy contributing!&lt;/p&gt;&lt;i&gt;Useful links:&lt;/i&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;/community/newcomers&quot;&gt;Newcomer Resources&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://slack.layer5.io/&quot;&gt;Slack Community&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</content:encoded></item></channel></rss>