{"componentChunkName":"component---src-templates-blog-tag-list-js","path":"/blog/tag/java.html","result":{"data":{"allMdx":{"nodes":[{"id":"cd1299a5-8e67-5835-9211-e24b698da225","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nIn our [ongoing exploration](/blog/category/docker) of Docker Model Runner, we've covered its OCI-based model management, performance architecture, OpenAI-compatible API, and Docker Compose integration. Now, we turn to a specific, yet highly impactful, synergy: how Docker Model Runner empowers **Java developers using the Spring AI framework** to seamlessly incorporate local Large Language Models (LLMs) into their applications.  \nFor Java engineers vested in the Spring ecosystem, Spring AI offers a familiar and powerful abstraction layer for interacting with various AI models. Docker Model Runner's compatibility provides a straightforward path to leverage these local models without stepping outside the conventional Spring development paradigm.\n\n## **Spring AI: Simplifying AI for Java Applications**\n\nBefore diving into the integration, it's worth briefly understanding Spring AI's mission. Spring AI aims to apply core Spring principles—such as autoconfiguration, dependency injection, and portable service abstractions—to the domain of artificial intelligence. It provides Java developers with:\n\n* **Consistent APIs:** A unified API for interacting with different AI models (both local and remote), reducing the need to learn multiple vendor-specific SDKs.  \n* **Abstraction Layers:** Components like ChatClient, EmbeddingClient, and ImageClient abstract away the underlying model provider.  \n* **Integration with Spring Boot:** Easy setup and configuration within Spring Boot applications.\n\n## **Docker Model Runner as a Local \"Ollama\" for Spring AI**\n\nSpring AI supports various AI model providers, including commercial cloud services (like OpenAI, Azure OpenAI) and self-hosted solutions (like Ollama). From Spring AI's perspective, Docker Model Runner, with its OpenAI-compatible API, effectively acts like a local, easily manageable Ollama-style endpoint.  \nWhen Docker Model Runner is active and serving a model (e.g., Llama 3, Gemma) with its API endpoint accessible (typically http://localhost:12434 or http://model-runner.docker.internal if accessed from another container), Spring AI can be configured to point to it.  \nHere's how a Java engineer benefits:\n\n1. **Simplified Configuration in Spring Boot**\n  \n   Spring AI's autoconfiguration can often detect and set up the necessary beans to interact with an OpenAI-compatible endpoint. For Docker Model Runner, this typically involves setting a few properties in your application.properties or application.yml file:  \n   \n   ```java\n   \\# For Spring AI 0.8.x (or similar versions)  \n   spring.ai.openai.chat.base-url=http://localhost:12434/engines/v1 \n   \\# Or your specific DMR endpoint  \n   spring.ai.openai.chat.options.model=ai/llama3.2:1B-Q8\\_0 \n   \\# The model you want to use  \n   use  \n   spring.ai.openai.api-key=YOUR\\_DUMMY\\_API\\_KEY\\_OR\\_EMPTY\n   \\# Potentially disable API key if DMR doesn't require it strictly for local \n   ```\n\n   _(Note: The exact property names and structure might vary slightly based on the Spring AI version and whether you're configuring a generic OpenAI client or a more specific Ollama-like client type if Spring AI introduces more direct DMR support.)_  \n\n2. **Leveraging Spring AI's ChatClient and EmbeddingClient**\n\n   Once configured, developers can inject and use Spring AI's standard clients without needing to know that the underlying provider is Docker Model Runner. \n\n```java \n   import org.springframework.ai.chat.ChatClient;  \n   import org.springframework.ai.chat.prompt.Prompt;  \n   import org.springframework.beans.factory.annotation.Autowired;  \n   import org.springframework.stereotype.Service;\n\n   @Service  \n   public class MyAiService {\n\n       private final ChatClient chatClient;\n\n       @Autowired  \n       public MyAiService(ChatClient chatClient) {  \n           this.chatClient \\= chatClient;  \n       }\n\n       public String getJokeAbout(String topic) {  \n           Prompt prompt \\= new Prompt(\"Tell me a short joke about \" \\+ topic);  \n           return chatClient.call(prompt).getResult().getOutput().getContent();  \n       }  \n   }\n```\n\n   This code remains the same whether Spring AI is talking to OpenAI's cloud API, a self-hosted Ollama instance, or Docker Model Runner serving a local model. This portability is a huge win.  \n\n3. **Seamless Local Development and Testing**\n   Engineers can develop and test AI-driven features entirely locally using their preferred Java tools and the Spring framework. Docker Model Runner handles the model serving, and Spring AI provides the clean Java interface. This speeds up iteration cycles and reduces reliance on potentially costly cloud APIs during development.  \n\n4. **Consistency with Production (Potentially)**  \n   While Docker Model Runner is primarily for local development, the abstraction provided by Spring AI means that switching to a production-grade, potentially cloud-hosted model provider for deployment can be achieved mainly through configuration changes, without altering the core application logic.\n\n## **The Bigger Picture: Local AI in Enterprise Java**\n\nThe integration with Spring AI is significant because it brings the ease of local LLM experimentation directly into the robust, enterprise-focused Java and Spring ecosystem. It allows Java teams to:\n\n* **Prototype AI features rapidly.**  \n* **Upskill on AI concepts using familiar tools.**  \n* **Conduct local, private testing of AI interactions with business data.**  \n* **Integrate AI into existing Spring Boot applications with minimal friction.**\n\nDocker's collaboration with Spring AI (as noted in some announcements) underscores a shared vision of making AI more accessible and developer-friendly across different programming environments. By ensuring Docker Model Runner presents an API that Spring AI can readily consume, both platforms contribute to lowering the barrier to entry for sophisticated AI development.  \nFor Java engineers, this means Docker Model Runner isn't just another tool; it's a key enabler for leveraging the power of local LLMs within the comfort and productivity of the Spring framework.\n\n## **Next, we'll delve into some practical, task-specific configurations and advanced use cases you can explore with Docker Model Runner, moving beyond basic chat completions.**\n\n*This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change. Configuration details for Spring AI may vary based on specific versions.*\n\n</BlogWrapper>","frontmatter":{"title":"Spring AI: Streamlining Local LLM Integration for Java Developers","subtitle":"Docker Model Runner - A Technical Primer for Engineers","date":"May 14th, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/7a5be1548e7efed21a2ef4929e78a5ab/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACbklEQVR42iXC2U/aYAAA8D5uQ6FA6devx9eLQqHcYqWc5daqUAVUBDGbYzpx80DmtWQvZolLfFjiFh+W/a0z2S8/zEkiAgVfuqAwx0hzJHrthg5WBksFkK/SK23SMN2pjIMWX+HQAXgHiRw+7n/MBQSCV4ES84qqS1TfUAIym8bBUWR1g0ploGGCxTwo1kI7IzZXfg34Oc4/x8gORnIwMoYD3gkFlxRyy+GXZN0qXcy0xgphWfTlBZnUiWTaV8jzZjWwNfQUa05RnVc0TzDmlkIYEU155bCLlnA+KKx3woO3QDdctYpv02bqy0yxgterhFkGqQyIL6G9d55syR2IwrjuS2exeSjgWoIIxXGSE3pDulJjbBt0NzyjXdTpekrm4P2gtNbOjSfeRgX1trlOH2hJIqHjkQUMp3iCU4Bfg5mC/8stvbyKDg/FRiPbtsL394q91fk8rt7crE1mnlwettrq9JrUkl5OIQMxDMoRSgqhzS3l9Fz78TO72k70u0S7RQ93tMnJ5O6Mu5o2H393p3ep/k6s2488/xEHIyqpA9aPQSFMGUX/7JrNmMGzq8T4ZLE3QkbZndLTdfvb17vO7ezg4VfJ3jVarYXj0+D0BpWb0um5R1Ix1h8D4YR4fCL0R0AIIctW9sdiuye1evbjs/3wZH9/7D/9Vfc/8MMDtLENtZRYqIv9PUKJYGJEByjoMwq8ZUuJLBtJw/oytNappSKTr/KVJqqtUJUGiOsQymzDkqYzIWuSaoyRo5gPBdy06PTQPkGF6RxbaQpHH4XJJz6U5sQoG88I00ux1eGrFp8x2WIVFMpupDh9rJdT/gG8qIzT1/dCagAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7a5be1548e7efed21a2ef4929e78a5ab/366fe/hero-image.jpg","srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/9b503/hero-image.jpg 750w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/321ef/hero-image.jpg 1080w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/84dcd/hero-image.jpg 1366w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/5f850/hero-image.webp 750w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/2c010/hero-image.webp 1080w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/5126b/hero-image.webp 1366w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/7a5be1548e7efed21a2ef4929e78a5ab/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACbklEQVR42iXC2U/aYAAA8D5uQ6FA6devx9eLQqHcYqWc5daqUAVUBDGbYzpx80DmtWQvZolLfFjiFh+W/a0z2S8/zEkiAgVfuqAwx0hzJHrthg5WBksFkK/SK23SMN2pjIMWX+HQAXgHiRw+7n/MBQSCV4ES84qqS1TfUAIym8bBUWR1g0ploGGCxTwo1kI7IzZXfg34Oc4/x8gORnIwMoYD3gkFlxRyy+GXZN0qXcy0xgphWfTlBZnUiWTaV8jzZjWwNfQUa05RnVc0TzDmlkIYEU155bCLlnA+KKx3woO3QDdctYpv02bqy0yxgterhFkGqQyIL6G9d55syR2IwrjuS2exeSjgWoIIxXGSE3pDulJjbBt0NzyjXdTpekrm4P2gtNbOjSfeRgX1trlOH2hJIqHjkQUMp3iCU4Bfg5mC/8stvbyKDg/FRiPbtsL394q91fk8rt7crE1mnlwettrq9JrUkl5OIQMxDMoRSgqhzS3l9Fz78TO72k70u0S7RQ93tMnJ5O6Mu5o2H393p3ep/k6s2488/xEHIyqpA9aPQSFMGUX/7JrNmMGzq8T4ZLE3QkbZndLTdfvb17vO7ezg4VfJ3jVarYXj0+D0BpWb0um5R1Ix1h8D4YR4fCL0R0AIIctW9sdiuye1evbjs/3wZH9/7D/9Vfc/8MMDtLENtZRYqIv9PUKJYGJEByjoMwq8ZUuJLBtJw/oytNappSKTr/KVJqqtUJUGiOsQymzDkqYzIWuSaoyRo5gPBdy06PTQPkGF6RxbaQpHH4XJJz6U5sQoG88I00ux1eGrFp8x2WIVFMpupDh9rJdT/gG8qIzT1/dCagAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7a5be1548e7efed21a2ef4929e78a5ab/22857/hero-image.jpg","srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/9976b/hero-image.jpg 125w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/e19ca/hero-image.jpg 250w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/22857/hero-image.jpg 500w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/842b0/hero-image.webp 125w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/b597c/hero-image.webp 250w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/49c7e/hero-image.webp 500w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/spring-ai-streamlining-local-llm-integration-for-java-developers"}}]}},"pageContext":{"tag":"java"}},"staticQueryHashes":["1485533831","4047814605","408154852","4152005505"],"slicesMap":{},"matchPath":"/blog/tag/java"}