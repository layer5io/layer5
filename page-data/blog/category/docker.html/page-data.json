{"componentChunkName":"component---src-templates-blog-category-list-js","path":"/blog/category/docker.html","result":{"data":{"allMdx":{"nodes":[{"id":"6cf0b49c-2a8a-5605-b99d-e02e227b0d29","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nOver the course of [this series](/blog/category/docker), we've embarked on a deep technical dive into Docker Model Runner, moving beyond surface-level descriptions to uncover the engineering principles and practical implications of this innovative toolkit. From its foundational architecture to its integration with the broader developer ecosystem, Model Runner presents a compelling vision for the future of local AI development. In this concluding post, we'll synthesize the key engineering takeaways and explore the promising horizons as Docker Model Runner matures.\n\n## **Key Engineering Takeaways: A Recap**\n\nOur journey has illuminated several critical aspects that define Docker Model Runner's value proposition for engineers:\n\n1. **OCI for Robust Model Management:** Model Runner's strategic adoption of the Open Container Initiative (OCI) standard for packaging and distributing AI models is transformative. It brings DevOps-like rigor to model lifecycle management, enabling versioning, provenance, and the use of existing container registries and CI/CD pipelines for AI models.  \n2. **Performance via Host-Native Execution:** The decision to run inference engines (like llama.cpp) as host-native processes, with direct GPU access (especially Metal API on Apple Silicon), prioritizes local performance. This minimizes latency and provides a responsive experience crucial for iterative development.  \n3. **OpenAI-Compatible API for Seamless Integration:** By offering an API compatible with OpenAI's standards, Model Runner drastically lowers the barrier to entry. Engineers can leverage existing SDKs, tools like LangChain and LlamaIndex, and familiar coding patterns with minimal friction.  \n4. **Docker Compose for Orchestrated AI Stacks:** The introduction of the provider service type in Docker Compose allows AI models to be declared and managed as integral components of multi-service applications, simplifying the orchestration of complex local AI development environments.  \n5. **Ecosystem Synergy (e.g., Spring AI):** Integrations with frameworks like Spring AI demonstrate Model Runner's ability to seamlessly fit into established development ecosystems, enabling Java developers, for instance, to easily incorporate local LLMs.  \n6. **Advanced Local Workflows & Fine-Grained Control:** Model Runner empowers engineers to execute sophisticated, multi-stage AI pipelines locally. The ability to dynamically tune model parameters for specific tasks without API costs fosters deep experimentation and accelerates the development of nuanced AI features.\n\nCollectively, these features address core engineering challenges in local AI development: cost, privacy, iteration speed, complexity, and environmental control.\n\n## **Future Horizons: From Beta to Mainstream**\n\nAs Docker Model Runner evolves beyond its Beta phase, several key developments will shape its impact:\n\n1. API Stability and Maturation:  \n   A crucial step will be the stabilization of its APIs. As noted during its Beta, APIs were subject to change. A stable API will provide the confidence developers need to build more robust and long-lasting integrations.  \n2. **Expanded Platform and Hardware Support:**  \n   * **Windows GPU Acceleration:** The full realization of performant GPU acceleration on Windows (especially for NVIDIA GPUs) will be a significant milestone, broadening its accessibility to a large segment of the developer community.  \n   * **Linux Enhancements:** While a Docker Engine plugin exists, further enhancements for Linux environments, potentially with more streamlined management features akin to Docker Desktop, will be important for server-side local development or specialized Linux-based AI workstations.  \n3. Comprehensive Custom Model Management:  \n   The ability for users to easily package, docker model push their own custom or fine-tuned models to any OCI-compliant registry, and then docker model pull and run them seamlessly is paramount. This will unlock Model Runner's full potential for organizations with bespoke AI needs, moving beyond curated public models.  \n4. Deeper Ecosystem Integrations:  \n   Expect continued and deeper integrations with:  \n   * **MLOps Tools:** Tighter connections with MLOps platforms for experiment tracking, model monitoring (even locally), and smoother transitions from local development to production deployment pipelines.  \n   * **IDEs:** More direct integrations within popular Integrated Development Environments for an even more fluid \"inner loop\" experience.  \n   * **More Inference Engines:** While llama.cpp is a strong start, the potential for a pluggable engine architecture could see Model Runner supporting a wider array of inference backends optimized for different model types or hardware.  \n5. Enhanced Observability and Debugging:  \n   As local AI workflows become more complex, improved tools for observing model behavior, debugging inference issues, and monitoring resource consumption locally will become increasingly valuable.\n\n## **The Enduring Impact: Local AI as a Standard Engineering Practice**\n\nDocker Model Runner is more than just a feature; it represents a significant step towards making local AI development a standard, accessible, and efficient engineering practice. By integrating AI model execution directly into the familiar and powerful Docker ecosystem, it lowers barriers, fosters innovation, and empowers developers to build the next generation of AI-powered applications with greater speed, control, and confidence.  \nThe journey from Beta to a fully mature product will undoubtedly bring further refinements and capabilities. However, the foundational principles and architectural choices already evident in Docker Model Runner signal a bright future for local-first AI development, driven by the needs and workflows of engineers.  \n\n*This blog post series has been based on information available about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change as the product evolves.*\n</BlogWrapper>     ","frontmatter":{"title":"Docker Model Runner: Engineering Summary & Future Horizons","subtitle":"A Technical Primer for Engineers","date":"May 20th, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/079679ec12e22e5eaead3990ab81da4f/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACjElEQVR42jWL7U8SARzH7+/orZubL7TNpQJqNS2zthweYcZTVMp4UBAFR4gPyBFwR8CdHB54Cgp33qFCbekos6XZ1KWb5ZT1JpsL9YVtNR/Wi17E6do+++7z+2w/gDpjE2cs/Yfl5BcTP2Kp02TiJEmdsuzfGeosmeBO9gLqQo4ZjhMGiOSixI8xdDMGz8bs5DjxJUEexkcP6ZED2rEUwL+NjRzEubKfIPfjJLf/OUgAE0drkb1Xvo0J91pQ1GVxvaTw3VhghxTDxpKbtaJelYEeDH6lvFskmo0GslGUI3ZOFIA/T8f2t1LHe8xJ1pok76tc8DsGhGxXW3XGKBpI4O1wwIFT4U/p/pVh5yoJrY5wrOSFBExzmGluqH85PfP7pz+VKahpLRPaSxsttwx2kRGGzQg9veTcWA+/XeiZCpvfhLozRPdrIr/mDAGomZBmEm+Z8BleRBq6bIW324uExksVoECpr1Wa9U8gCMKFTKoLj1OLH7STuIbFtecvagYHJAQmiwxJw6gQcddqew32cWxq0YAydUYvXzcotvmsEVbcMtCsggl6vnOUbnyOSIZRGYFJQigAerFGyCd86rP4Z1ILOx+3c6vbueXNXXZ+HaIyIihcZXDVW9B7PcPNjijYm5b5ZuXB8B0IAREMqLN6ZRgdSi6Nz634mYyFYFXesXrtQBVodAcnlXrbZbCt4K6mqMlU2NBZLIXI99nlre8t/ugNqxeo1nmu65G6DqRS5ahoHeSrnQLNsyqdq1rtrHncV6m0XWvzCJR9pU2mkiZjsbjjkY3ow9OWoelqjQuokHl4Une51M2TwwIFUqlABHmRe/hymKeA+QqE6w+RPFyRuMolzrIH0JVmJ0/m+QemD2rnUbfIewAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/079679ec12e22e5eaead3990ab81da4f/366fe/hero-image.jpg","srcSet":"/static/079679ec12e22e5eaead3990ab81da4f/9b503/hero-image.jpg 750w,\n/static/079679ec12e22e5eaead3990ab81da4f/321ef/hero-image.jpg 1080w,\n/static/079679ec12e22e5eaead3990ab81da4f/84dcd/hero-image.jpg 1366w,\n/static/079679ec12e22e5eaead3990ab81da4f/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/079679ec12e22e5eaead3990ab81da4f/5f850/hero-image.webp 750w,\n/static/079679ec12e22e5eaead3990ab81da4f/2c010/hero-image.webp 1080w,\n/static/079679ec12e22e5eaead3990ab81da4f/5126b/hero-image.webp 1366w,\n/static/079679ec12e22e5eaead3990ab81da4f/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/079679ec12e22e5eaead3990ab81da4f/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACjElEQVR42jWL7U8SARzH7+/orZubL7TNpQJqNS2zthweYcZTVMp4UBAFR4gPyBFwR8CdHB54Cgp33qFCbekos6XZ1KWb5ZT1JpsL9YVtNR/Wi17E6do+++7z+2w/gDpjE2cs/Yfl5BcTP2Kp02TiJEmdsuzfGeosmeBO9gLqQo4ZjhMGiOSixI8xdDMGz8bs5DjxJUEexkcP6ZED2rEUwL+NjRzEubKfIPfjJLf/OUgAE0drkb1Xvo0J91pQ1GVxvaTw3VhghxTDxpKbtaJelYEeDH6lvFskmo0GslGUI3ZOFIA/T8f2t1LHe8xJ1pok76tc8DsGhGxXW3XGKBpI4O1wwIFT4U/p/pVh5yoJrY5wrOSFBExzmGluqH85PfP7pz+VKahpLRPaSxsttwx2kRGGzQg9veTcWA+/XeiZCpvfhLozRPdrIr/mDAGomZBmEm+Z8BleRBq6bIW324uExksVoECpr1Wa9U8gCMKFTKoLj1OLH7STuIbFtecvagYHJAQmiwxJw6gQcddqew32cWxq0YAydUYvXzcotvmsEVbcMtCsggl6vnOUbnyOSIZRGYFJQigAerFGyCd86rP4Z1ILOx+3c6vbueXNXXZ+HaIyIihcZXDVW9B7PcPNjijYm5b5ZuXB8B0IAREMqLN6ZRgdSi6Nz634mYyFYFXesXrtQBVodAcnlXrbZbCt4K6mqMlU2NBZLIXI99nlre8t/ugNqxeo1nmu65G6DqRS5ahoHeSrnQLNsyqdq1rtrHncV6m0XWvzCJR9pU2mkiZjsbjjkY3ow9OWoelqjQuokHl4Une51M2TwwIFUqlABHmRe/hymKeA+QqE6w+RPFyRuMolzrIH0JVmJ0/m+QemD2rnUbfIewAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/079679ec12e22e5eaead3990ab81da4f/22857/hero-image.jpg","srcSet":"/static/079679ec12e22e5eaead3990ab81da4f/9976b/hero-image.jpg 125w,\n/static/079679ec12e22e5eaead3990ab81da4f/e19ca/hero-image.jpg 250w,\n/static/079679ec12e22e5eaead3990ab81da4f/22857/hero-image.jpg 500w,\n/static/079679ec12e22e5eaead3990ab81da4f/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/079679ec12e22e5eaead3990ab81da4f/842b0/hero-image.webp 125w,\n/static/079679ec12e22e5eaead3990ab81da4f/b597c/hero-image.webp 250w,\n/static/079679ec12e22e5eaead3990ab81da4f/49c7e/hero-image.webp 500w,\n/static/079679ec12e22e5eaead3990ab81da4f/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/docker-model-runner-engineering-summary-future-horizons"}},{"id":"cd1299a5-8e67-5835-9211-e24b698da225","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nIn our [ongoing exploration](/blog/category/docker) of Docker Model Runner, we've covered its OCI-based model management, performance architecture, OpenAI-compatible API, and Docker Compose integration. Now, we turn to a specific, yet highly impactful, synergy: how Docker Model Runner empowers **Java developers using the Spring AI framework** to seamlessly incorporate local Large Language Models (LLMs) into their applications.  \nFor Java engineers vested in the Spring ecosystem, Spring AI offers a familiar and powerful abstraction layer for interacting with various AI models. Docker Model Runner's compatibility provides a straightforward path to leverage these local models without stepping outside the conventional Spring development paradigm.\n\n## **Spring AI: Simplifying AI for Java Applications**\n\nBefore diving into the integration, it's worth briefly understanding Spring AI's mission. Spring AI aims to apply core Spring principles—such as autoconfiguration, dependency injection, and portable service abstractions—to the domain of artificial intelligence. It provides Java developers with:\n\n* **Consistent APIs:** A unified API for interacting with different AI models (both local and remote), reducing the need to learn multiple vendor-specific SDKs.  \n* **Abstraction Layers:** Components like ChatClient, EmbeddingClient, and ImageClient abstract away the underlying model provider.  \n* **Integration with Spring Boot:** Easy setup and configuration within Spring Boot applications.\n\n## **Docker Model Runner as a Local \"Ollama\" for Spring AI**\n\nSpring AI supports various AI model providers, including commercial cloud services (like OpenAI, Azure OpenAI) and self-hosted solutions (like Ollama). From Spring AI's perspective, Docker Model Runner, with its OpenAI-compatible API, effectively acts like a local, easily manageable Ollama-style endpoint.  \nWhen Docker Model Runner is active and serving a model (e.g., Llama 3, Gemma) with its API endpoint accessible (typically http://localhost:12434 or http://model-runner.docker.internal if accessed from another container), Spring AI can be configured to point to it.  \nHere's how a Java engineer benefits:\n\n1. **Simplified Configuration in Spring Boot**\n  \n   Spring AI's autoconfiguration can often detect and set up the necessary beans to interact with an OpenAI-compatible endpoint. For Docker Model Runner, this typically involves setting a few properties in your application.properties or application.yml file:  \n   \n   ```java\n   \\# For Spring AI 0.8.x (or similar versions)  \n   spring.ai.openai.chat.base-url=http://localhost:12434/engines/v1 \n   \\# Or your specific DMR endpoint  \n   spring.ai.openai.chat.options.model=ai/llama3.2:1B-Q8\\_0 \n   \\# The model you want to use  \n   use  \n   spring.ai.openai.api-key=YOUR\\_DUMMY\\_API\\_KEY\\_OR\\_EMPTY\n   \\# Potentially disable API key if DMR doesn't require it strictly for local \n   ```\n\n   _(Note: The exact property names and structure might vary slightly based on the Spring AI version and whether you're configuring a generic OpenAI client or a more specific Ollama-like client type if Spring AI introduces more direct DMR support.)_  \n\n2. **Leveraging Spring AI's ChatClient and EmbeddingClient**\n\n   Once configured, developers can inject and use Spring AI's standard clients without needing to know that the underlying provider is Docker Model Runner. \n\n```java \n   import org.springframework.ai.chat.ChatClient;  \n   import org.springframework.ai.chat.prompt.Prompt;  \n   import org.springframework.beans.factory.annotation.Autowired;  \n   import org.springframework.stereotype.Service;\n\n   @Service  \n   public class MyAiService {\n\n       private final ChatClient chatClient;\n\n       @Autowired  \n       public MyAiService(ChatClient chatClient) {  \n           this.chatClient \\= chatClient;  \n       }\n\n       public String getJokeAbout(String topic) {  \n           Prompt prompt \\= new Prompt(\"Tell me a short joke about \" \\+ topic);  \n           return chatClient.call(prompt).getResult().getOutput().getContent();  \n       }  \n   }\n```\n\n   This code remains the same whether Spring AI is talking to OpenAI's cloud API, a self-hosted Ollama instance, or Docker Model Runner serving a local model. This portability is a huge win.  \n\n3. **Seamless Local Development and Testing**\n   Engineers can develop and test AI-driven features entirely locally using their preferred Java tools and the Spring framework. Docker Model Runner handles the model serving, and Spring AI provides the clean Java interface. This speeds up iteration cycles and reduces reliance on potentially costly cloud APIs during development.  \n\n4. **Consistency with Production (Potentially)**  \n   While Docker Model Runner is primarily for local development, the abstraction provided by Spring AI means that switching to a production-grade, potentially cloud-hosted model provider for deployment can be achieved mainly through configuration changes, without altering the core application logic.\n\n## **The Bigger Picture: Local AI in Enterprise Java**\n\nThe integration with Spring AI is significant because it brings the ease of local LLM experimentation directly into the robust, enterprise-focused Java and Spring ecosystem. It allows Java teams to:\n\n* **Prototype AI features rapidly.**  \n* **Upskill on AI concepts using familiar tools.**  \n* **Conduct local, private testing of AI interactions with business data.**  \n* **Integrate AI into existing Spring Boot applications with minimal friction.**\n\nDocker's collaboration with Spring AI (as noted in some announcements) underscores a shared vision of making AI more accessible and developer-friendly across different programming environments. By ensuring Docker Model Runner presents an API that Spring AI can readily consume, both platforms contribute to lowering the barrier to entry for sophisticated AI development.  \nFor Java engineers, this means Docker Model Runner isn't just another tool; it's a key enabler for leveraging the power of local LLMs within the comfort and productivity of the Spring framework.\n\n## **Next, we'll delve into some practical, task-specific configurations and advanced use cases you can explore with Docker Model Runner, moving beyond basic chat completions.**\n\n*This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change. Configuration details for Spring AI may vary based on specific versions.*\n\n</BlogWrapper>","frontmatter":{"title":"Spring AI: Streamlining Local LLM Integration for Java Developers","subtitle":"Docker Model Runner - A Technical Primer for Engineers","date":"May 14th, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/7a5be1548e7efed21a2ef4929e78a5ab/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACbklEQVR42iXC2U/aYAAA8D5uQ6FA6devx9eLQqHcYqWc5daqUAVUBDGbYzpx80DmtWQvZolLfFjiFh+W/a0z2S8/zEkiAgVfuqAwx0hzJHrthg5WBksFkK/SK23SMN2pjIMWX+HQAXgHiRw+7n/MBQSCV4ES84qqS1TfUAIym8bBUWR1g0ploGGCxTwo1kI7IzZXfg34Oc4/x8gORnIwMoYD3gkFlxRyy+GXZN0qXcy0xgphWfTlBZnUiWTaV8jzZjWwNfQUa05RnVc0TzDmlkIYEU155bCLlnA+KKx3woO3QDdctYpv02bqy0yxgterhFkGqQyIL6G9d55syR2IwrjuS2exeSjgWoIIxXGSE3pDulJjbBt0NzyjXdTpekrm4P2gtNbOjSfeRgX1trlOH2hJIqHjkQUMp3iCU4Bfg5mC/8stvbyKDg/FRiPbtsL394q91fk8rt7crE1mnlwettrq9JrUkl5OIQMxDMoRSgqhzS3l9Fz78TO72k70u0S7RQ93tMnJ5O6Mu5o2H393p3ep/k6s2488/xEHIyqpA9aPQSFMGUX/7JrNmMGzq8T4ZLE3QkbZndLTdfvb17vO7ezg4VfJ3jVarYXj0+D0BpWb0um5R1Ix1h8D4YR4fCL0R0AIIctW9sdiuye1evbjs/3wZH9/7D/9Vfc/8MMDtLENtZRYqIv9PUKJYGJEByjoMwq8ZUuJLBtJw/oytNappSKTr/KVJqqtUJUGiOsQymzDkqYzIWuSaoyRo5gPBdy06PTQPkGF6RxbaQpHH4XJJz6U5sQoG88I00ux1eGrFp8x2WIVFMpupDh9rJdT/gG8qIzT1/dCagAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7a5be1548e7efed21a2ef4929e78a5ab/366fe/hero-image.jpg","srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/9b503/hero-image.jpg 750w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/321ef/hero-image.jpg 1080w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/84dcd/hero-image.jpg 1366w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/5f850/hero-image.webp 750w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/2c010/hero-image.webp 1080w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/5126b/hero-image.webp 1366w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/7a5be1548e7efed21a2ef4929e78a5ab/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACbklEQVR42iXC2U/aYAAA8D5uQ6FA6devx9eLQqHcYqWc5daqUAVUBDGbYzpx80DmtWQvZolLfFjiFh+W/a0z2S8/zEkiAgVfuqAwx0hzJHrthg5WBksFkK/SK23SMN2pjIMWX+HQAXgHiRw+7n/MBQSCV4ES84qqS1TfUAIym8bBUWR1g0ploGGCxTwo1kI7IzZXfg34Oc4/x8gORnIwMoYD3gkFlxRyy+GXZN0qXcy0xgphWfTlBZnUiWTaV8jzZjWwNfQUa05RnVc0TzDmlkIYEU155bCLlnA+KKx3woO3QDdctYpv02bqy0yxgterhFkGqQyIL6G9d55syR2IwrjuS2exeSjgWoIIxXGSE3pDulJjbBt0NzyjXdTpekrm4P2gtNbOjSfeRgX1trlOH2hJIqHjkQUMp3iCU4Bfg5mC/8stvbyKDg/FRiPbtsL394q91fk8rt7crE1mnlwettrq9JrUkl5OIQMxDMoRSgqhzS3l9Fz78TO72k70u0S7RQ93tMnJ5O6Mu5o2H393p3ep/k6s2488/xEHIyqpA9aPQSFMGUX/7JrNmMGzq8T4ZLE3QkbZndLTdfvb17vO7ezg4VfJ3jVarYXj0+D0BpWb0um5R1Ix1h8D4YR4fCL0R0AIIctW9sdiuye1evbjs/3wZH9/7D/9Vfc/8MMDtLENtZRYqIv9PUKJYGJEByjoMwq8ZUuJLBtJw/oytNappSKTr/KVJqqtUJUGiOsQymzDkqYzIWuSaoyRo5gPBdy06PTQPkGF6RxbaQpHH4XJJz6U5sQoG88I00ux1eGrFp8x2WIVFMpupDh9rJdT/gG8qIzT1/dCagAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7a5be1548e7efed21a2ef4929e78a5ab/22857/hero-image.jpg","srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/9976b/hero-image.jpg 125w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/e19ca/hero-image.jpg 250w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/22857/hero-image.jpg 500w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/7a5be1548e7efed21a2ef4929e78a5ab/842b0/hero-image.webp 125w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/b597c/hero-image.webp 250w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/49c7e/hero-image.webp 500w,\n/static/7a5be1548e7efed21a2ef4929e78a5ab/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/spring-ai-streamlining-local-llm-integration-for-java-developers"}},{"id":"e9a58b6f-459e-5026-bb39-eb2ce65f1ab5","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nSo far in our [series on Docker Model Runner](/blog/category/docker), we've dissected its OCI-based model management, its performance-optimized execution architecture, and its OpenAI-compatible API. Now, we explore a feature that truly elevates its utility for engineers building complex systems: **deep integration with Docker Compose via a novel provider service type.**  \n\nFor engineers, Docker Compose is the go-to tool for defining and running multi-container Docker applications. The introduction of the provider service type specifically for Model Runner bridges the gap between local AI model execution and the broader application stack, allowing you to define and manage AI models as integral components of your local development environment declaratively.\n\n## **Beyond CLI: Models as First-Class Services in Compose**\n\nWhile docker model run is handy for quick tests, real-world applications often involve multiple interacting services—a web frontend, a backend API, a database, and now, an AI model. Docker Model Runner's Compose integration allows you to define the AI model itself as a service within your `docker-compose.yml` file.  \n\nThe key innovation here is the provider attribute within a service definition. Here's a conceptual example based on Docker's documentation:\n\n```yaml\nservices:  \n  model\\_provider\\_service: \\# You can name this service as you like  \n    provider:  \n      type: model        \\# Specifies this is a model provider  \n      image: ai/llama3.2:1B-Q8\\_0 \\# The OCI image for the model  \n    \\# No 'build' or 'image' directives here in the traditional sense for the provider\n\n  my\\_app\\_service:  \n    build: ./app  \n    ports:  \n      \\- \"8080:80\"  \n    depends\\_on:  \n      \\- model\\_provider\\_service \\# Ensures model is ready before the app starts  \n    environment:  \n      \\# Environment variables will be injected here (see below)  \n      MODEL\\_NAME: ${MODEL\\_PROVIDER\\_SERVICE\\_MODEL}  \n      MODEL\\_URL: ${MODEL\\_PROVIDER\\_SERVICE\\_URL}\n```\n\nIn this setup:\n\n* model\\_provider\\_service doesn't run a traditional container in the same way my\\_app\\_service does. Instead, it instructs Docker Compose to leverage Docker Model Runner.  \n* Docker Model Runner, when processing this provider service, will ensure the specified image (the AI model) is pulled and made available via its host-native inference engine.\n\n## **Automatic Model Provisioning and Service Discovery**\n\nThis Compose integration brings significant benefits for engineers:\n\n1. **Declarative Model Dependencies:**  \n   * You declare your AI model dependency directly in your docker-compose.yml. Docker Model Runner handles the provisioning (pulling and preparing the model if needed) when you run docker compose up.  \n   * This is a stark improvement over manual docker model run commands or custom scripts to manage model lifecycle alongside your application stack.  \n2. **Automated Service Discovery via Environment Variables:**  \n   * This is a crucial feature for seamless integration. When my\\_app\\_service starts (after model\\_provider\\_service is ready), Docker Compose automatically injects environment variables into my\\_app\\_service.  \n   * These variables typically follow the pattern: PROVIDER\\_SERVICE\\_NAME\\_MODEL and PROVIDER\\_SERVICE\\_NAME\\_URL.  \n     * MODEL\\_PROVIDER\\_SERVICE\\_MODEL: Contains the name/tag of the model being served (e.g., ai/llama3.2:1B-Q8\\_0).  \n     * MODEL\\_PROVIDER\\_SERVICE\\_URL: Provides the URL your application should use to access the Model Runner's API endpoint for this model. This would often point to the internal DNS http://model-runner.docker.internal or a host-accessible TCP port if configured.  \n   * Your application code can then dynamically use these environment variables to configure its AI client, making the connection to the local model effortless and portable.  \n3. **Simplified depends\\_on for Startup Order:**  \n   * Using depends\\_on ensures that your application services only start after Model Runner has signaled that the model provider is ready. This prevents your application from trying to connect to a model that isn't yet available.\n\n## **Engineering Benefits for Complex AI Applications**\n\nThis declarative, integrated approach offers tangible advantages:\n\n* **Reproducible AI Development Environments:** Your entire local stack, including the specific AI model version, is defined in code (docker-compose.yml), making it easy to share, version control, and ensure consistency across development team members.  \n* **Simplified Onboarding:** New developers can get a complex AI-powered application stack running locally with a single docker compose up command.  \n* **Streamlined Local Testing of AI Features:** Test end-to-end flows involving your application logic and AI model interactions in a fully integrated local environment that mirrors how services would interact.  \n* **Foundation for Local MLOps Loops:** While focused on local development, this pattern lays a conceptual foundation for how AI models can be treated as manageable dependencies within larger application architectures, aligning with MLOps principles.\n\nBy treating AI models as discoverable services managed by Compose, Docker Model Runner significantly lowers the barrier to building and iterating on sophisticated multi-service applications that leverage local AI capabilities. This moves beyond simply running a model in isolation to truly integrating AI into your development workflow.  \nNext up, we'll explore how Docker Model Runner specifically caters to Java developers through its integration with frameworks like Spring AI, further simplifying the adoption of local AI.  \n\n*This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.*\n\n</BlogWrapper>","frontmatter":{"title":"Docker Compose: Orchestrating Multi-Service AI Applications Locally","subtitle":"Docker Model Runner - A Technical Primer for Engineers","date":"April 24th, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/97d66df31989169322ece96b70bfd7a8/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACo0lEQVR42jWL209SAQDGz1/Rk+VDq1aGpQk6BW2o6NDkKiTZFJAIsABJwbiIoAgcOQeO4EFuAuq84FDIynJuqZslm6WUa5lTmm3OFfrQW08d59p++/b9tu8DQqfR4GnkjGwkeBLBVLPUr5jXeQ7DhvURZHdq/M+c7zTsz4b82TA2wAicnJcwgBygrn3UkwkMH4Y8P4LQN/Qmt6HerJfPT+D4WmlsciAWcyzNIpno8E+/+8A/lAm4/wMEf6WQ/Tn7F79hGbZto/aU9yHseBx5LZ5ISkYjzoCprktlDIxb4vH2mX5wxzuwhdrSXus2ak2jQN9WDN7fmPx79DQwRWnp4um8DwSWwnrJxQrm3YbniMJr29s2H3+FkottPrf544huza1f8ehW3fo1D9C/s2LYmFcv+jhGa04+s5CqqpX7KyXQ5Sp2GVXmV010WMM8KGGZWVVPL90HbfK4s+PVkCKJKBeGAGHE2Zve7E2nJCNBls5xncS5w1ZWCuw1T1CqzKexzT2yTFHEo0zNLA9+IzCPkVsNLX6obcwliLoAjhPiumDF2+Wed+sMZW85Q1zKU4gMwxRe97Xy1oI6OaszZEtsBJY/d7gTL9/vhl9siqYTXATiOGGAZoZpJked3krrg8kKfXGzNK+WV0AT3Wvvq1eZyO3aS0VsYTQZzByv7f7+9D2b2jvSji9Qe0C62QlUqxwYlGcwRQXVqhFKp7NCai5gS3JLGxrVlq5gvKkbMgYWzEsfdOGEVAsTyNx8IrtKCWIXgMgHSQKQyLeTBHYsia02knCwQgSX8EyExs5KvgZfzRNKrQy28sYtyoXckpy8GhxVhr2IAhDA0+14BgZ4XghM8EzpNgJrEM9y4GqMV4iynNtNuUXNV8vEuGp1Ed1azIbO9gzwH9t6T8ElxPFqAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/97d66df31989169322ece96b70bfd7a8/366fe/hero-image.jpg","srcSet":"/static/97d66df31989169322ece96b70bfd7a8/9b503/hero-image.jpg 750w,\n/static/97d66df31989169322ece96b70bfd7a8/321ef/hero-image.jpg 1080w,\n/static/97d66df31989169322ece96b70bfd7a8/84dcd/hero-image.jpg 1366w,\n/static/97d66df31989169322ece96b70bfd7a8/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/97d66df31989169322ece96b70bfd7a8/5f850/hero-image.webp 750w,\n/static/97d66df31989169322ece96b70bfd7a8/2c010/hero-image.webp 1080w,\n/static/97d66df31989169322ece96b70bfd7a8/5126b/hero-image.webp 1366w,\n/static/97d66df31989169322ece96b70bfd7a8/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/97d66df31989169322ece96b70bfd7a8/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACo0lEQVR42jWL209SAQDGz1/Rk+VDq1aGpQk6BW2o6NDkKiTZFJAIsABJwbiIoAgcOQeO4EFuAuq84FDIynJuqZslm6WUa5lTmm3OFfrQW08d59p++/b9tu8DQqfR4GnkjGwkeBLBVLPUr5jXeQ7DhvURZHdq/M+c7zTsz4b82TA2wAicnJcwgBygrn3UkwkMH4Y8P4LQN/Qmt6HerJfPT+D4WmlsciAWcyzNIpno8E+/+8A/lAm4/wMEf6WQ/Tn7F79hGbZto/aU9yHseBx5LZ5ISkYjzoCprktlDIxb4vH2mX5wxzuwhdrSXus2ak2jQN9WDN7fmPx79DQwRWnp4um8DwSWwnrJxQrm3YbniMJr29s2H3+FkottPrf544huza1f8ehW3fo1D9C/s2LYmFcv+jhGa04+s5CqqpX7KyXQ5Sp2GVXmV010WMM8KGGZWVVPL90HbfK4s+PVkCKJKBeGAGHE2Zve7E2nJCNBls5xncS5w1ZWCuw1T1CqzKexzT2yTFHEo0zNLA9+IzCPkVsNLX6obcwliLoAjhPiumDF2+Wed+sMZW85Q1zKU4gMwxRe97Xy1oI6OaszZEtsBJY/d7gTL9/vhl9siqYTXATiOGGAZoZpJked3krrg8kKfXGzNK+WV0AT3Wvvq1eZyO3aS0VsYTQZzByv7f7+9D2b2jvSji9Qe0C62QlUqxwYlGcwRQXVqhFKp7NCai5gS3JLGxrVlq5gvKkbMgYWzEsfdOGEVAsTyNx8IrtKCWIXgMgHSQKQyLeTBHYsia02knCwQgSX8EyExs5KvgZfzRNKrQy28sYtyoXckpy8GhxVhr2IAhDA0+14BgZ4XghM8EzpNgJrEM9y4GqMV4iynNtNuUXNV8vEuGp1Ed1azIbO9gzwH9t6T8ElxPFqAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/97d66df31989169322ece96b70bfd7a8/22857/hero-image.jpg","srcSet":"/static/97d66df31989169322ece96b70bfd7a8/9976b/hero-image.jpg 125w,\n/static/97d66df31989169322ece96b70bfd7a8/e19ca/hero-image.jpg 250w,\n/static/97d66df31989169322ece96b70bfd7a8/22857/hero-image.jpg 500w,\n/static/97d66df31989169322ece96b70bfd7a8/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/97d66df31989169322ece96b70bfd7a8/842b0/hero-image.webp 125w,\n/static/97d66df31989169322ece96b70bfd7a8/b597c/hero-image.webp 250w,\n/static/97d66df31989169322ece96b70bfd7a8/49c7e/hero-image.webp 500w,\n/static/97d66df31989169322ece96b70bfd7a8/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/docker-compose-orchestrating-multi-service-ai-applications-locally"}},{"id":"0c082d17-11b2-511a-bc98-be03ac9474a3","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nIn our series on [Docker Model Runner](/blog/category/docker), we've explored Docker Model Runner's role in simplifying local AI development and its strategic use of OCI artifacts for model management. Now, we peel back another layer to examine a critical aspect for any engineer working with Large Language Models (LLMs): **performance**. How does Docker Model Runner achieve the responsiveness needed for an efficient local development loop? The answers lie in its architectural choices, particularly its embrace of host-native execution and direct GPU access.  \n\nFor engineers, \"local\" often implies a trade-off: convenience versus raw power. Docker Model Runner attempts to bridge this gap, and understanding its performance model is key to leveraging it effectively.\n\n## **The Architectural Pivot: Why docker model run Isn't docker container run**\n\nOne of the most crucial, and perhaps initially counter-intuitive, aspects of Docker Model Runner is how it executes AI models. Seasoned Docker users might expect docker model run some-model to spin up an isolated Docker container housing the model and its inference engine. However, Model Runner takes a more direct path to prioritize local performance.  \n\nAs detailed in multiple technical breakdowns and official documentation, when you execute `docker model run`:\n\n* **No Traditional Container for Inference:** The command doesn't launch a standard Docker container for the core inference task.  \n* **Host-Native Inference Server:** Instead, it interacts with an inference server (initially built on the efficient llama.cpp engine) that runs as a **native process directly on your host machine**. This server is managed as part of Docker Desktop or the Model Runner plugin.\n\nThis is a deliberate engineering decision. Docker's own statements reveal that this approach was chosen to \"significantly improve performance by eliminating containerization overhead for resource-intensive AI workloads\" and to avoid the \"performance limitations of running models inside virtual machines.\" While Docker's traditional strength lies in containerization for isolation and portability, for the demanding task of LLM inference locally, the raw performance gains from direct host execution were deemed paramount.\n\n## **Unlocking Hardware: Direct GPU Acceleration**\n\nA major bottleneck for LLM performance is often GPU access. Docker Model Runner addresses this head-on:\n\n1. **Optimized for Apple Silicon (Metal API):**  \n   * For developers on macOS with Apple Silicon (M-series chips), Model Runner's host-native inference engine is designed to **directly access Apple's Metal API**. This provides a highly optimized path to the GPU, bypassing virtualization layers that can throttle performance. This direct access can offer a noticeable speed advantage compared to running models within a container that has to go through more layers to reach the GPU.  \n2. **Windows GPU Support on the Roadmap:**  \n   * Recognizing the diverse hardware landscape, Docker has explicitly included support for GPU acceleration on Windows platforms (primarily targeting NVIDIA GPUs) in its development plans. This is a critical feature for broadening Model Runner's appeal and utility.\n\nThis strategy of direct hardware access, especially for GPUs, is a pragmatic choice. It acknowledges that for the \"inner loop\" of local AI development—where rapid iteration and experimentation are key—minimizing inference latency is crucial.\n\n## **Intelligent Resource Management: Efficiency Under the Hood**\n\nBeyond raw execution speed, Docker Model Runner incorporates features for efficient resource utilization:\n\n* **On-Demand Model Loading:** Models are not kept in memory at all times. When you make a request to a specific model (e.g., via an API call from your application or a docker model run command), Model Runner loads it into memory \"on-demand,\" provided the model files have already been pulled locally. This means you don't necessarily have to issue a docker model run before your application can start interacting with a model.  \n* **Memory Caching with Inactivity Timeout:** Once loaded, a model remains in memory to serve subsequent requests quickly. However, to conserve system resources, models are automatically unloaded if they remain inactive for a predefined period. This inactivity timeout is **currently set to 5 minutes**. This is a practical detail that impacts how long a model stays \"warm\" and ready for immediate use during an interactive development session.\n\nThis combination of on-demand loading and inactivity-based unloading helps balance responsiveness with efficient use of your local machine's memory.\n\n## **The Engineering Trade-Off: Performance vs. Isolation**\n\nThe decision to run the inference engine as a host-native process is a clear trade-off: Docker is prioritizing local inference speed and direct hardware access over the complete process isolation typically provided by containers *for the inference step itself*. While the applications *using* the model can still be containerized and benefit from Docker's isolation, the model execution core operates closer to the metal.  \nThis architectural choice highlights Docker's commitment to making the local AI development experience as smooth and fast as possible, even if it means deviating slightly from its traditional container-centric execution model for this specific, performance-sensitive component.  \nUnderstanding this performance architecture—host-native execution, direct GPU access, and smart resource management—allows engineers to better anticipate Model Runner's behavior, optimize their local AI workflows, and appreciate the engineering decisions aimed at making local LLM development more practical and efficient.  \n\nIn our next post, we'll explore the API architecture of Docker Model Runner, focusing on its OpenAI compatibility and the various ways you can connect your applications to the local inference engine.  \n\n*This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.*\n</BlogWrapper>","frontmatter":{"title":"Host-Native Execution & GPU Deep Dive","subtitle":"Docker Model Runner - A Technical Primer for Engineers","date":"April 15th, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/a0d502a06407981844fc859ba51cbc91/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACjElEQVR42iXNW1MSAQAF4C0XFGHl5goLC7sEI7IgIsvuIpQQ4KiIgJcAQS4qchMEVEAQNcnMnExsGrtM9ebUc8/9uLBmvjlPZ84BhmJl/kZdmGuLy53hwyvJ2a308pPs5of88z369Sfa/Y5EM0goKTu6kNzdS999k7y+g0+6w423onIH4K5keJEiFK8ObdUFhbZg96Wg9kZ0/J7f+cCP7yAGRm6y4W6v0r0AF1u8vVf86rmwdMrPtKBUDRj0JblLaSich+IVKLUPZVv8dI0fy6l2245qm86UJ1NFUyyvDSRM6V3n2RUSy/OSe7xEFYqWAM5MeNC7zvWu82bDUCgDL23IXYuYzRO8+DiRr0hXo8pgXBlJYYthZDPtuO5qGLfctzYULQ4GNoE+av6RzQ8xs5JxRj4TxDx+hJzWeIJU9QT2+rXhxFrpgNhpocm0Yn7ZkCjIjDZrrknsn7PcIQAO5ZnGBe72YwvLCpcXexEdYRxqd1CbKaGBkK3UoGvHloOKvrE/Fs0SGznFfMB+dGU9vWU/WwIsX35lf/8Z8y+j4ajSv6JKb0vdc/hzHxHN6bYKU6Wmr3O5064zsYghWzWnKwjjcHVuqNY1m5wDQPXUgMEFEbRYa4SN9Aj9VKQxwmOkpdDSRwqYc8WZbwaOu1RybzJdtx+ei0mrNVMb3W4+HrUDLIwGcYatorhKPYewcUfNfJ1JoNIpaKdqNiCzOGRWl4Jyan2rT3yryDgtlKnFBMWZ8PRpbAAoJ1mohaWkehMgbmXhTD9OctWkkKDgqWnYQEtoO2ywiPRmsZES6UiB3srGLCBG914BEJl8IDP/XwF7FDSI0mwF1Y+RPQNK8wBOcjBzTz9OPRTQfymz/AX1Lad060IfMQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/a0d502a06407981844fc859ba51cbc91/366fe/hero-image.jpg","srcSet":"/static/a0d502a06407981844fc859ba51cbc91/9b503/hero-image.jpg 750w,\n/static/a0d502a06407981844fc859ba51cbc91/321ef/hero-image.jpg 1080w,\n/static/a0d502a06407981844fc859ba51cbc91/84dcd/hero-image.jpg 1366w,\n/static/a0d502a06407981844fc859ba51cbc91/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/a0d502a06407981844fc859ba51cbc91/5f850/hero-image.webp 750w,\n/static/a0d502a06407981844fc859ba51cbc91/2c010/hero-image.webp 1080w,\n/static/a0d502a06407981844fc859ba51cbc91/5126b/hero-image.webp 1366w,\n/static/a0d502a06407981844fc859ba51cbc91/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/a0d502a06407981844fc859ba51cbc91/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACjElEQVR42iXNW1MSAQAF4C0XFGHl5goLC7sEI7IgIsvuIpQQ4KiIgJcAQS4qchMEVEAQNcnMnExsGrtM9ebUc8/9uLBmvjlPZ84BhmJl/kZdmGuLy53hwyvJ2a308pPs5of88z369Sfa/Y5EM0goKTu6kNzdS999k7y+g0+6w423onIH4K5keJEiFK8ObdUFhbZg96Wg9kZ0/J7f+cCP7yAGRm6y4W6v0r0AF1u8vVf86rmwdMrPtKBUDRj0JblLaSich+IVKLUPZVv8dI0fy6l2245qm86UJ1NFUyyvDSRM6V3n2RUSy/OSe7xEFYqWAM5MeNC7zvWu82bDUCgDL23IXYuYzRO8+DiRr0hXo8pgXBlJYYthZDPtuO5qGLfctzYULQ4GNoE+av6RzQ8xs5JxRj4TxDx+hJzWeIJU9QT2+rXhxFrpgNhpocm0Yn7ZkCjIjDZrrknsn7PcIQAO5ZnGBe72YwvLCpcXexEdYRxqd1CbKaGBkK3UoGvHloOKvrE/Fs0SGznFfMB+dGU9vWU/WwIsX35lf/8Z8y+j4ajSv6JKb0vdc/hzHxHN6bYKU6Wmr3O5064zsYghWzWnKwjjcHVuqNY1m5wDQPXUgMEFEbRYa4SN9Aj9VKQxwmOkpdDSRwqYc8WZbwaOu1RybzJdtx+ei0mrNVMb3W4+HrUDLIwGcYatorhKPYewcUfNfJ1JoNIpaKdqNiCzOGRWl4Jyan2rT3yryDgtlKnFBMWZ8PRpbAAoJ1mohaWkehMgbmXhTD9OctWkkKDgqWnYQEtoO2ywiPRmsZES6UiB3srGLCBG914BEJl8IDP/XwF7FDSI0mwF1Y+RPQNK8wBOcjBzTz9OPRTQfymz/AX1Lad060IfMQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/a0d502a06407981844fc859ba51cbc91/22857/hero-image.jpg","srcSet":"/static/a0d502a06407981844fc859ba51cbc91/9976b/hero-image.jpg 125w,\n/static/a0d502a06407981844fc859ba51cbc91/e19ca/hero-image.jpg 250w,\n/static/a0d502a06407981844fc859ba51cbc91/22857/hero-image.jpg 500w,\n/static/a0d502a06407981844fc859ba51cbc91/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/a0d502a06407981844fc859ba51cbc91/842b0/hero-image.webp 125w,\n/static/a0d502a06407981844fc859ba51cbc91/b597c/hero-image.webp 250w,\n/static/a0d502a06407981844fc859ba51cbc91/49c7e/hero-image.webp 500w,\n/static/a0d502a06407981844fc859ba51cbc91/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/host-native-execution-gpu-deep-dive"}},{"id":"35e2e324-224b-5a3d-a67d-0b8ed1694ace","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nIn our last [post in this series](/blog/category/docker), explored Docker Model Runner's OCI-based model management and its performance-centric execution model, we now turn our attention to another critical area for engineers: its **API architecture and connectivity options**. How do your applications actually *talk* to the models running locally via Model Runner? The answer lies in a thoughtfully designed API layer, with OpenAI compatibility at its core, and flexible connection methods to suit diverse development scenarios.  \n\nFor engineers, a well-defined and accessible API is paramount. It dictates the ease of integration, the reusability of existing code, and the overall developer experience when building AI-powered applications.\n\n## **The Heart of the Engine: llama.cpp and a Pluggable Future**\n\nIn its initial Beta release, Docker Model Runner's inference capabilities are powered by an integrated engine built on llama.cpp. This open-source project is renowned for its efficient execution of LLMs across various hardware, making it a solid foundation for local inference.  \n\nWhen you interact with Model Runner, you're essentially communicating with this llama.cpp-based server, which runs as a native host process. The API paths often reflect this underlying engine, for example, with endpoints structured under /engines/llama.cpp/v1/... or a more generalized `/engines/v1/...`.  \nWhile llama.cpp provides a robust initial backbone, the API path structure (e.g., `/engines/...`) hints at a potentially pluggable architecture. This is a common design pattern that could allow Docker to integrate other inference engines or model serving technologies in the future. This foresight means Model Runner could evolve to support a wider array of model types, quantization methods, or hardware acceleration frameworks without requiring a fundamental redesign of its API interaction model.\n\n## **The \"Superpower\": OpenAI-Compatible API**\n\nPerhaps the most strategically significant aspect of Model Runner's API is its **OpenAI compatibility**. This is a game-changer for several reasons:\n\n1. **Leverage Existing SDKs and Tools:** Engineers can use their existing OpenAI SDKs (Python, Node.js, etc.) and a vast ecosystem of compatible tools like LangChain or LlamaIndex with minimal, if any, code changes. This dramatically lowers the barrier to adoption.  \n2. **Simplified Migration:** If you've been developing against OpenAI's cloud APIs, transitioning to local models with Model Runner can often be as simple as changing the baseURL in your client configuration. This seamless switch accelerates local development and testing.  \n3. **Reduced Learning Curve:** There's no need to learn a new, proprietary API. The familiar OpenAI request/response structures for tasks like chat completions (`/chat/completions`) or embeddings (`/embeddings`) remain consistent. \n\nThis adherence to a de facto industry standard API is a deliberate choice by Docker to maximize interoperability and ease of integration, allowing developers to focus on application logic rather than wrestling with new API paradigms.\n\n## **Connecting Your Applications: A Multi-Pronged Approach**\n\nDocker Model Runner offers several ways for your applications and tools to connect to the local inference engine, providing flexibility for different development setups:\n\n1. **Internal DNS for Containerized Applications (model-runner.docker.internal):**  \n   * **How it works:** For applications running as Docker containers themselves (e.g., a backend API service), Model Runner provides a stable internal DNS name: http://model-runner.docker.internal.  \n   * **Benefit for Engineers:** This is incredibly convenient. Your containerized service can simply target this DNS name to reach the Model Runner API, without needing to know the host's IP address or worry about dynamic port mappings. It simplifies network configuration within your Docker environment.  \n   * **Endpoint Example:** http://model-runner.docker.internal/engines/v1/chat/completions  \n2. **Host TCP Port for Direct Access:**  \n   * **How it works:** You can configure Model Runner to listen on a specific TCP port on your host machine. This is typically done via a Docker Desktop setting or a command like docker desktop enable model-runner \\--tcp \\<port\\> (e.g., port 12434).  \n   * **Benefit for Engineers:** This allows applications running directly on your host (outside of Docker containers)—such as IDEs, local scripts, or standalone Java applications using Spring AI—to connect to the Model Runner.  \n   * **Endpoint Example:** http://localhost:12434/engines/v1/chat/completions  \n3. **Docker Socket (Advanced/CLI Use):**  \n   * **How it works:** For direct interactions via the Docker API or for certain CLI scripting scenarios, the Docker socket (/var/run/docker.sock on Linux/macOS) can be used. API calls through the socket might have a specific path prefix (e.g., `/exp/vDD4.40/...` as seen in early versions).  \n   * **Benefit for Engineers:** This offers a lower-level interface, useful for automation scripts or tools that integrate deeply with the Docker daemon.\n\nThis multi-faceted approach to connectivity ensures that whether your application is containerized, running natively on the host, or interacting via CLI tools, there's a clear and supported path to communicate with the local AI models managed by Docker Model Runner.  \n\nUnderstanding these API mechanics and connection options is crucial for effectively integrating Docker Model Runner into your development workflows. It allows you to choose the most appropriate method for your specific application architecture and leverage the power of local AI models with ease.  \nIn our next post, we'll explore how Docker Model Runner integrates with Docker Compose, enabling the orchestration of complex, multi-service AI applications locally.  \n\n*This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.*\n\n</BlogWrapper>","frontmatter":{"title":"API Architecture, OpenAI Compatibility, and Connection Strategies","subtitle":"Docker Model Runner - A Technical Primer for Engineers","date":"April 9th, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/7ad71d9ddffb4f0135947f5b528e0c93/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqElEQVR42hXCW09SAQAA4PMvqrWVEwM9hzsIKIqc4CAcBJE7HORyuB9QkCb3CZh4D/BCDlbW1pjpbGtqoqXmrOWq1Xyg3nvspYceeq6+fYCYGDFmIoNjCUNq1pwrVw/eN1s/yieX86/O04cXheN3+ZfHmcbuvdWaNz7lTOSckbjZGx6dSBbrT4EnzRdvvl4mV5Z3zk7k/hCIYnQV1mvxK1M57WTeUVjC5lesq4+JWj3T2MkdXkyUaxgRe3Z51fr9BzhtfZurb0am5jBfDFZZw4tZb2qSZyOYdo86EM9GfaJACF2s8tLZ+N7Rwy/fI9VNe6pQOPmwfPEJiFZKbbAYUqJkmfQOG86sF5OlYpfeTkb0I8pgPRVTJhLaszN4f99Yf1T79TO+vWPPz0y3rrKfPwLOdOEWU06DjXSZFhIpYNwE4zaG1UVSWKyI+W0htFqvuI/2+itlKbHkLe3icxUsPx05fx05PQLyG5XrHAZZLAWRoY5upB9V8RA10+qjaIwDXCQ56YnP5Aa0AZba2WsieIbxbl0Um037m9u27QawcdzMlx+gRkMXj9vO4kNqeeegvM/uGcD99B45iSEdj8U1DoKKantcmGDUwdEFRe4xz1ZJu1kF+ou58eeNhYP99MONwtq6LUBIRzRsRCq0Y3cJr8DmGo6F5JiFIkF5Nh1bP0wRGKgwFt2aVawtAJxoFAoGaeGwIJlAZ+47VyqhUkXtDtD5IjpbKNSoekZtzGGT0GVmaYZI3YMkjpLM11GH8Z50AmDgfhYe+Jfh9EGjbrLV2eXwMn0hph0HUTXU10fpFnfyZRKr+doNys1O8W26pI0ha+eoaFobABnw/404aHDRTG662UM1uiG9CzJ5IKsXtHhAnR1UaEARQqGzO6h8MlcKcsVUvoQrhP8CrjoFDpQfwNgAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/7ad71d9ddffb4f0135947f5b528e0c93/366fe/hero-image.jpg","srcSet":"/static/7ad71d9ddffb4f0135947f5b528e0c93/9b503/hero-image.jpg 750w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/321ef/hero-image.jpg 1080w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/84dcd/hero-image.jpg 1366w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ad71d9ddffb4f0135947f5b528e0c93/5f850/hero-image.webp 750w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/2c010/hero-image.webp 1080w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/5126b/hero-image.webp 1366w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/7ad71d9ddffb4f0135947f5b528e0c93/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqElEQVR42hXCW09SAQAA4PMvqrWVEwM9hzsIKIqc4CAcBJE7HORyuB9QkCb3CZh4D/BCDlbW1pjpbGtqoqXmrOWq1Xyg3nvspYceeq6+fYCYGDFmIoNjCUNq1pwrVw/eN1s/yieX86/O04cXheN3+ZfHmcbuvdWaNz7lTOSckbjZGx6dSBbrT4EnzRdvvl4mV5Z3zk7k/hCIYnQV1mvxK1M57WTeUVjC5lesq4+JWj3T2MkdXkyUaxgRe3Z51fr9BzhtfZurb0am5jBfDFZZw4tZb2qSZyOYdo86EM9GfaJACF2s8tLZ+N7Rwy/fI9VNe6pQOPmwfPEJiFZKbbAYUqJkmfQOG86sF5OlYpfeTkb0I8pgPRVTJhLaszN4f99Yf1T79TO+vWPPz0y3rrKfPwLOdOEWU06DjXSZFhIpYNwE4zaG1UVSWKyI+W0htFqvuI/2+itlKbHkLe3icxUsPx05fx05PQLyG5XrHAZZLAWRoY5upB9V8RA10+qjaIwDXCQ56YnP5Aa0AZba2WsieIbxbl0Um037m9u27QawcdzMlx+gRkMXj9vO4kNqeeegvM/uGcD99B45iSEdj8U1DoKKantcmGDUwdEFRe4xz1ZJu1kF+ou58eeNhYP99MONwtq6LUBIRzRsRCq0Y3cJr8DmGo6F5JiFIkF5Nh1bP0wRGKgwFt2aVawtAJxoFAoGaeGwIJlAZ+47VyqhUkXtDtD5IjpbKNSoekZtzGGT0GVmaYZI3YMkjpLM11GH8Z50AmDgfhYe+Jfh9EGjbrLV2eXwMn0hph0HUTXU10fpFnfyZRKr+doNys1O8W26pI0ha+eoaFobABnw/404aHDRTG662UM1uiG9CzJ5IKsXtHhAnR1UaEARQqGzO6h8MlcKcsVUvoQrhP8CrjoFDpQfwNgAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/7ad71d9ddffb4f0135947f5b528e0c93/22857/hero-image.jpg","srcSet":"/static/7ad71d9ddffb4f0135947f5b528e0c93/9976b/hero-image.jpg 125w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/e19ca/hero-image.jpg 250w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/22857/hero-image.jpg 500w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/7ad71d9ddffb4f0135947f5b528e0c93/842b0/hero-image.webp 125w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/b597c/hero-image.webp 250w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/49c7e/hero-image.webp 500w,\n/static/7ad71d9ddffb4f0135947f5b528e0c93/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/api-architecture-openai-compatibility-and-connection-strategies"}},{"id":"3178e285-15ec-5ce8-9bc1-fd0a4eb61036","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\nIn our [previous post](https://layer5.io/blog/docker/docker-model-runner), we introduced Docker Model Runner as a promising new toolkit for simplifying local AI development. Now, let's delve into one of its foundational—and perhaps most strategically significant—aspects: its deep reliance on the Open Container Initiative (OCI) standard for managing AI models.\n\nIf you've wrestled with AI models, you know the \"messy landscape\" of model distribution. Models often arrive as loose files, tucked behind proprietary download tools, or lacking clear versioning. This fragmentation makes standardization, reproducibility, and integration into automated workflows a real headache for engineers. Docker Model Runner aims to bring order to this chaos by treating AI models as OCI artifacts, and this decision has profound implications for how you, as an engineer, can manage the entire lifecycle of your AI models.\n\n## **OCI: More Than Just docker model pull**\n\nYou might see docker model pull `ai/llama3.2:1B-Q8_0` and think it's just a convenient way to download models. But packaging models as OCI artifacts is a strategic move by Docker that goes far deeper. It aligns AI model management with the mature, robust ecosystem already built around OCI for container images.  \n\nEssentially, Docker is working to make AI models **first-class citizens within the Docker ecosystem**. This means the same trusted registries and workflows you use for your application containers can now, in principle, be applied to your AI models. Imagine the possibilities:\n\n* **Unified Workflows:** Manage, version, and distribute your AI models using the same tools and processes you already use for your containerized applications. No more separate, bespoke systems for model management.  \n* **Leveraging Existing Infrastructure:** Your existing private container registries (like Docker Hub, Artifactory, Harbor, etc.) can become repositories for your AI models. This allows you to apply the same security scanning, access control policies, and auditing mechanisms you trust for your containers directly to your AI assets.\n\n## **Engineering Benefits: What OCI Brings to Your AI Model Lifecycle**\n\nAdopting OCI for models isn't just about tidiness; it brings tangible engineering benefits:\n\n1. **Robust Versioning & Provenance:**  \n   * **How you benefit:** OCI's tagging system (e.g., :1B-Q8\\_0, :latest, :v2.1-finetuned) provides robust version control for your models. This is critical for reproducibility in experiments and ensuring stability in deployments. You can track exactly which model version was used for a particular result or release.  \n   * **Immutability:** Like container images, OCI artifacts can be treated as immutable. Once a version is tagged and pushed, it remains consistent, preventing accidental modifications and ensuring that when you pull my-model:v1.0, you always get the same bits.  \n2. **Streamlined CI/CD for ML Models:**  \n   * **How you benefit:** This is a big one. Your existing CI/CD pipelines, likely already geared to handle OCI artifacts for application builds and deployments, can be extended to manage your AI models.  \n   * **Think about it:**  \n     * Automated testing and validation of new model versions.  \n     * Triggering model deployments based on updates in your model training repositories.  \n     * Integrating model security scanning into your pipeline.  \n   * This moves you closer to comprehensive MLOps automation by leveraging familiar tools and processes.  \n3. **Enhanced Governance and Security:**  \n   * **How you benefit:** By storing models in your existing OCI-compliant registries, you can apply consistent governance. Use the same tools for vulnerability scanning on your models as you do for your container images. Enforce role-based access control (RBAC) to determine who can pull or push specific models or versions.\n\n## **The Future is Custom: Pushing Your Own Models**\n\nWhile Docker Model Runner currently provides access to curated models from Docker Hub (often under the ai/ namespace or from partners like Hugging Face via hf.co/), the real power of OCI will unlock when you can easily manage your *own* custom models.  \n\nThe inclusion of commands like docker model push and docker model tag in the CLI strongly signals this future direction. Imagine:\n\n* Training or fine-tuning a model for your specific needs.  \n* Packaging it as an OCI artifact.  \n* Pushing it to your private or public OCI registry.  \n* Seamlessly pulling and running it with docker model pull your-namespace/your-custom-model:v1 and docker model run ....\n\nThis capability will be transformative, allowing you to integrate bespoke AI directly into your standardized Docker workflows, free from vendor lock-in for model storage and distribution.\n\n## **A More Cohesive AI Development World**\n\nBy embracing OCI, Docker Model Runner isn't just offering a new command; it's paving the way for a more unified and manageable AI development landscape. As an engineer, this means you can apply familiar, battle-tested DevOps principles and tools to your AI models, reducing complexity and accelerating your path from experimentation to production. This strategic choice for an open standard also offers a degree of future-proofing. As the AI ecosystem evolves, models packaged as OCI artifacts will likely be manageable by an ever-expanding array of tools and platforms that support this widely adopted standard.  \n\nIn our next post, we'll shift gears and look under the hood at Docker Model Runner's performance architecture, particularly its use of host-native execution and GPU acceleration.  \n\n*This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.*\n</BlogWrapper>","frontmatter":{"title":"Taming the Wild West of AI Model Management","subtitle":"Docker Model Runner - A Technical Primer for Engineers","date":"April 2nd, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/ad894700277bdd8c181ccbffc7dc4e16/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqklEQVR42gGfAmD9AEx0mkdpi0VlhUhqi0xvkk5zllB3m1F2mUZifVh/olqDp1uBo1d7m1R2k1FwjE5rhUtmfUdfc0NXaj5PYABHZX9Na4VPbolTdZFRcYtWeJNYeJFIX28xP0pLXm5bfI5Ve4pVdYVbcYNheIlmfo9qgpJzi5t/mqqForMAf6/DVnmRXoCXdp+1hK3AiK/Bd5aqV6awL4CDWIuZd6S0etvchcrNirzAirC2j6GqjJ+mkaOpnK+0obO3AJK4vllwf3OLk3SNlkJYblRvhDVBWRhfbBmEhiRGSUVLV3+cpJm7t5TXzqjKw7S8t7fBubvAt7/AtMTBswC30sZ/mp1sgoxxgogPHToaKUUeKT4dWWYacXVccWtjWViYlpadrayNu7W7w7HVxa7SxKzayK3hzq7n1bAArK+cn66fQlVeJUdbJ0JXGCw+GS9BF2FrFnF2WXV2ZXh2b6miYGZwW11hm5SI38mp992y79uv9ee5+vHIANLIqezQqKiSfik/VTRRXB9AURo4SRwwSCM5USAvRyU+VR0+SCwyPTNJZERggGV4iL2ymPbgs/zuwP763ACKmZ+guLZ8jI4hMkgtNkIkNUMqO08aJj8dMEkqSGEyUGswPFQjMEIjNFI6XnMlOEklMUKYjYnQtaDq17AAUG6QUnKSZYyiY4yUQ2NsM0hfSV96TWaDMURhGytFHzRPLUNhMUhmIzVQGjFBJy89JjE+Nj1LmYmP1cGiAEBcg0Bcgz1VfkppjWqVqXWisGKKokdkhDRFXS08Uy5BYR4uRx0sRSU2Tyw8WSs2Sy02R09UaIV8iM27nAAuRnEvRnAxSHItQ28sQm9BX4F1prN5qKYkMD4vRGs4UHozSnAwRGkzR2o9UXZCVXpGWHpRX39san7AsJUIaSe4l4uqbwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/ad894700277bdd8c181ccbffc7dc4e16/366fe/hero-image.jpg","srcSet":"/static/ad894700277bdd8c181ccbffc7dc4e16/9b503/hero-image.jpg 750w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/321ef/hero-image.jpg 1080w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/84dcd/hero-image.jpg 1366w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/ad894700277bdd8c181ccbffc7dc4e16/5f850/hero-image.webp 750w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/2c010/hero-image.webp 1080w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/5126b/hero-image.webp 1366w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/ad894700277bdd8c181ccbffc7dc4e16/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqklEQVR42gGfAmD9AEx0mkdpi0VlhUhqi0xvkk5zllB3m1F2mUZifVh/olqDp1uBo1d7m1R2k1FwjE5rhUtmfUdfc0NXaj5PYABHZX9Na4VPbolTdZFRcYtWeJNYeJFIX28xP0pLXm5bfI5Ve4pVdYVbcYNheIlmfo9qgpJzi5t/mqqForMAf6/DVnmRXoCXdp+1hK3AiK/Bd5aqV6awL4CDWIuZd6S0etvchcrNirzAirC2j6GqjJ+mkaOpnK+0obO3AJK4vllwf3OLk3SNlkJYblRvhDVBWRhfbBmEhiRGSUVLV3+cpJm7t5TXzqjKw7S8t7fBubvAt7/AtMTBswC30sZ/mp1sgoxxgogPHToaKUUeKT4dWWYacXVccWtjWViYlpadrayNu7W7w7HVxa7SxKzayK3hzq7n1bAArK+cn66fQlVeJUdbJ0JXGCw+GS9BF2FrFnF2WXV2ZXh2b6miYGZwW11hm5SI38mp992y79uv9ee5+vHIANLIqezQqKiSfik/VTRRXB9AURo4SRwwSCM5USAvRyU+VR0+SCwyPTNJZERggGV4iL2ymPbgs/zuwP763ACKmZ+guLZ8jI4hMkgtNkIkNUMqO08aJj8dMEkqSGEyUGswPFQjMEIjNFI6XnMlOEklMUKYjYnQtaDq17AAUG6QUnKSZYyiY4yUQ2NsM0hfSV96TWaDMURhGytFHzRPLUNhMUhmIzVQGjFBJy89JjE+Nj1LmYmP1cGiAEBcg0Bcgz1VfkppjWqVqXWisGKKokdkhDRFXS08Uy5BYR4uRx0sRSU2Tyw8WSs2Sy02R09UaIV8iM27nAAuRnEvRnAxSHItQ28sQm9BX4F1prN5qKYkMD4vRGs4UHozSnAwRGkzR2o9UXZCVXpGWHpRX39san7AsJUIaSe4l4uqbwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/ad894700277bdd8c181ccbffc7dc4e16/22857/hero-image.jpg","srcSet":"/static/ad894700277bdd8c181ccbffc7dc4e16/9976b/hero-image.jpg 125w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/e19ca/hero-image.jpg 250w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/22857/hero-image.jpg 500w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/ad894700277bdd8c181ccbffc7dc4e16/842b0/hero-image.webp 125w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/b597c/hero-image.webp 250w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/49c7e/hero-image.webp 500w,\n/static/ad894700277bdd8c181ccbffc7dc4e16/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/taming-the-wild-west-of-ai-model-management"}},{"id":"77e04089-65ad-5a89-976c-c56185e19307","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nThe shift towards local-first AI development is undeniable, driven by engineers seeking to overcome the practical hurdles of cloud-centric model interaction. Escalating API costs, data privacy concerns when handling sensitive information, network latency impacting iteration speed, and the desire for finer-grained control over execution environments have all highlighted the need for robust local solutions. Docker Model Runner is Docker's response to these engineering challenges, aiming to significantly streamline how we develop and test AI models locally.  \n\nThis post, the first in a series, will dissect Docker Model Runner from an engineering perspective. We'll explore its core technical value propositions and how you can leverage this new toolkit to enhance your AI development workflows.\n\n## **The Engineering Case for Local AI Development**\n\nFor engineers, the \"local-first\" approach to AI isn't just a trend; it's a pragmatic choice offering tangible benefits:\n\n* **Reduced Iteration Costs:** Experimenting with prompts, parameters, and model variations can lead to substantial API expenses. Local execution eliminates these costs during the crucial development and debugging phases.  \n* **Enhanced Data Privacy & Security:** Working with proprietary or sensitive datasets locally mitigates the risks associated with transmitting data to external services, a critical consideration for many enterprise applications.  \n* **Accelerated Development Cycles:** Eliminating network latency allows for near-instantaneous feedback, dramatically speeding up iterative tasks like prompt engineering, parameter tuning, and debugging model behavior.  \n* **Granular Environmental Control:** Local execution provides engineers with complete control over the model's runtime environment, dependencies, and specific configurations, facilitating reproducible experiments and precise debugging.\n\n## **Docker Model Runner: Key Technical Capabilities for Engineers**\n\nDocker Model Runner aims to integrate local AI model execution seamlessly into the familiar Docker ecosystem. Here are some of its core technical aspects beneficial for engineers:\n\n1. Simplified Local Inference Setup:  \n   While the \"Docker\" name might imply traditional containerization for the model itself, Model Runner takes a different architectural path for performance. It facilitates running models like ai/llama3.2:1B-Q8\\_0 or hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF via commands such as docker model pull and docker model run. The key is that the inference itself often runs as a host-native process (initially leveraging llama.cpp), interacting with Docker Desktop or a Model Runner plugin. This design choice, which we'll explore in detail later, prioritizes direct hardware access.  \n2. Performance through Host-Native Execution & GPU Access:  \n   To tackle the performance demands of LLMs, Model Runner enables the inference engine to directly access host resources. For macOS users with Apple Silicon, this means direct Metal API utilization for GPU acceleration. Windows GPU support is also on the roadmap. This approach aims to minimize the overhead often associated with virtualized GPU access in containerized environments, offering a potential speed advantage for local development.  \n3. OpenAPI-Compatible API for Seamless Integration:  \n   One of the most significant engineering benefits is the provision of an OpenAI-compatible API. This allows you to reuse existing codebases, SDKs (like LangChain or LlamaIndex), and tools with minimal, if any, modification. For many, transitioning to a local model might be as simple as changing an API endpoint URL, drastically reducing the integration effort and learning curve.  \n4. Standardized Model Management with OCI Artifacts:  \n   Docker Model Runner treats AI models as Open Container Initiative (OCI) artifacts. This is a strategic move towards standardizing model distribution, versioning, and management, aligning it with the mature ecosystem already in place for container images. This opens the door to leveraging existing container registries and CI/CD pipelines for models, a crucial step towards robust MLOps practices. We'll dedicate our next post to a deep dive into this OCI integration.\n\n## **Beyond Single Invocations: The Potential for Local AI Pipelines**\n\nWhile running individual models is a core function, the architecture of Docker Model Runner also supports the local orchestration of more complex, multi-stage AI workflows. As detailed in examples like the Gemma 3 Comment Processing System, engineers can design and debug entire pipelines—involving synthetic data generation, categorization, embedding generation, feature extraction, and response generation—all on their local machines. This capability for end-to-end local development of AI-driven features is invaluable.\n\n## **Engineering the Future of Local AI**\n\nDocker Model Runner, even in its Beta phase (introduced with Docker Desktop 4.40, with APIs still evolving), presents a compelling toolkit for engineers looking to overcome the traditional challenges of local AI development. It offers a pathway to faster iteration, greater control, enhanced privacy, and reduced costs.  \nIn our next post, we will delve into the technical specifics of how Docker Model Runner's use of **OCI artifacts is set to revolutionize AI model management**, bringing DevOps principles to your MLOps workflows.  \n*This blog post is based on information about Docker Model Runner, a Beta feature. Features, commands, and APIs are subject to change.*\n\n</BlogWrapper>","frontmatter":{"title":"Docker Model Runner","subtitle":"A Technical Primer for Engineers","date":"March 27th, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/dfd3200061f7dd06b9c2ae3283e13437/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACkklEQVR42h3I+U+SAQDG8fefqX6stozlLN1Qcy2MEC8QlcO4FDQQEUVADgF5CYUXUQE1jjIQSo4RqNhSJ+bUXmkqvByKd/0Rgdtn3z17gMXC6MeUxJ2VudKyhWOJMy11pko+5WW+G9XSlcpzqSz6clGq91q1dD1WPIvDe6MCZv4oR7x9o8t8eUAgXuyT+nnyYL/sK08VFkwn5XN59edbcPGv3vPP4L7UOc7GHQXQWdA7iz0HgRlk0gSDk/s64wEIwaBxv1g9dDBu2AMVIbnQOCiEZB1afvuMQL3zwXZqtmWMtixkzZqsORMAnVjNiM2SsU8hdnPabsnMWbJ2S9YtjVsofqhaI68kt1axSfewmKedzRT7iPXaZUrbTMezs8gCoEs6dEkneEd36CpWveci6SUP0NXPzTq0AypndGj1YqJQcL8GXcWlD32d9cDR4NWG5cQLKPb9Rco7ij3f2FGQ4p56hG+p7aShJKIXfB6WzapchpgiybNKDJreixvUGuxu1/kv63EcGE5ESrZLJDvf+yK+Ro3hDZ5ehyE/VioeNhIHxRq13Upk95a3Upuk2joy551gNJbLrd1eAPyf60W8H2v9m+vccJDAFpGkYJsMbBWqsCJF/YCUOWEjyw2Y90Mv6dwyXMvrbj7dMC2Oxwc21wFObKckmuiN73JWtrDz8/Um49seYcOwqkEowzEHaghdZfiWJ9jminYmTmXqi27ztuCe1QRnJQHQAzAzBHOiSf5Gaujogp+/JARjVVQuCtOEwjSjavEVBNYrvqZtapkV+N29mmKEYEbggBU+ZIZhgOpDaD6EGchwYvme1TNGJNf1DSEt7DZNRBrBING6RfEc0UOFruApzY9QvCdUX5rmL6H6kP851nx/MSExEQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/dfd3200061f7dd06b9c2ae3283e13437/366fe/hero-image.jpg","srcSet":"/static/dfd3200061f7dd06b9c2ae3283e13437/9b503/hero-image.jpg 750w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/321ef/hero-image.jpg 1080w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/84dcd/hero-image.jpg 1366w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/366fe/hero-image.jpg 1408w","sizes":"100vw"},"sources":[{"srcSet":"/static/dfd3200061f7dd06b9c2ae3283e13437/5f850/hero-image.webp 750w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/2c010/hero-image.webp 1080w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/5126b/hero-image.webp 1366w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/83fb1/hero-image.webp 1408w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5454545454545455}}},"darkthumbnail":{"extension":"png","publicURL":"/static/dfd3200061f7dd06b9c2ae3283e13437/hero-image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACkklEQVR42h3I+U+SAQDG8fefqX6stozlLN1Qcy2MEC8QlcO4FDQQEUVADgF5CYUXUQE1jjIQSo4RqNhSJ+bUXmkqvByKd/0Rgdtn3z17gMXC6MeUxJ2VudKyhWOJMy11pko+5WW+G9XSlcpzqSz6clGq91q1dD1WPIvDe6MCZv4oR7x9o8t8eUAgXuyT+nnyYL/sK08VFkwn5XN59edbcPGv3vPP4L7UOc7GHQXQWdA7iz0HgRlk0gSDk/s64wEIwaBxv1g9dDBu2AMVIbnQOCiEZB1afvuMQL3zwXZqtmWMtixkzZqsORMAnVjNiM2SsU8hdnPabsnMWbJ2S9YtjVsofqhaI68kt1axSfewmKedzRT7iPXaZUrbTMezs8gCoEs6dEkneEd36CpWveci6SUP0NXPzTq0AypndGj1YqJQcL8GXcWlD32d9cDR4NWG5cQLKPb9Rco7ij3f2FGQ4p56hG+p7aShJKIXfB6WzapchpgiybNKDJreixvUGuxu1/kv63EcGE5ESrZLJDvf+yK+Ro3hDZ5ehyE/VioeNhIHxRq13Upk95a3Upuk2joy551gNJbLrd1eAPyf60W8H2v9m+vccJDAFpGkYJsMbBWqsCJF/YCUOWEjyw2Y90Mv6dwyXMvrbj7dMC2Oxwc21wFObKckmuiN73JWtrDz8/Um49seYcOwqkEowzEHaghdZfiWJ9jminYmTmXqi27ztuCe1QRnJQHQAzAzBHOiSf5Gaujogp+/JARjVVQuCtOEwjSjavEVBNYrvqZtapkV+N29mmKEYEbggBU+ZIZhgOpDaD6EGchwYvme1TNGJNf1DSEt7DZNRBrBING6RfEc0UOFruApzY9QvCdUX5rmL6H6kP851nx/MSExEQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/dfd3200061f7dd06b9c2ae3283e13437/22857/hero-image.jpg","srcSet":"/static/dfd3200061f7dd06b9c2ae3283e13437/9976b/hero-image.jpg 125w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/e19ca/hero-image.jpg 250w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/22857/hero-image.jpg 500w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/3d93d/hero-image.jpg 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/dfd3200061f7dd06b9c2ae3283e13437/842b0/hero-image.webp 125w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/b597c/hero-image.webp 250w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/49c7e/hero-image.webp 500w,\n/static/dfd3200061f7dd06b9c2ae3283e13437/f6732/hero-image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":273}}}},"fields":{"slug":"/blog/docker/docker-model-runner"}},{"id":"89dcc3da-3c78-58ea-bd12-4386b28da209","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\nimport { BlockquoteAlt } from \"../../../../reusecore/Blockquote/Blockquote-alt-style\";\n\n<BlogWrapper>\nWhile building a Docker image for one of <Link to=\"/projects\">Layer5's open source projects</Link>, I ran into a build warning in one of the multi-stage Dockerfiles. Up to this point, while creating container images, I hadn't paid attention to a particular feature of `docker build`: <i><a href=\"https://docs.docker.com/reference/build-checks/\">build checks</a></i>. I had otherwise overlooked warnings and informational notices that were lost among the fray of docker build log lines. I've stopped ignoring them, however. In this post, I'll touch upon a handful of the more common build warnings, delve into the importance of these checks, and explore how and why to address each. But, first...\n\n<BlockquoteAlt\n  style={{ width: \"100%\" }} person=\"Lee Calcote\" quote=\"Build checks are your Dockerfile linter.\" />\n\n###  Why Use Docker Build Checks?\n\nNow, of course we're aware that `docker build` is a bread and butter command (very commonplace), however, if you're like me, you might have overlooked the `--check` flag. Think of build checks as a linter for your Dockerfile, and as it turns out, build checks are a valuable tool for validating your Dockerfile and build options (flags) against established best practices. Build checks help you identify potential issues early on, too, which I've come to appreciate. In fact, whereas am occassionally annoyed by overzealous linting rules, in general, I've turned the corner with lint checks as when you're maintaining large codebases with a couple thousand contributors, it becomes essential to enforce some uniformity and idiomatic coding. \n\nThey analyze your build configuration and provide feedback on potential problems, such as:\n\n* **Inefficiencies:**  Checks can identify unnecessary layers or overly large images, helping you optimize for size and performance.\n* **Security risks:**  They can flag potential security vulnerabilities, like using outdated base images or insecure commands.\n* **Maintainability issues:** Checks can highlight inconsistencies and deviations from best practices, improving the readability and maintainability of your Dockerfiles.\n\n### Under the Hood of Build Checks\n\nIntroduced in Dockerfile 1.8, build checks are integrated into the build process itself. When you invoke a build with the `--check` flag, Docker analyzes your Dockerfile and build options *before* executing any build steps. This pre-emptive analysis allows for early detection of potential issues, saving you time and resources.\n\nDocker employs a rule-based system to perform these checks. Each rule targets a specific aspect of your Dockerfile, such as image size, security practices, or Dockerfile syntax. These rules are continuously updated to reflect evolving best practices and address new vulnerabilities.\n\nAs you familiarize yourself with build checks, you'll notice that they are categorized into three levels, which are pretty self-evident, but worth noting, so that you can prioritize your remediation efforts. These levels are:\n\n* **Errors:**  These are critical issues that prevent the build from completing successfully. Errors must be resolved before you can proceed with the build.\n* **Warnings:**  Warnings indicate potential problems that don't halt the build but should be addressed to ensure best practices.\n* **Info:**  Informational messages provide additional context or suggestions for improving your Dockerfile. While not critical, acting on these messages can enhance your Dockerfile's quality.\n\n## Common Warnings and Remediation\n\nBefore we dive into some common warnings and how to address them, let's review how to run build checks on a Dockerfile.\n#### Running Build Checks\n\nTo run build checks, simply use the `--check` flag with your `docker build` command:\n\n```bash\ndocker build --check .\n```\n\nYes, quite straightforward. This command triggers build checks for the Dockerfile in the current directory. If any issues are detected, Docker will display the corresponding warnings or errors.\n\nNow, we're ready to explore some frequently encountered warnings and how to address them. Here are some of the warnings that I've run into, what they mean, and how to fix them:\n\n### 1. FromAsCasing\n\n\n- **Warning:** `WARN: FromAsCasing: 'as' and 'FROM' keywords' casing do not match`\n- **Explanation:** This warning occurs when the `FROM` and `AS` keywords in a multi-stage build have different casing (e.g., `FROM` and `as`). While Docker allows both uppercase and lowercase, mixing casing can hinder readability. Consistency in casing enhances readability and maintainability, making your Dockerfile easier to understand for both yourself and collaborators.\n- **Remediation:**  Ensure consistent casing for `FROM` and `AS` keywords throughout your Dockerfile.\n\n```docker\n# Incorrect\nFROM ubuntu:latest as builder\n\n# Correct\nFROM ubuntu:latest AS builder\n```\n\n### 2. LatestType\n\n* **Warning:** `WARN: LatestType: 'latest' tag used for base image`\n* **Explanation:** Using the `latest` tag can lead to unpredictable builds since the base image may change unexpectedly. Pinning to specific tags ensures reproducible builds, preventing unexpected behavior due to changes in the base image. This is crucial for production environments where stability is paramount.\n* **Remediation:**  Specify a specific tag for your base image to ensure consistency and reproducibility.\n\n```dockerfile\n# Incorrect\nFROM ubuntu:latest\n\n# Correct\nFROM ubuntu:22.04\n```\n\n### 3. AptGetNoInstallRecommends\n\n* **Warning:** `WARN: AptGetNoInstallRecommends: 'apt-get install' with no ' --no-install-recommends' flag`\n* **Explanation:**  Installing recommended packages can increase image size and potentially introduce unnecessary dependencies. Recommended packages often include extra dependencies that bloat your image size. This can lead to increased storage costs, slower downloads, and potentially longer build times.\n* **Remediation:**  Use the `--no-install-recommends` flag with `apt-get install` to avoid installing recommended packages.\n\n```dockerfile\n# Incorrect\nRUN apt-get update && apt-get install -y package-name\n\n# Correct\nRUN apt-get update && apt-get install -y --no-install-recommends package-name\n```\n\n### 4. RunCommandWithNoExecForm\n\n* **Warning:** `WARN: RunCommandWithNoExecForm: 'RUN' command with no 'exec' form`\n* **Explanation:** Using the exec form (`RUN [\"executable\", \"param1\", \"param2\"]`) for RUN commands is generally more efficient and avoids potential issues with shell interpretation. The exec form provides better performance and avoids potential shell injection vulnerabilities by directly executing the command without shell processing.\n* **Remediation:** Use the exec form for `RUN` commands whenever possible.\n\n```dockerfile\n# Incorrect\nRUN apt-get update && apt-get install -y package-name\n\n# Correct\nRUN [\"apt-get\", \"update\"]\nRUN [\"apt-get\", \"install\", \"-y\", \"--no-install-recommends\", \"package-name\"]\n```\n\n### 5. RedunantAptUpdate\n\n* **Warning:** `WARN: RedundantAptUpdate: 'apt-get update' found in multiple RUN instructions`\n* **Explanation:** Running `apt-get update` multiple times within a Dockerfile is inefficient. Each `apt-get update` fetches package information from repositories. Repeating this unnecessarily consumes bandwidth and increases build time.\n* **Remediation:**  Combine `apt-get update` with the subsequent `apt-get install` command in a single `RUN` instruction.\n\n```dockerfile\n# Incorrect\nRUN apt-get update\nRUN apt-get install -y package-name\n\n# Correct\nRUN apt-get update && apt-get install -y package-name\n```\n\nThese are just the checks that I ran into. See the full list of available checks by running `docker build --help` or visiting the [official documentation](https://docs.docker.com/reference/build-checks/).\n\n## Beyond the Build Check Basics\n\nNow, that you've gotten started with Dockerfile optimatization, you can further customize build checks to suit your needs. Know that you're not limited to just enabling or disabling all checks, but that Docker offers granular control, allowing you to:\n\n* **Skip specific checks:**  If a particular check isn't relevant to your use case, you can skip it using the `--check=rule1,rule2,!rule3` syntax. This enables `rule1` and `rule2` while skipping `rule3`.\n* **Fail on warnings:** By default, warnings don't halt the build process. However, you can enforce stricter compliance by using the `--fail-on=warn` flag, ensuring any warning triggers a build failure.\n* **Explore experimental checks:**  Push the boundaries with experimental checks by using the `--check=experimental` flag. These checks offer a glimpse into future best practices and can help you stay ahead of the curve.\n\nBuild checks are not just about fixing warnings; they are about proactively adopting best practices. Up your Dockerfile game with these additional tips:\n\n* **Embrace multi-stage builds:**  Leverage multi-stage builds to create smaller, more efficient images by separating build-time dependencies from runtime essentials.\n* **Utilize a `.dockerignore` file:** Exclude unnecessary files from your build context to reduce image size and speed up builds.\n* **Order your instructions strategically:** Place frequently changing instructions towards the end of your Dockerfile to take advantage of Docker's layer caching.\n\nBy understanding the mechanics of build checks and actively addressing the warnings, you can unlock the full potential of Docker and build robust, efficient, and secure containerized applications.\n</BlogWrapper>","frontmatter":{"title":"Docker Build Check Failures","subtitle":"Linting Dockerfiles to ensure best practices and avoid image build failures","date":"October 9th, 2024","author":"Lee Calcote","thumbnail":{"extension":"webp","publicURL":"/static/2a7dde65e579f386ecbed3f2fc4606ac/docker-extension-meshery-logo.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRiwCAABXRUJQVlA4WAoAAAAQAAAAEwAAFQAAQUxQSAMBAAABgGvbtrHn/LHt5Alc2a4yUtl2OrW2bVe2+dtWFTuljd/f9woRMQGg1K1bvB0KKM152EYezgtFGnb4CDfchSKFtzLGW+mOqSwxReDMX5jhLjxuDXK2bFVB0c4Gd4/US8T1NiIIoJ73ZHCJDWxys6oIAb6JdXPYysU296jabqKTm2jXh61sWk40RkEDFrCkPiw5Infp/Vsi6QAAv2Or/lH26pjAqfNNohipZtEkaXZY4c7jL54lyg8OJU+SBdBL50kQvsX8SSQHBS13EsTgXo6VoH0HgApUJACIQlUMAOafKB4MAADqEP3NOYKiJg6JJBuUG86oukoBhAHT57//t0uJWgAAAFZQOCACAQAAEAYAnQEqFAAWAD7RXKZOqCUjIigKqQAaCWwAnTMYL0EzOEYzCUcfGIl3ENai4eFZQu+1MxBrwrAAAP6e2afMPBN+4pGaascUU4u+Q/nINgyrLk2RBNA9vvzhOtWcx2DZE3iPefuoTrIDZcltVH/7R/zVPJLZmhHo6jeqvnraJOwL3nhwxW7mjyuSMBjfIgtbZ8WFXngRTltsvvsMDS5JA9On6QWnBP68Li0bPT0OAJxnEmf3+/qYxHOP+O9KSAEJU2QUNkHP7rG0w9LJx+xRksEQryQ4/yFpjym1b6KAux8wiHfr/5hPUR1pCZgUMmu3K3M4BHbdGETzGS3gItK+SwAA"},"images":{"fallback":{"src":"/static/2a7dde65e579f386ecbed3f2fc4606ac/ad7cd/docker-extension-meshery-logo.webp","srcSet":"/static/2a7dde65e579f386ecbed3f2fc4606ac/ad7cd/docker-extension-meshery-logo.webp 363w","sizes":"100vw"},"sources":[]},"width":1,"height":1.0853994490358125}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/2a7dde65e579f386ecbed3f2fc4606ac/docker-extension-meshery-logo.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRiwCAABXRUJQVlA4WAoAAAAQAAAAEwAAFQAAQUxQSAMBAAABgGvbtrHn/LHt5Alc2a4yUtl2OrW2bVe2+dtWFTuljd/f9woRMQGg1K1bvB0KKM152EYezgtFGnb4CDfchSKFtzLGW+mOqSwxReDMX5jhLjxuDXK2bFVB0c4Gd4/US8T1NiIIoJ73ZHCJDWxys6oIAb6JdXPYysU296jabqKTm2jXh61sWk40RkEDFrCkPiw5Infp/Vsi6QAAv2Or/lH26pjAqfNNohipZtEkaXZY4c7jL54lyg8OJU+SBdBL50kQvsX8SSQHBS13EsTgXo6VoH0HgApUJACIQlUMAOafKB4MAADqEP3NOYKiJg6JJBuUG86oukoBhAHT57//t0uJWgAAAFZQOCACAQAAEAYAnQEqFAAWAD7RXKZOqCUjIigKqQAaCWwAnTMYL0EzOEYzCUcfGIl3ENai4eFZQu+1MxBrwrAAAP6e2afMPBN+4pGaascUU4u+Q/nINgyrLk2RBNA9vvzhOtWcx2DZE3iPefuoTrIDZcltVH/7R/zVPJLZmhHo6jeqvnraJOwL3nhwxW7mjyuSMBjfIgtbZ8WFXngRTltsvvsMDS5JA9On6QWnBP68Li0bPT0OAJxnEmf3+/qYxHOP+O9KSAEJU2QUNkHP7rG0w9LJx+xRksEQryQ4/yFpjym1b6KAux8wiHfr/5hPUR1pCZgUMmu3K3M4BHbdGETzGS3gItK+SwAA"},"images":{"fallback":{"src":"/static/2a7dde65e579f386ecbed3f2fc4606ac/ad7cd/docker-extension-meshery-logo.webp","srcSet":"/static/2a7dde65e579f386ecbed3f2fc4606ac/2884b/docker-extension-meshery-logo.webp 91w,\n/static/2a7dde65e579f386ecbed3f2fc4606ac/686c1/docker-extension-meshery-logo.webp 182w,\n/static/2a7dde65e579f386ecbed3f2fc4606ac/ad7cd/docker-extension-meshery-logo.webp 363w","sizes":"(min-width: 363px) 363px, 100vw"},"sources":[]},"width":500,"height":542.6997245179064}}}},"fields":{"slug":"/blog/docker/docker-build-check-failures"}},{"id":"c7d6ad99-b105-58c2-b6b9-3878b0d3e58a","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\nimport Button from \"../../../../reusecore/Button\";\nimport DockerLogo from \"../../../../assets/images/partners/docker.svg\";\nimport dockercompose_logo from \"../../../../assets/images/dockercompose-logo/dockercompose-logo.webp\";\n\n<BlogWrapper>\n<div className=\"intro\">\nExperience the power and ease of the Docker Meshery Extension. Download the extension from <a href=\"https://hub.docker.com/extensions/meshery/docker-extension-meshery\">Docker Hub</a> and start streamlining your multi-container application development and management today.\n</div>\n\nStreamline your Docker Compose workflow and gain powerful visualization and lifecycle management for your multi-container applications with the Docker Meshery Extension. This extension seamlessly integrates Meshery, a service management platform, with Docker, allowing you to effortlessly import, deploy, visualize, and manage your Docker Compose applications.\n<img src={dockercompose_logo} className=\"image-right-no-shadow\" />\n\n<h2>Effortless Import: Docker Compose as Meshery Design</h2>\n\nGet started quickly by importing your existing Docker Compose applications directly into Meshery. The extension streamlines the process, eliminating the need for manual configuration. Simply navigate to Meshery and leverage the intuitive UI to import your Compose file. Meshery intelligently parses the file, recognizing your application's components and dependencies.\nImport your existing designs and existing infrastructure configurations into Meshery. The platform supports a variety of application definition formats, and you can import designs using either the Meshery CLI or the Meshery UI.\n**Supported Design Definition Formats**\n\nMeshery supports the following design definition formats:\n\n- [Kubernetes Manifests](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/)\n- [Helm Charts](https://helm.sh/docs/topics/charts/)\n- --> **[Docker Compose](https://docs.docker.com/compose/)**\n- [Meshery Designs](https://docs.meshery.io/concepts/logical/designs)\n\n#### Import Docker Compose Apps Using Meshery CLI\n\n<strong>Step 1: Install Meshery CLI</strong>\n\nBefore you can use the Meshery CLI to import a Docker Compose app, you must first install it. You can install Meshery CLI by <a href=\"https://docs.meshery.io/installation#install-mesheryctl\">following the instructions</a>.\n<strong>Step 2: Import the Design Manifest</strong>\n\nOnce you have created your Design Definition file, you can use the Meshery CLI to import your Docker Compose app into Meshery. To do this, run the following command:\n\n```shell\nmesheryctl pattern import -f [file/url] -s [source-type]\n```\n\nThis command enable users to import their existing designs from sources as\n\n- Helm Charts\n- Kubernetes Manifests\n- Docker Compose\n\n**Example :**\n\n```shell\nmesheryctl pattern import -f docker-compose.yaml -s \"Docker Compose\"\n```\n\n<br />\n\n### Seamless Deployment to Kubernetes\n\nWith your application imported, take advantage of Meshery's robust deployment capabilities.  Deploy your multi-container application to a Kubernetes cluster with a single click. Meshery handles the intricacies of the deployment process, ensuring your application runs smoothly within your Kubernetes environment.\n### Visualize Your Application with MeshMap\n\nGain valuable insights into your application's health and performance with MeshMap, Meshery's interactive service graph.  MeshMap provides a clear visual representation of your application's architecture,  allowing you to easily identify dependencies and potential bottlenecks.\n\n### Interactive Management with MeshMap\n\n[MeshMap](/cloud-native-management/meshmap) empowers you to manage your application directly within the visualized service graph.  The web-based interactive terminal integrated with MeshMap grants you the ability to execute commands on individual containers or services within your application. This eliminates the need to switch between different tools or terminals, streamlining troubleshooting and management tasks.\n\n## Seamless Integration with Docker: A Powerful Partnership\n\n<img src={DockerLogo} className=\"image-right-no-shadow\" />\n\nThe Docker Meshery Extension fosters a powerful collaboration between <Link to=\"/company/news/layer5-joins-docker-extension-program-bringing-kubernetes-and-service-mesh-management-to-docker-with-meshery\">Layer5 and Docker</Link>. Leveraging the familiarity and widespread adoption of Docker Compose, the extension empowers you to harness the advanced management and visualization capabilities of Meshery. This translates to a more efficient and streamlined workflow for managing your multi-container applications.\n<strong>Get Started Today</strong>\n\nExperience the power and ease of the Docker Meshery Extension. Download the extension from <a href=\"https://hub.docker.com/extensions/meshery/docker-extension-meshery\">Docker Hub</a> and start streamlining your multi-container application development and management today.\n</BlogWrapper>\n","frontmatter":{"title":"Simplify Multi-container Application Management with the Docker Meshery Extension","subtitle":"Multi-Cluster Kubernetes Deployments of Docker Compose Apps","date":"April 25th, 2024","author":"Layer5 Team","thumbnail":{"extension":"webp","publicURL":"/static/c7a2abd06dcfa5a17ef79309fbcb392e/dockercompose-logo.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRlwCAABXRUJQVlA4WAoAAAAQAAAAEwAAGAAAQUxQSB8BAAABkHPb1rHXwnfslKkyUtpWZTupbNu2bfvat7TV27YdY4fPGxETAGHLnmOnP/pC4lGSfC7l2I5rUqy/cemKrpR3v+yUIaoaUtOU+P2dJkT9PnPXK+oC9ivc85byXhcp2AFAxqG1zXxVZF4VGCd5hcImwBdyrfilkDbgf5V1iBK5iZ23m4Cuvf67O6kDBt+0ALmruy0OL/FzII4eAICUXf6E/SX5Hz+PAdC+u2MpUHWZ5D+ZXxvHysb+8NX8T/7+MGfUNzH/EgmLJO8++sSmVc5zf81jZir13j0fYSozRY6EP9kg5y8FouDvgaU76XHv+XljKOcRk2UAVCx4OP8kuW56n1yskQEA9UNl0Jn5+CgCT0megdSkJfKbJBjdOqO3CwBWUDggFgEAABAGAJ0BKhQAGQA+yVCgS6ekoyG39VgA8BkJQBdhxAA8+vNaNpiGd/xvGRbRKfLxnoCR1oFhIlZFgAD+uQ946a62hehxxXmn4XR8oFOrjHpacO8HQm+Pz0AQfhU6coiAUoRFRdMI5CdwipGmMCZEJ0g01mc8+cXh2z5tggZgLAI6XoOE3RoNC/HsGOSVMUEl9misaiWnDg7lNYVni1O7ZB7n3YmI+A5kJ7bSy8AP7V3OCiuv/OvTUzg7G6TkeErA3QI+lTW40osd50YWocKfqune/v8axD9hy3JAm2vraeK6Wr5nYLVltAt7tAbbhDMnQMH0WG9+tcwM72pzJv4TBMTYR4/88wVZL49YKQJ9znJNccxOKAAA"},"images":{"fallback":{"src":"/static/c7a2abd06dcfa5a17ef79309fbcb392e/f9a1c/dockercompose-logo.webp","srcSet":"/static/c7a2abd06dcfa5a17ef79309fbcb392e/f9a1c/dockercompose-logo.webp 526w","sizes":"100vw"},"sources":[]},"width":1,"height":1.2680608365019013}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/c7a2abd06dcfa5a17ef79309fbcb392e/dockercompose-logo.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRlwCAABXRUJQVlA4WAoAAAAQAAAAEwAAGAAAQUxQSB8BAAABkHPb1rHXwnfslKkyUtpWZTupbNu2bfvat7TV27YdY4fPGxETAGHLnmOnP/pC4lGSfC7l2I5rUqy/cemKrpR3v+yUIaoaUtOU+P2dJkT9PnPXK+oC9ivc85byXhcp2AFAxqG1zXxVZF4VGCd5hcImwBdyrfilkDbgf5V1iBK5iZ23m4Cuvf67O6kDBt+0ALmruy0OL/FzII4eAICUXf6E/SX5Hz+PAdC+u2MpUHWZ5D+ZXxvHysb+8NX8T/7+MGfUNzH/EgmLJO8++sSmVc5zf81jZir13j0fYSozRY6EP9kg5y8FouDvgaU76XHv+XljKOcRk2UAVCx4OP8kuW56n1yskQEA9UNl0Jn5+CgCT0megdSkJfKbJBjdOqO3CwBWUDggFgEAABAGAJ0BKhQAGQA+yVCgS6ekoyG39VgA8BkJQBdhxAA8+vNaNpiGd/xvGRbRKfLxnoCR1oFhIlZFgAD+uQ946a62hehxxXmn4XR8oFOrjHpacO8HQm+Pz0AQfhU6coiAUoRFRdMI5CdwipGmMCZEJ0g01mc8+cXh2z5tggZgLAI6XoOE3RoNC/HsGOSVMUEl9misaiWnDg7lNYVni1O7ZB7n3YmI+A5kJ7bSy8AP7V3OCiuv/OvTUzg7G6TkeErA3QI+lTW40osd50YWocKfqune/v8axD9hy3JAm2vraeK6Wr5nYLVltAt7tAbbhDMnQMH0WG9+tcwM72pzJv4TBMTYR4/88wVZL49YKQJ9znJNccxOKAAA"},"images":{"fallback":{"src":"/static/c7a2abd06dcfa5a17ef79309fbcb392e/413ee/dockercompose-logo.webp","srcSet":"/static/c7a2abd06dcfa5a17ef79309fbcb392e/35320/dockercompose-logo.webp 125w,\n/static/c7a2abd06dcfa5a17ef79309fbcb392e/27cc7/dockercompose-logo.webp 250w,\n/static/c7a2abd06dcfa5a17ef79309fbcb392e/413ee/dockercompose-logo.webp 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":634}}}},"fields":{"slug":"/blog/docker/simplify-multi-container-application-management-with-the-docker-meshery-extension"}},{"id":"f2512afd-5871-5b42-9a4d-d8e7e3ed3a8e","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport Blockquote from \"../../../../reusecore/Blockquote\";\nimport { Link } from \"gatsby\";\nimport DDE from './docker-extension-meshery.webp';\nimport developer from './developers-need.webp';\nimport dashboard from './dashboard.webp';\nimport designer from './designer-1.webp';\nimport visualizer from './viz-1.webp';\n\n<BlogWrapper>\n\n<div className=\"intro \">\n    <Link to=\"/community/members/lee-calcote\">Lee Calcote</Link> and <Link to=\"/community/members/nic-jackson\">Nic Jackson</Link> gave a presentation entitled <i>Extending the Docker Compose Experience to Service Mesh</i> at DockerCon 2022.\n</div>\n\n    The <Link to=\"/docker-extension-meshery\">Meshery Docker Extension</Link> extends Docker Desktop's position as the cloud native developer's go-to Kubernetes environment with easy access to the next\n    layer of cloud native infrastructure.\n    <Link to=\"/community/members/lee-calcote\">Lee Calcote</Link> is an innovative product and technology leader, passionate about empowering engineers and enabling organizations. As the founder and CEO of Layer5, he is at the forefront of the cloud native movement. \n    <Link to=\"/community/members/nic-jackson\"> Nic Jackson </Link> is a developer advocate at HashiCorp, and the author of “Building Microservices in Go”, a book which examines the best patterns and practices for building microservices with the Go. Nic is also a coauthor of the Service Mesh Patterns book.\n    Nic Jackson has been using Docker Desktop from long time and tells how Docker Desktop provides feasibility to run Kubernetes for his local environment,\n    while Lee Calcote recalls how Docker Desktop has become a staple of his daily routine while building an extension for Meshery.\n<h3>Discussing Developers need to access Kubernetes</h3>\n<img\n    src={developer}\n    alt=\"Developers need to access Kubernetes\"\n    style={{ width: \"50%\", float: \"right\" }}\n/>\nMoving forward with the introduction to Docker Extension for Meshery, Nic says, \"When it comes to Kubernetes, should developers care about Kubernetes? And the answer is \"Yes\" because developers need Kubernetes because of the changing reliability patterns implementation rapidly, it provides a tight feedback loop for developing and deploying workloads onto your Kubernetes and makes the over developer experience smooth. With the help of Kubernetes, developers should be able to create environments without needing to be operational experts.\"\n    <h3>Meshery Extension offers an easy single click button to go from Docker Compose to Kubernetes wih all cloud native infrastructure supported.</h3>\n<img\n    src={DDE}\n    alt=\"Docker Desktop Extension Meshery\"\n    style={{ width: \"50%\", float: \"right\" }}\n/>\n    The Docker Extension for Meshery provides:\n<ul>\n<li>Kubernetes support for your Docker Compose apps - Import your Docker Compose apps. Configure and deploy them to Kubernetes and integrate into your GitOps pipeline.</li>\n<li>Collaborative designer for Docker Compose apps - Early access to the Docker Extension for Meshery that offers a visual topology for designing Docker Compose applications, operating Kubernetes, Kubernetes Operators, and their workloads.</li>\n<li>Single-click deployment of your infrastructure - Support of 250+ different cloud native infrastructure projects (including all the CNCF projects) at the fingertips of developers in connection with Docker Desktop’s ability to deliver Kubernetes locally.</li>\n</ul>\n        <img\n            src={dashboard}\n            alt=\"Docker Desktop Extension Meshery Dashboard\"\n            style={{ width: \"50%\", float: \"left\", paddingRight: \"30px\" }}\n        />\n    <ul>\n    <li>\n    While giving the demo Meshery Lee explained how with Docker Desktop running and Meshery extension installed we can see that Meshery has discovered\n    the installed Kubernetes cluster automatically and it performs action with the adapter present. \n    </li>\n    <li>\n    Meshery is a Kubernetes multi-cluster manager capable of performing lifecycle management of all CNCF projects, like Flux, Argo, Prometheus, Envoy, Jaeger, and so on.\n     </li>  \n    <li>\n    We can configure different cloud native infrastructure based on their configurations and performs actions such as deploy, design, and explore all of the capabilities of cloud native infrastructure via a rich schema with a live preview.\n    </li> \n    </ul>\n        <h3>Lee Calcote introduces MeshMap</h3>\n                <img\n                    src={designer}\n                    alt=\"MeshMap Designer\"\n                    style={{ width: \"50%\", float: \"right\" }}\n                />\nMeshMap is the world's only visual designer for Kubernetes and all cloud native infrastructure. Collaborate with other engineers in real-time as you use MeshMap to design, deploy, and manage your Kubernetes-based deployments. Save time and use a design template. Take advantage of the best practices embedded in the patterns found in Meshery Catalog. MeshMap not only allows you to create and verify your cloud native application and infrastructure configurations, but to deploy and operate that infrastructure as well.\n\n    Lee demonstrated MeshMap <strong>Designer</strong>  design capabilities using Consul Service Mesh as an eample and configures various Consul-specific features. He designs a service mesh deployment with application and Envoy filter from scratch.\n                            <img\n                                src={visualizer}\n                                alt=\"MeshMap Designer\"\n                                style={{ width: \"50%\", float: \"left\", paddingRight: \"30px\" }}\n                            />\n     MeshMap <strong>Visualizer Mode</strong> allows you to examines a visual topology of Kubernetes cluster and its services. View and search log streams from your pod's containers. Connect an interactive terminal to instances of your containers.\n    To Lee's demo, Nic also added in the context of developer experience that MeshMap also makes it easy to do the job and with Meshery extension it makes it super easy to write and configure code without being worried for the developer environment.\n           These Docker Extensions are so powerful that it allows you to do multiple tasks without leaving Docker Desktop.\n<div className=\"iframe-container\">\n<iframe src=\"https://www.youtube.com/embed/3DPZafR8VWM\" loading=\"lazy\" title=\"YouTube video player\" frameBorder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></iframe>\n</div>\n\n<strong>Lee Calcote and Nic Jackson packed a great deal of information in this talk. Find the recording below. The Meshery Extension is now out! Try now, and Share your Experience </strong>\n<Link to=\"/meshmap\">Apply for MeshMap Beta Program</Link>\n</BlogWrapper>\n","frontmatter":{"title":"Extending the Docker Compose Experience to Service Mesh","subtitle":null,"date":"May 10th, 2022","author":"Gaurav Chadha","thumbnail":{"extension":"webp","publicURL":"/static/353e6b4dcf99ef4c8b16120ff9e1291e/DC22-talk-HashiCorp.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRnIAAABXRUJQVlA4IGYAAABwBACdASoUAAsAPtFUo0uoJKMhsAgBABoJbACdMoRwAB6/pABoj2NUy/koAADL/k2zUZNSDZNP4McvnXa+KZwCUuZErfVazXD/7BG3PNmBDAo+RXP2IQbdHXgMBjCf0RJ75yHgAAA="},"images":{"fallback":{"src":"/static/353e6b4dcf99ef4c8b16120ff9e1291e/4cd12/DC22-talk-HashiCorp.webp","srcSet":"/static/353e6b4dcf99ef4c8b16120ff9e1291e/a66aa/DC22-talk-HashiCorp.webp 750w,\n/static/353e6b4dcf99ef4c8b16120ff9e1291e/65dd5/DC22-talk-HashiCorp.webp 1080w,\n/static/353e6b4dcf99ef4c8b16120ff9e1291e/f9724/DC22-talk-HashiCorp.webp 1366w,\n/static/353e6b4dcf99ef4c8b16120ff9e1291e/4cd12/DC22-talk-HashiCorp.webp 1499w","sizes":"100vw"},"sources":[]},"width":1,"height":0.5630420280186791}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/353e6b4dcf99ef4c8b16120ff9e1291e/DC22-talk-HashiCorp.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRnIAAABXRUJQVlA4IGYAAABwBACdASoUAAsAPtFUo0uoJKMhsAgBABoJbACdMoRwAB6/pABoj2NUy/koAADL/k2zUZNSDZNP4McvnXa+KZwCUuZErfVazXD/7BG3PNmBDAo+RXP2IQbdHXgMBjCf0RJ75yHgAAA="},"images":{"fallback":{"src":"/static/353e6b4dcf99ef4c8b16120ff9e1291e/6e8bd/DC22-talk-HashiCorp.webp","srcSet":"/static/353e6b4dcf99ef4c8b16120ff9e1291e/46142/DC22-talk-HashiCorp.webp 125w,\n/static/353e6b4dcf99ef4c8b16120ff9e1291e/81c3e/DC22-talk-HashiCorp.webp 250w,\n/static/353e6b4dcf99ef4c8b16120ff9e1291e/6e8bd/DC22-talk-HashiCorp.webp 500w,\n/static/353e6b4dcf99ef4c8b16120ff9e1291e/bf95e/DC22-talk-HashiCorp.webp 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":282}}}},"fields":{"slug":"/blog/docker/extending-the-docker-compose-experience-to-service-mesh"}},{"id":"6f70ddf7-740d-5640-b104-fa03b4622855","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport Blockquote from \"../../../../reusecore/Blockquote\";\nimport { Link } from \"gatsby\";\nimport CloudNative from './cloud-native-management.webp';\nimport CloudNativeIdentity from './cloud-native-identity.webp';\nimport dde from './dde.webp';\nimport designer from './Designer.webp';\nimport visualizer from './Visualizer.webp';\nimport meshmapInDocker from './meshmap-docker-extension-for-meshery.webp';\n\n<BlogWrapper>\n\n<div className=\"intro\">\n  <Link to=\"/community/members/lee-calcote\">Lee Calcote</Link> and{\" \"}\n  <Link to=\"/community/members/maximiliano-churichi\">Maximiliano Churichi</Link>{\" \"}\n  gave a presentation entitled <i>Extending Docker with Meshery, SPIRE, and Istio</i> at DockerCon 2022.\n</div>\n\n<p style={{ marginLeft: \"10px\", fontStyle: \"italic\" }}>\n  <Link to=\"/community/members/lee-calcote\">Lee Calcote</Link> is an innovative\n  product and technology leader, passionate about empowering engineers and\n  enabling organizations. As the founder and CEO of Layer5, he is at the\n  forefront of the cloud native movement.\n</p>\n\n<p style={{ marginLeft: \"10px\", fontStyle: \"italic\" }}>\n  <Link to=\"/community/members/maximiliano-churichi\">Maximiliano Churichi</Link>{\" \"}\n  is a Software Engineer at Hewlett Packard Enterprise, working in the Security\n  Engineering team, and fully engaged in open source technologies, passionate\n  about service mesh and cloud-native security.\n</p>\n\n<div>\n  <h3>\n    Cloud Native Management\n    <img\n      src={CloudNative}\n      alt=\"Meshery Docker Extension\"\n      style={{ width: \"50%\", float: \"right\" }}\n    />\n  </h3>\n  <p>\n    Lee Calcote introduces Meshery as a Cloud Native Management Plane, stating:\n    <Blockquote\n      quote=\"Meshery does Lifecycle and Performance Management of 10 different service meshes; more than that, it helps with configuration management with Kubernetes and with the Meshery Docker Extension it does the same for the Docker Compose application.\"\n    />\n  </p>\n</div>\n\n<p>\n  As a{\" \"}\n  <Link to=\"https://www.docker.com/blog/docker-captain-take-5-lee-calcote/\">Docker Captain</Link>, Lee has always been a proponent of Docker, particularly its enablement of developer workflows. Docker Extensions bring an integrated experience with ecosystem tooling like Meshery — a critical tool for developers configuring and managing cloud native applications.\n</p>\n\n<div>\n  <h3>Cloud Native Identity</h3>\n\n  <p>\n    Maximiliano Churichi briefly explains Cloud Native Identity and HPE's open\n    source Project Mithril:\n  </p>\n\n  <p>\n    <Link to=\"https://spiffe.io\">SPIFFE</Link> (Secure Production Identity\n    Framework For Everyone) is a{\" \"}\n    <Link to=\"https://www.cncf.io/projects/\">CNCF-incubated</Link> project that\n    defines standards for identifying and securing communications between\n    application services. The{\" \"}\n    <Link to=\"https://spiffe.io/docs/latest/spire-about\">SPIRE</Link> project is\n    a production-ready reference implementation of these principles, offering\n    APIs for attestation policies, certificate issuance, and rotation.\n  </p>\n\n  <img\n    src={CloudNativeIdentity}\n    alt=\"SPIRE and SPIFFE in Cloud Native Identity\"\n    style={{ width: \"50%\", float: \"right\" }}\n  />\n\n  <p>\n    Maximiliano explains how HPE's Project Mithril integrates SPIRE and Istio to\n    strengthen service identity in the data plane.{\" \"}\n    <strong>Project Mithril leverages the service management capabilities of Istio and the strong identity-by-attestation principles of SPIFFE and SPIRE to deliver robust and flexible attestation beyond Kubernetes namespaces and service accounts</strong>. It provides end-to-end secure workload attestation based on zero-trust principles, regardless of workload location.\n  </p>\n\n  <p>\n    Improvements from Project Mithril have been upstreamed into Istio and are\n    expected in Istio 1.14, enabling users to leverage SPIRE for SPIFFE identity\n    management and stronger attestation mechanisms.\n  </p>\n</div>\n\n<h3>How the Docker Extension for Meshery enables single-click deployment</h3>\n\n<img\n  src={dde}\n  alt=\"Docker Extension for Meshery\"\n  style={{ width: \"50%\", float: \"left\", paddingRight: \"2rem\" }}\n/>\n\n<p>\n  The new Meshery Docker Extension brings{\" \"}\n  <Link to=\"/meshmap\">Layer5 MeshMap</Link>, the world's only visual designer\n  for Kubernetes and service mesh deployments, to millions of developers’\n  desktops. Developers and operators can visually configure and operate cloud\n  native infrastructure using MeshMap’s low-code visual designer.\n</p>\n\n<p>\n  Maximiliano Churichi of HPE describes how Meshery conveniently integrates\n  multiple services into Docker:\n</p>\n\n<ul>\n  <li>\n    <strong>Kubernetes and service mesh support for your Docker Compose apps</strong>{\" \"}— Import Docker Compose apps and deploy them to Kubernetes or any service mesh.\n  </li>\n  <li>\n    <strong>Visual design of Kubernetes applications</strong> — Use{\" \"}\n    <Link to=\"/meshmap\">MeshMap</Link> as a visual topology designer for Docker\n    Compose, Kubernetes workloads, CRDs, and operators.\n  </li>\n  <li>\n    <strong>Single-click deployment</strong> — 250+ Kubernetes operators and\n    60+ cloud services ready to use alongside Docker Desktop’s local Kubernetes.\n  </li>\n  <li>\n    <strong>Detection of Kubernetes environments</strong> — Scan kubeconfigs,\n    switch clusters, or manage them concurrently.\n  </li>\n</ul>\n\n<h3>Maximiliano demonstrates MeshMap</h3>\n\n<img src={meshmapInDocker} alt=\"MeshMap in Meshery Docker Extension\" />\n\n<div\n  style={{\n    display: \"grid\",\n    gridTemplateColumns: \"1fr 1fr\",\n    padding: \"10px\",\n    gap: \"1rem\",\n  }}\n>\n  <div style={{ padding: \"20px\", textAlign: \"center\" }}>\n    <h5>Designer Mode</h5>\n    <p>\n      Design a service mesh deployment with applications and Envoy filters from\n      scratch, or customize deployments from patterns.\n    </p>\n    <img src={designer} alt=\"MeshMap Designer\" />\n  </div>\n\n  <div style={{ padding: \"20px\", textAlign: \"center\" }}>\n    <h5>Visualizer Mode</h5>\n    <p>\n      Examine a visual topology of your Kubernetes cluster and its services. View\n      logs from pods and open interactive terminals to containers.\n    </p>\n    <img src={visualizer} alt=\"MeshMap Visualizer\" />\n  </div>\n</div>\n\n<div className=\"iframe-container\">\n  <iframe\n    src=\"https://www.youtube.com/embed/SazEJizK4xQ\"\n    loading=\"lazy\"\n    title=\"YouTube video player\"\n    frameBorder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n    allowFullScreen\n  ></iframe>\n</div>\n\nLee Calcote and Maximiliano Churichi packed a great deal of information into this talk. Watch the recording above! The Meshery Extension is now out—try it and share your experience!\n\n<p>\n  <Link to=\"/meshmap\">Apply for the MeshMap Beta Program</Link>\n</p>\n\n</BlogWrapper>\n","frontmatter":{"title":"Extending Docker with Meshery, SPIRE, and Istio","subtitle":null,"date":"May 10th, 2022","author":"Gaurav Chadha","thumbnail":{"extension":"webp","publicURL":"/static/6d9aae3d5df3089bd154e53c9d945331/dc22-hpe-talk.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRsAAAABXRUJQVlA4ILQAAADwBACdASoUAAsAPtFUo0uoJKMhsAgBABoJbACdMoRwIsAKKsXQ3bDU0Sh7xNByx8AA3QTx+NCIdbtlgNUIj8p8TTldQEJK/Onajgz/QWQc/tznhrECMWd6h2YF5Vur29u/3qZIIEF6x1ljtCpku538y8znYOZfzbugSVf57n6bu5fySdDxuSAlPHMO8HeUR+DU0Js9dn/ooTZ5vtV1ZIsjnj3E/a5Rec1TEKlY1hCyDOdAAAA="},"images":{"fallback":{"src":"/static/6d9aae3d5df3089bd154e53c9d945331/a7268/dc22-hpe-talk.webp","srcSet":"/static/6d9aae3d5df3089bd154e53c9d945331/8d8ff/dc22-hpe-talk.webp 750w,\n/static/6d9aae3d5df3089bd154e53c9d945331/eedfa/dc22-hpe-talk.webp 1080w,\n/static/6d9aae3d5df3089bd154e53c9d945331/e048d/dc22-hpe-talk.webp 1366w,\n/static/6d9aae3d5df3089bd154e53c9d945331/a7268/dc22-hpe-talk.webp 1907w","sizes":"100vw"},"sources":[]},"width":1,"height":0.5595175668589407}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/6d9aae3d5df3089bd154e53c9d945331/dc22-hpe-talk.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRsAAAABXRUJQVlA4ILQAAADwBACdASoUAAsAPtFUo0uoJKMhsAgBABoJbACdMoRwIsAKKsXQ3bDU0Sh7xNByx8AA3QTx+NCIdbtlgNUIj8p8TTldQEJK/Onajgz/QWQc/tznhrECMWd6h2YF5Vur29u/3qZIIEF6x1ljtCpku538y8znYOZfzbugSVf57n6bu5fySdDxuSAlPHMO8HeUR+DU0Js9dn/ooTZ5vtV1ZIsjnj3E/a5Rec1TEKlY1hCyDOdAAAA="},"images":{"fallback":{"src":"/static/6d9aae3d5df3089bd154e53c9d945331/f03ce/dc22-hpe-talk.webp","srcSet":"/static/6d9aae3d5df3089bd154e53c9d945331/46142/dc22-hpe-talk.webp 125w,\n/static/6d9aae3d5df3089bd154e53c9d945331/2cd09/dc22-hpe-talk.webp 250w,\n/static/6d9aae3d5df3089bd154e53c9d945331/f03ce/dc22-hpe-talk.webp 500w,\n/static/6d9aae3d5df3089bd154e53c9d945331/3a00d/dc22-hpe-talk.webp 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":280}}}},"fields":{"slug":"/blog/docker/extending-docker-with-meshery-spire-and-istio"}}]}},"pageContext":{"category":"Docker"}},"staticQueryHashes":["1485533831","4047814605","408154852","4152005505"],"slicesMap":{},"matchPath":"/blog/category/docker"}