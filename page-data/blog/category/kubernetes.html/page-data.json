{"componentChunkName":"component---src-templates-blog-category-list-js","path":"/blog/category/kubernetes.html","result":{"data":{"allMdx":{"nodes":[{"id":"d47787bd-4982-5d4c-89d7-c99fbcb183d1","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\n\n<BlogWrapper>\n\nA common point of confusion when working with Kubernetes is understanding how `ConfigMap` updates are handled. You‚Äôve pushed a change to your ConfigMap, but your application isn't seeing the new values. What's going on?\n\nThe answer depends entirely on **how your application consumes the ConfigMap**. There isn't a \"type\" of ConfigMap object itself, but rather two distinct *methods of consumption* by a Pod, and each has drastically different behavior regarding updates.\n\nTo tell what \"kind\" you have, you need to look at your Pod or Deployment's YAML definition.\n\n## How to Check Your Pod's ConfigMap Consumption\n\nYou can find out how a Pod is using a ConfigMap by inspecting its YAML definition. üßê  Run this command to get the running YAML for a specific pod:\n\n```bash\nkubectl get pod <your-pod-name> -o yaml\n```\n\nNow, look for two key sections in the `spec.containers` list:\n\n1.  **Environment Variables:** Look for `env` or `envFrom`.\n2.  **Mounted Volumes:** Look for `volumeMounts` and the corresponding `volumes` section at the Pod spec level.\n\nLet's break down what each one means for reloading.\n\n## 1. Consumed as Environment Variables\n\nThis is when your Pod's YAML injects ConfigMap data directly as environment variables for the container.\n\n### How to Identify It\n\nIn your Pod spec, you'll see blocks like this:\n\n```yaml\n# ...\nspec:\n  containers:\n  - name: my-app-container\n    image: my-app\n    env: # <-- Look here\n      - name: MY_CONFIG_KEY\n        valueFrom: # <-- Or here\n          configMapKeyRef:\n            name: my-special-config\n            key: some.config.key\n    envFrom: # <-- Or here\n      - configMapRef:\n          name: my-special-config\n# ...\n```\n\nIf you see `env` or `envFrom` pointing to a `configMapKeyRef` or `configMapRef`, your application is consuming the ConfigMap as environment variables.\n\n### Reload Behavior: üõë No Hot-Reload\n\nThis is the most critical difference: **Changes to a ConfigMap are NOT reflected in running Pods that use them as environment variables.**\n\nEnvironment variables are set by the container runtime *only when the container is created*. They are immutable for the life of that running process.\n\n**How to Apply Changes:** To make the application see the new ConfigMap values, you **must restart the Pod**. The simplest way to do this for a `Deployment` is with a rolling restart:\n\n```bash\nkubectl rollout restart deployment <your-deployment-name>\n```\n\nWhen the new Pods are created, they will read the *updated* ConfigMap data and set the new environment variables.\n\n\n## 2. Consumed as a Mounted Volume\n\nThis method mounts your ConfigMap as one or more files inside your Pod's filesystem. Your application is programmed to read its configuration from these files (e.g., `/app/config/settings.properties`).\n\n### How to Identify It\n\nYou'll see two corresponding sections in your Pod spec:\n\n1.  `spec.containers.volumeMounts`: This tells the container where to mount the volume.\n2.  `spec.volumes`: This defines the volume itself and links it to the ConfigMap.\n\n\n```yaml\n# ...\nspec:\n  containers:\n  - name: my-app-container\n    image: my-app\n    volumeMounts: # <-- Look here\n    - name: config-volume\n      mountPath: /etc/config\n  volumes: # <-- And here\n  - name: config-volume\n    configMap:\n      name: my-special-config\n# ...\n```\n\nIf you see this `volumes` and `volumeMounts` pairing, your application is consuming the ConfigMap as files.\n\n### Reload Behavior: ‚úÖ Automatic... With a Catch\n\nThis method **does support hot-reloading**, but with two important caveats:\n\n1.  **There is a delay.** When you update the ConfigMap object, the `kubelet` on the node is responsible for updating the mounted files. This is not instantaneous. It relies on a periodic sync cycle, and the total delay can be **60 to 90 seconds (or even longer)** before the files at `mountPath` are actually updated.\n\n2.  **Your application must support it.** Kubernetes *only* updates the files on disk. It does **not** send a signal (like `SIGHUP`) to the process or restart the container. Your application must be built to:\n\n      * Watch the configuration files for changes (using a library like `fsnotify`).\n      * Periodically re-read the configuration files on its own timer.\n\nIf your application only reads its config files on startup, it will behave just like the environment variable method: **it will not see the changes until it is restarted.**\n\n\n## ConfigMap Reload Behavior Summary\n\nHere‚Äôs a simple table to remember the differences:\n\n| Consumption Method | How to Identify in Pod YAML | Are Changes Updated in Running Pod? | How Are Changes Seen? |\n| :--- | :--- | :--- | :--- |\n| **Environment Variables** | `spec.containers.env` `spec.containers.envFrom` | **No** ‚ùå | Pod must be **restarted**. |\n| **Mounted Volume** | `spec.containers.volumeMounts` `spec.volumes` | **Yes** ‚úÖ (with delay) | Kubelet updates files. **Application must be coded** to reload the updated file. |\n\n### What If I Need Automatic Restarts?\n\nIf you are using the volume mount method but your application doesn't support live reloading, you can use a \"reloader\" tool. A popular open-source controller like [**Stakater's Reloader**](https://github.com/stakater/Reloader) can watch for ConfigMap changes and automatically trigger a rolling restart of any Deployment that uses it. This gives you the best of both worlds: configuration in files and automatic updates for apps that can't reload on their own.\n\n<br />\n<hr />\n<br />\n\n## Skip the CLI. Power up with Kanvas\n\nAlternatively, you can skip the YAML editing and make these changes visually. That is, if you're managing your Kubernetes cluster using Kanvas. Let's break down how to use it to manage your resources, like a `ConfigMap`. \n\n## ü§î What is Kanvas Designer?\n\n[Layer5's Kanvas](https://layer5.io/kanvas) is a powerful tool for designing, deploying, and managing your Kubernetes and Cloud infrastructure and workloads from a visual interface. Instead of writing hundreds of lines of YAML by hand, you build a **Design**. This design is a visual representation of your components (`Deployment`, `Service`, `ConfigMap`, etc.) and their relationships.\n\n## üé® How to Update a ConfigMap in Kanvas Designer\n\nUpdating a `ConfigMap` through the Designer follows this \"design-first\" workflow. You don't just \"edit\" the live resource in the cluster; you **update your design** and then **(re-)deploy it**.\n\nHere is the step-by-step process:\n\n1.  **Open Kanvas Designer:** Log in to your Kanvas UI and navigate to Designer mode (the default mode).\n\n2.  **Load Your Design:** Open the design file that contains the `ConfigMap` you want to edit. If you don't have a design yet, you can import your existing `ConfigMap` from your cluster directly onto the canvas.\n\n3.  **Find the ConfigMap Component:** On the visual canvas, find the block representing your `ConfigMap`. It will have the Kubernetes icon and the \"ConfigMap\" kind.\n\n4.  **Edit the Configuration:** Click on the component. A configuration panel will slide out, often with a 'Configure' tab or an editor icon. This will show you the key/value pairs for that *specific* `ConfigMap` resource.\n\n5.  **Deploy the Design:** Changes are automatically saved in your design as you make them. Use the **Deploy** button to send your entire design to your target Kubernetes or Cloud environment. Kanvas will calculate the difference (a \"diff\") and apply the updated `ConfigMap` manifest to your cluster. This action is the equivalent of running `kubectl` server-side apply using your design.\n\n\n## üõë The Most Important Part: Reload Behavior\n\nThis is critical: **Using Kanvas Designer to update a ConfigMap does NOT change how your application reloads it.**\n\nDeploying from Kanvas is just a friendly, visual way to run `kubectl apply`. The rules we discussed in our previous post about ConfigMap behavior still apply completely:\n\n  * **If your Pod consumes the ConfigMap as Environment Variables:** Your running Pods **will not** see the change. You must still restart them (e.g., `kubectl rollout restart deployment ...`).\n  * **If your Pod consumes the ConfigMap as a Mounted Volume:** The files inside the Pod **will** be updated (after the kubelet sync delay), but your application *still* needs to be smart enough to re-read that file from disk.\n\nKanvas Designer simplifies the *applying* of the change and helps you visually manage your application's state, but it doesn't change the fundamental Kubernetes behavior of *how* that change is consumed by your workloads.\n\n</BlogWrapper>\n","frontmatter":{"title":"How can you tell what kind of Kubernetes Configmap you have?","subtitle":"Understanding when changes are reloaded","date":"November 3rd, 2025","author":"Lee Calcote","thumbnail":{"extension":"png","publicURL":"/static/88427231d61b05cc39b0208166c642ea/image.png","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqklEQVR42gGfAmD9APzQBffNCPbJC+S8GaKZNFlzTC5YWw89WBw9XWl8lWeAp32Polh0ill7lF6Em1aDmk2Al0d9lUZ8lEd8kgDduiDPsyq5qTmZnE9yi2RNeHZCc4U9a4XJ0dTp8f+Nr/Tr8v/T1tdsi6BtkqhgjKJRhZxIfpZGfpZFfpcAmpxUiZZgaop6UoSUW42iaJiviK3Bp73J09TVysvNqK+8vr7Av8XKk66+g6a5bpuxWY+pS4ahRIGgQYCiAFqFhlWEkk2FqFWQwHqp0a3K4ODr8d7g4dDR0t3d3eHi4dLW2cPQ2LbN2Ze7zXmrw1+cukyRtUCKtDuHtQA6eKNBfq5SjcF0pdeoyOni7fj7/P3g4+Pj5+bl6OfW4uT////u9fjH3+mgy9x8uNNep8xHm8k5k8oyj8sAMnCqQHy1XpPHjLTextrx9Pf8////5e7txeDd9vLz+fv7+Pv75PL1weLrmc/hc77aVK/YPaTZLpzdJpfhADFnnUN1qWaRvZe11c7c7fb6/f/9/snh367o4f////b6+ejw8s3k6arX4IPJ2mC92EOz2y2p4h+h6hmd8QAwWYJCaY5khKSSqL/G0d3x9Pb////T4N/T5eP////r8PHR3uCuzdGIwspkvMhFt8wss9Qard8Qpu0LovkAK0ZdO1RrV22BfY6dq7S92Nve/P398PDw7evr////3uLjr77BhqqvYqarQqmwKK+5FbLCCrDMBKreAqb1ACMyNy8+RkNTXV9td36Ij56lqdDU1eLo59PV1trb3K6ytXqHjFd9gTuFhyOWlRGnowWzqgK0qAKxrwCtzAAdIhomLis0P0NGUlhUX2ZVYGVTYGVebXFPXWJEVlpYXGJET1UwVVgfbGwPiIEEoZECspQQuIQ5uGx6ulHgG5cIXzLUqQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/88427231d61b05cc39b0208166c642ea/87706/image.png","srcSet":"/static/88427231d61b05cc39b0208166c642ea/0dee1/image.png 750w,\n/static/88427231d61b05cc39b0208166c642ea/8beaa/image.png 1080w,\n/static/88427231d61b05cc39b0208166c642ea/87706/image.png 1280w","sizes":"100vw"},"sources":[{"srcSet":"/static/88427231d61b05cc39b0208166c642ea/a66aa/image.webp 750w,\n/static/88427231d61b05cc39b0208166c642ea/65dd5/image.webp 1080w,\n/static/88427231d61b05cc39b0208166c642ea/71d4d/image.webp 1280w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5625}}},"darkthumbnail":{"extension":"png","publicURL":"/static/88427231d61b05cc39b0208166c642ea/image.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqklEQVR42gGfAmD9APzQBffNCPbJC+S8GaKZNFlzTC5YWw89WBw9XWl8lWeAp32Polh0ill7lF6Em1aDmk2Al0d9lUZ8lEd8kgDduiDPsyq5qTmZnE9yi2RNeHZCc4U9a4XJ0dTp8f+Nr/Tr8v/T1tdsi6BtkqhgjKJRhZxIfpZGfpZFfpcAmpxUiZZgaop6UoSUW42iaJiviK3Bp73J09TVysvNqK+8vr7Av8XKk66+g6a5bpuxWY+pS4ahRIGgQYCiAFqFhlWEkk2FqFWQwHqp0a3K4ODr8d7g4dDR0t3d3eHi4dLW2cPQ2LbN2Ze7zXmrw1+cukyRtUCKtDuHtQA6eKNBfq5SjcF0pdeoyOni7fj7/P3g4+Pj5+bl6OfW4uT////u9fjH3+mgy9x8uNNep8xHm8k5k8oyj8sAMnCqQHy1XpPHjLTextrx9Pf8////5e7txeDd9vLz+fv7+Pv75PL1weLrmc/hc77aVK/YPaTZLpzdJpfhADFnnUN1qWaRvZe11c7c7fb6/f/9/snh367o4f////b6+ejw8s3k6arX4IPJ2mC92EOz2y2p4h+h6hmd8QAwWYJCaY5khKSSqL/G0d3x9Pb////T4N/T5eP////r8PHR3uCuzdGIwspkvMhFt8wss9Qard8Qpu0LovkAK0ZdO1RrV22BfY6dq7S92Nve/P398PDw7evr////3uLjr77BhqqvYqarQqmwKK+5FbLCCrDMBKreAqb1ACMyNy8+RkNTXV9td36Ij56lqdDU1eLo59PV1trb3K6ytXqHjFd9gTuFhyOWlRGnowWzqgK0qAKxrwCtzAAdIhomLis0P0NGUlhUX2ZVYGVTYGVebXFPXWJEVlpYXGJET1UwVVgfbGwPiIEEoZECspQQuIQ5uGx6ulHgG5cIXzLUqQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/88427231d61b05cc39b0208166c642ea/86bab/image.png","srcSet":"/static/88427231d61b05cc39b0208166c642ea/e1b03/image.png 125w,\n/static/88427231d61b05cc39b0208166c642ea/81b70/image.png 250w,\n/static/88427231d61b05cc39b0208166c642ea/86bab/image.png 500w,\n/static/88427231d61b05cc39b0208166c642ea/8cf60/image.png 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/88427231d61b05cc39b0208166c642ea/46142/image.webp 125w,\n/static/88427231d61b05cc39b0208166c642ea/81c3e/image.webp 250w,\n/static/88427231d61b05cc39b0208166c642ea/cd07d/image.webp 500w,\n/static/88427231d61b05cc39b0208166c642ea/bf95e/image.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":281}}}},"fields":{"slug":"/blog/kubernetes/how-can-you-tell-what-kind-of-kubernetes-configmap-you-have"}},{"id":"c3ee879a-1381-5698-82ef-418a59817e7c","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport Layer5 from \"./Layer5.svg\";\n\n<BlogWrapper>\n\n# Load Generation\n\nLoad generation is a crucial testing technique employed to scrutinize a system's behavior under specific loads. The primary objectives include identifying bottlenecks, measuring performance, and gaining insights into how the system operates under extreme conditions.\n\nLoad tests empower developers to:\n\n- Understand the scalability of the application\n- Identify and rectify bottlenecks\n- Ensure reliability and robustness of the application\n- Predict system behavior under varying conditions\n\n## [Meshery](https://meshery.io/) Load Generator\n\nLet's delve into Meshery, a multi-Cloud Native management plane founded by [Layer5](https://layer5.io/)‚Äîa community dedicated to open-source initiatives in network management and observability. Meshery aids developers in assessing the performance of their Cloud Native applications.\n\n Meshery boasts a comprehensive set of features, including:\n\n- Performance testing\n- Cloud Native Lifecycle Management\n- Configuration management\n\nSupporting various load generators like HTTP/HTTPS/HTTP2, [Nighthawk](https://layer5.io/projects/nighthawk) and its built-in load generator, Meshery facilitates the creation of synthetic HTTP load and enables benchmarking the performance of Cloud Native applications.\nDevelopers can choose their preferred load generator for tests and benefit from the capability to run tests, providing a more comprehensive performance analysis.\n\nWith Meshery, developers can:\n\n- Manage the lifecycle of various [Cloud Native](https://play.meshery.io/) applications.\n- Conduct performance tests and compare results between across different [Cloud Native](https://play.meshery.io/) environments.\n- Apply custom or pre-defined configurations to enhance [Cloud Native](https://play.meshery.io/) functionality.\n\n\n# Load Balancing  \n\nLoad balancing is a critical technique that enhances the responsiveness, reliability, and scalability of applications by efficiently distributing incoming network traffic across multiple servers. In this guide, we'll explore various load balancing techniques and algorithms, shedding light on their roles, benefits, and challenges in the context of Kubernetes networking.\n\n## Introduction: The Crucial Role of Load Balancing\n\nLoad balancing plays a pivotal role in optimizing the performance of internet applications by efficiently managing network traffic. Imagine a grocery store with multiple checkout lines‚Äîone open and the others closed. Load balancing is akin to opening all checkout lines,thereby reducing wait times, and improving overall efficiency.\n<br/>\n\n <figure className=\"imgWithCaption fig-right\">\n    <img src={Layer5} />\n    <figcaption>\n    Layer5\n    </figcaption>\n </figure>\n\n### How Load Balancing Works:\n\nLoad balancing involves three main components:\n\n1. **Load Balancer:** Positioned between users and application servers, it monitors incoming traffic and redirects it to the most suitable server. It can be hardware-based or software-based, with the latter running on existing infrastructure.\n\n2. **Application Servers:** These servers host applications, receiving traffic from the load balancer and serving requested content to users.\n\n3. **Users:** End-users access applications through the internet or intranet.\n\nWhen a user initiates a request, the load balancer evaluates factors such as server capacity, usage rate, and response time to determine the optimal server. This process enhances efficiency and reduces user wait times.\n\n## Benefits of Load Balancers: Performance, Reliability, and Scalability\n\n > **Improved Performance:** By distributing workloads evenly, response time and system latency are reduced.\n  \n > **Increased Reliability:** Fault tolerance is enhanced by detecting and removing failed or unhealthy servers,redirecting traffic to functional servers. \n\n >  **Enhanced Scalability:** The system's capacity can be horizontally scaled by adding more servers without impacting existing ones.\n\n > **Reduced Costs:** Efficient resource utilization avoids underutilization or overprovisioning of servers.\n\n## Load Balancing with [Meshery](https://meshery.io/)\n    \nMeshery  serves as a platform for managing various Cloud Native applications . While it doesn't directly perform load balancing directly, Meshery allows users to configure and observe load balancing within their chosen Cloud Native Adaptors. Through a unified dashboard, users can customize load balancing settings, monitor relevant metrics, and leverage observability tools to gain insights into the performance and health of load balancing features. Meshery's focus is on providing a consistent and user-friendly interface for Cloud Native Lifecycle Management, including load balancing configuration.\n\n**Adapters**\n\n  - [Istio](https://docs.meshery.io/service-meshes/adapters/istio)\n  - [Linkerd](https://docs.meshery.io/service-meshes/adapters/linkerd)\n  - [Consul](https://docs.meshery.io/service-meshes/adapters/consul)\n  -\t[Network Service Mesh](https://docs.meshery.io/service-meshes/adapters/nsm)\n\n\n## Types of Load Balancers in Kubernetes: Service, Ingress, and External\n\n1. **Service Load Balancing:** This basic Kubernetes tactic routes all requests to a service and directs them to matching pods. It's suitable for cluster-internal traffic but lacks advanced features like SSL termination.\n\n2. **Ingress Load Balancing:** Handling external traffic, it routes requests based on rules defined in an ingress resource. It offers more flexibility and functionality, including SSL termination and authentication.\n\n3. **External Load Balancing:** Involving external load balancers from cloud providers (AWS ELB, Google Cloud Load Balancer), it provides layer-4 or layer-7 load balancing and high availability. However, it adds complexity and additional costs.\n\n## Challenges of Load Balancing\n\nDespite its advantages, load balancing comes with challenges such as the need for careful configuration to address specific application requirements and potential complexities in managing dynamic workloads.\n\n\n## Load Balance Algorithms: Static and Dynamic\n <br/>\n\n##### Static Load Balancing Algorithms:\n\n1. **Round Robin:** Distributes requests sequentially across servers.\n  \n2. **Hash:** Distributes requests based on a key, such as client IP address or request URL.\n  \n3. **Random:** Distributes requests randomly.\n\n##### Dynamic Load Balancing Algorithms:\n\n1. **Least Connections:** Directs requests to the server with the fewest active connections.\n\n2. **Least Response Time:** Considers active connections and average response time to determine the optimal server.\n\n3. **Least Bandwidth:** Routes requests to the server with the least traffic.\n\n## Choosing the Right Load Balancer: Tips and Considerations\n\nConsider factors like application needs, performance requirements, and scalability when selecting a load balancer‚Äîwhether hardware, software, or a combination.\n<br/>\n\n## Conclusion: Empowering Applications with Optimal Load Balancing\n\nLoad balancing is a powerful technique that significantly impacts the performance, reliability, and scalability of applications. Whether distributing workloads based on static or dynamic algorithms, choosing the right load balancing strategy is crucial for delivering an exceptional user experience and optimizing resources. As technologies evolve, staying informed about the latest load balancing practices ensures continuous improvement in application delivery.\n\n</BlogWrapper>\n","frontmatter":{"title":"Load Generation and Load Balancing","subtitle":"Fueling Performance Testing and Ensuring Fair Distribution of Workload","date":"March 29th, 2024","author":"Chandravijay Rai","thumbnail":{"extension":"webp","publicURL":"/static/c3be7efc7f24f8a837138874bf897435/MeshMap.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRqAAAABXRUJQVlA4WAoAAAAQAAAAEwAACwAAQUxQSBIAAAABD9D/iAiIBZOd+WNHENH/oAFWUDggaAAAABAEAJ0BKhQADAA+0VSjS6gkoyGwCAEAGgllAEP/AWLruGiPscbozwoAAP7xPXb+Ikj+pfVRRa1AJAkUQq8rTAJP81uiMxpOR/FJ8FetwglLfCVUQrRzAKlGILguQEtAJp/xkOgc0SAA"},"images":{"fallback":{"src":"/static/c3be7efc7f24f8a837138874bf897435/f7592/MeshMap.webp","srcSet":"/static/c3be7efc7f24f8a837138874bf897435/bf0cd/MeshMap.webp 750w,\n/static/c3be7efc7f24f8a837138874bf897435/15fc6/MeshMap.webp 1080w,\n/static/c3be7efc7f24f8a837138874bf897435/2e603/MeshMap.webp 1366w,\n/static/c3be7efc7f24f8a837138874bf897435/f7592/MeshMap.webp 1920w","sizes":"100vw"},"sources":[]},"width":1,"height":0.5807291666666666}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/c3be7efc7f24f8a837138874bf897435/MeshMap.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRqAAAABXRUJQVlA4WAoAAAAQAAAAEwAACwAAQUxQSBIAAAABD9D/iAiIBZOd+WNHENH/oAFWUDggaAAAABAEAJ0BKhQADAA+0VSjS6gkoyGwCAEAGgllAEP/AWLruGiPscbozwoAAP7xPXb+Ikj+pfVRRa1AJAkUQq8rTAJP81uiMxpOR/FJ8FetwglLfCVUQrRzAKlGILguQEtAJp/xkOgc0SAA"},"images":{"fallback":{"src":"/static/c3be7efc7f24f8a837138874bf897435/c2056/MeshMap.webp","srcSet":"/static/c3be7efc7f24f8a837138874bf897435/357be/MeshMap.webp 125w,\n/static/c3be7efc7f24f8a837138874bf897435/5c3ae/MeshMap.webp 250w,\n/static/c3be7efc7f24f8a837138874bf897435/c2056/MeshMap.webp 500w,\n/static/c3be7efc7f24f8a837138874bf897435/586cb/MeshMap.webp 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":290}}}},"fields":{"slug":"/blog/kubernetes/load-generation-and-load-balancing"}},{"id":"adc38a61-abc3-5bb4-8fae-413de18161ac","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\nimport Button from \"../../../../reusecore/Button\";\n\n<BlogWrapper>\nAs the final Kubernetes release of 2023, Kubernetes 1.29 is a <a href=\"https://www.kubernetes.dev/resources/release/\">new release</a> of the popular container orchestration platform. It offers a number of new features and improvements that will help platform engineers and DevOps engineers manage their Kubernetes clusters more effectively. Here are some of the highlights of this release.\n<div className=\"intro\">\n      As a longstanding CNCF member, Layer5 has donated two of its open source projects to the CNCF: <Link to=\"/cloud-native-management/meshery\">Meshery</Link> and <Link to=\"/projects/cloud-native-performance\">Cloud Native Performance</Link>. As an end-to-end, open-source, multi-cluster Kubernetes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you.\n</div>\n\nWhile there are a number of enhancements tracked in this release (72), you need to be aware that a number of these are code freezes or deprecations in 1.29. In this article, we will focus on some highlighted enhancements, and important deprecations, so that you can be confident before upgrading your clusters. \n\nLet's breakdown new K8s features between networking and security categories.\n\n## Networking in Kubernetes 1.29\n\n### [Gateway API reaches 1.0](https://gateway-api.sigs.k8s.io) [Stable] \n\nThe Gateway API has reached v1.0 in between Kubernetes 1.28 and 1.29. The Gateway API is the eventual successor to the Ingress API and significantly augments the Service API. The Kubernetes Service API is one of the oldest and most used APIs in Kubernetes. The Service API, in fact, was present in the very first public commit of Kubernetes in 2014, and almost everyone who deploys workloads in Kubernetes ends up using Services. Nearly ten years later in 2024, While Service API offers a kitchen sink full of functionality, it‚Äôs nearly ten year old design is limiting what can be done in Kubernetes networking.\n\nThe Gateway API takes a fresh approach in offering expressive, extensible, and role-oriented interfaces. Gateway API is expressive and extensible in that it offers core support for advanced traffic management (like traffic weighting, header-based matching) that is only possible through customizations in Ingress API. Gateway API is role-oriented in that it defines three main roles: infrastructure provider, cluster operator, and application developer, who need to work together to use and configure Kubernetes service networking. \n\n\n### [Sidecar Containers](https://github.com/kubernetes/enhancements/issues/753) [Beta]\n\nThe new sidecar feature enables restartable init containers and is available in alpha in Kubernetes 1.28. The concept of a ‚Äúsidecar‚Äù has been part of Kubernetes since nearly the very beginning. Sidecar containers have become a common Kubernetes deployment pattern and are often used for network proxies or as part of a logging system. Until now, sidecars were a concept that Kubernetes users applied without native support. The lack of native support has caused some usage friction, which this enhancement aims to resolve.\n\nThe new feature adds a new restartPolicy field to init containers that is available when the SidecarContainers feature gate is enabled. The field is optional and, if set, the only valid value is Always. Setting this field changes the behavior of init containers as follows: The container restarts if it exits. Any subsequent init container starts immediately after the startupProbe has successfully completed instead of waiting for the restartable init container to exit. The resource usage calculation changes for the pod as restartable init container resources are now added to the sum of the resource requests by the main containers. Pod termination continues to only depend on the main containers. An init container with a restartPolicy of Always (named a sidecar) won‚Äôt prevent the pod from terminating after the main containers exit.\n\nBeginning in Kubernetes 1.29, if your Pod includes one or more sidecar containers (init containers with an Always restart policy), the kubelet will delay sending the TERM signal to these sidecar containers until the last main container has fully terminated. The sidecar containers will be terminated in the reverse order they are defined in the Pod spec. This ensures that sidecar containers continue serving the other containers in the Pod until they are no longer needed.\n\nThe behavior of this beta capability in 1.29 is that sidecar containers that have a `PreStop` hook will be notified when the Pod has begun terminating by executing the `PreStop` hook. Once the last primary container terminates, the last started sidecar container is notified by sending a `SIGTERM` signal. The next sidecar (in reverse order) is notified by sending a `SIGTERM` signal after the previous sidecar container terminates. This continues until all sidecar containers have terminated, or the Pod‚Äôs termination grace period expires. In the latter case, all remaining containers are notified by a `SIGTERM`, followed by a fixed grace period of 2 seconds and finally terminated. The Pod will be terminated after that.\n\nNote that slow termination of a main container will also delay the termination of the sidecar containers. If the grace period expires before the termination process is complete, the Pod may enter emergency termination. In this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.\n\nSimilarly, if the Pod has a `preStop` hook that exceeds the termination grace period, emergency termination may occur. In general, if you have used `preStop` hooks to control the termination order without sidecar containers, you can now remove them and allow the kubelet to manage sidecar termination automatically.\n\n### [Transition SPDY to WebSockets](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4006-transition-spdy-to-websockets) [Alpha]\n\nThis change aims to deprecate SPDY in favor of WebSockets for Kubernetes API server communications. WebSockets provide a more modern and scalable protocol that can improve the overall reliability and maintainability of Kubernetes communications. This can enhance security by making sure that the communication protocols used by Kubernetes are robust and well-supported.\n\nThese general enhancements contribute to a more secure and efficient Kubernetes environment. By improving Kubernetes‚Äô underlying reliability, performance, and operational control, the above features lay the groundwork for a more secure infrastructure platform capable of hosting sensitive and critical workloads.\n\n## Security in Kubernetes 1.29\n\n### [Ensure Secret Pulled Images](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2535-ensure-secret-pulled-images) [Alpha]\n\nContainer images often contain sensitive components, making the security of image pull operations critical. The new alpha feature makes sure that images are always pulled using Kubernetes secrets of the Pod using them. This is important when Kubelet pulls an image for one Pod and another Pod is pointing to the same image. Up until now, the second Pod‚Äôs image request did not need authentication since it still used the first one‚Äôs credentials. This could lead to unauthorized access. By securing the image pull process, this enhancement prevents attackers from intercepting or tampering with container images, which is vital for maintaining the integrity of workloads.\n\n### [SignedSigning Release Artifacts](https://github.com/kubernetes/enhancements/issues/3031) [Beta]\n\nEvery Kubernetes release produces a set of artifacts such as binaries, container images, documentation, and metadata. Since the 1.24 release, the artifacts have been signed as an alpha feature. In the 1.26 release, artifact signing graduates to beta to increase software supply chain security for the Kubernetes release process and mitigate man-in-the-middle attacks.\n\n### [Reduction of Secret-Based Service Account Tokens](https://github.com/kubernetes/enhancements/issues/2799) [Beta]\n\n`BoundServiceAccountTokenVolume` has been GA since version 1.22: Service account tokens for pods are obtained via the TokenRequest API and stored as a projected volume. The new enhancement, in beta, eliminates the need to auto-generate secret-based service account tokens. In addition, Kubernetes will warn about using auto-created secret-based service account tokens, and purge the unused ones.\n\n### [Reduction of Secret-Based Service Account Tokens](https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/2799-reduction-of-secret-based-service-account-token) [Beta]\n\nIn addition to ‚ÄúBound Service Account Token Improvements‚Äù enhancement and narrowing the scope of service account tokens, this improvement seeks to reduce the reliance on long-lived secret-based service account tokens. By limiting the use of these tokens, the potential attack surface is significantly diminished. \n\nThis move aligns with the broader industry trend of short-lived, just-in-time credentials that minimize the chances of token leakage leading to unauthorized access.\n\n### [Structured Authentication Configuration](https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3221-structured-authorization-configuration) [Alpha]\n\nSimilar to the authorization counterpart, Kubernetes 1.29 provides another alpha feature for structured configuration for authentication mechanisms. This enhancement offers users a more maintainable and secure approach to managing authentication, allowing administrators to implement complex authentication schemes more efficiently and with less room for error.\n\nWith the new enhancement, it is now possible to configure multiple OIDC providers, clients, and validation rules:\n\n```yaml\napiVersion: apiserver.config.k8s.io/v1alpha1\nkind: AuthenticationConfiguration\njwt:\n- issuer:\n  claimValidationRules:\n  ...\n  - expression: 'claims.exp - claims.nbf <= 86400'\n    message: total token lifetime must not exceed 24 hours\n  claimMappings:\n    username:\n      expression: 'claims.username + \":external-user\"'\n    groups:\n      expression: 'claims.roles.split(\",\")'\n       ‚Ä¶\n  userValidationRules:\n  - expression: \"!user.username.startsWith('system:')\"\n    message: username cannot used reserved system: prefix\n  - expression: \"user.groups.all(group, !group.startsWith('system:'))\"\n    message: groups cannot used reserved system: prefix\n```\n\n## Last Kubernetes release of 2023\n\nKubernetes is an ever-evolving platform. For those of you running workloads on Kubernetes taking detailed note of API changes and enhancements is an important activity as you endeavour to keep your clusters upgraded with release releases. A more secure, scalable, and flexible Kubernetes is our collective goal. Dign into more details about deprecation, removals, and the latest changes in the 1.29 [release notes](https://relnotes.k8s.io/).\n\nOn behalf of the Layer5 community and all of the CNCF projects that its contributors steward, thank you to everyone who participated in this Kubernetes release, and congratulations! \n\nAs an end-to-end, open-source, multi-cluster Kuberentes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you. \n\n## What's ahead for Kubernetes in 2024?\n\n### AI/MLOps\n\nGenerative AI came into the world with a big splash this year and helmsman, Kubernetes, is here helping navigate the AI waters. While some people suggest that the CNCF and Kubernetes ecosystem is in catch up mode in context of GenAI, they perhaps, forget that OpenAI is running a 7,500 node Kubernetes cluster in the development and hosting of GPT-3, CLIP, DALL¬∑E, and research on other models. Where I think that Kubernetes and the CNCF can improve is in embracing and enabling AI/ML as a forerunner of our workload use cases. The CNCF has a massive developer ecosystem that will that can be enabled with MLOps and new model training projects like Kubeflow with its Kartib and KFServing sub-projects.\n\n### Looking past YAML\n\nDZone‚Äôs annual trend report on Kubernetes in the Enterprise has 55% of respondents identifying \"Maintaining YAML files‚Äù as one of the more significant, ongoing pain-points in their experience with Kubernetes. In 2024, I‚Äôm looking forward to CNCF projects like Meshery addressing this pain-point with [a visual designer for Kubernetes](/cloud-native-management/meshmap) resource and workflow configuration. The inability for some developers to either be capable of or have time to comprehend the intricacies of their virtualized applications by staring at endless lines of YAML is poised to significantly improve in 2024. \n\n</BlogWrapper>","frontmatter":{"title":"Kubernetes 1.29 Highlights, Features, and Deprecations","subtitle":null,"date":"December 13th, 2023","author":"Lee Calcote","thumbnail":{"extension":"webp","publicURL":"/static/a4d33a4caec76921f0279be3c71b48ae/kubernetes-new.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRowBAABXRUJQVlA4WAoAAAAQAAAAEwAADwAAQUxQSIUAAAABgJpt27Lsxrov4NolEqE538EGZF3Bq9sEPoA1GjR2sOby/59/zwoRMQGQDM7uIw8UqyIblmXZv3lITpM9sPLbqFSZVZ6SWBH6It2NcBQFWOK/BUCewsYAzEhNACfSDsCeNDH6ndYz5cuyZ4Q/BJZlK0COdNUDaFHyAKBbfiU/Y4ZhmLgOAFZQOCDgAAAAcAUAnQEqFAAQAD7RVKNLqCSjIbAIAQAaCWwAsRtd6q4sAI8D2bIvNVvcVSZVMe7y5rvUAAD+1O1N+i568I+jjvex6S/RDVp+numDA7ovdqcUfNb8t7kk4/V5tU25zIiKFORSqU70tZLf9xmyH3hJh4vgFKeKrquj0fsFOsVq5Hn3kNDALJfeXRpHRD5t76jpyAaXHjulVIwln2v5wKtC0x5+0pzU2nQ6fo/vWo1BjacDMf7RxLgdF40ulzIG4zldtqdxhJrTZG+Q8ihOJoH+dP+Tv9snTP/8Fhff7nJk+AA="},"images":{"fallback":{"src":"/static/a4d33a4caec76921f0279be3c71b48ae/7826a/kubernetes-new.webp","srcSet":"/static/a4d33a4caec76921f0279be3c71b48ae/d331c/kubernetes-new.webp 750w,\n/static/a4d33a4caec76921f0279be3c71b48ae/8b4a5/kubernetes-new.webp 1080w,\n/static/a4d33a4caec76921f0279be3c71b48ae/7826a/kubernetes-new.webp 1334w","sizes":"100vw"},"sources":[]},"width":1,"height":0.8193403298350824}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/a4d33a4caec76921f0279be3c71b48ae/kubernetes-new-dark.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRowBAABXRUJQVlA4WAoAAAAQAAAAEwAADwAAQUxQSIUAAAABgJpt27Lsxrov4NolEqE538EGZF3Bq9sEPoA1GjR2sOby/59/zwoRMQGQDM7uIw8UqyIblmXZv3lITpM9sPLbqFSZVZ6SWBH6It2NcBQFWOK/BUCewsYAzEhNACfSDsCeNDH6ndYz5cuyZ4Q/BJZlK0COdNUDaFHyAKBbfiU/Y4ZhmLgOAFZQOCDgAAAAcAUAnQEqFAAQAD7RVKNLqCSjIbAIAQAaCWwAsRtd6q4sAI8D2bIvNVvcVSZVMe7y5rvUAAD+1O1N+i568I+jjvex6S/RDVp+numDA7ovdqcUfNb8t7kk4/V5tU25zIiKFORSqU70tZLf9xmyH3hJh4vgFKeKrquj0fsFOsVq5Hn3kNDALJfeXRpHRD5t76jpyAaXHjulVIwln2v5wKtC0x5+0pzU2nQ6fo/vWo1BjacDMf7RxLgdF40ulzIG4zldtqdxhJrTZG+Q8ihOJoH+dP+Tv9snTP/8Fhff7nJk+AA="},"images":{"fallback":{"src":"/static/a4d33a4caec76921f0279be3c71b48ae/73210/kubernetes-new-dark.webp","srcSet":"/static/a4d33a4caec76921f0279be3c71b48ae/6aeef/kubernetes-new-dark.webp 125w,\n/static/a4d33a4caec76921f0279be3c71b48ae/fc268/kubernetes-new-dark.webp 250w,\n/static/a4d33a4caec76921f0279be3c71b48ae/73210/kubernetes-new-dark.webp 500w,\n/static/a4d33a4caec76921f0279be3c71b48ae/e87bc/kubernetes-new-dark.webp 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":410}}}},"fields":{"slug":"/blog/kubernetes/kubernetes-129-highlights-features-and-deprecations"}},{"id":"8506be5c-8c38-5051-9aa1-efa2e5635e60","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\nimport Button from \"../../../../reusecore/Button\";\nimport Blockquote from \"../../../../reusecore/Blockquote\";\n\n<BlogWrapper>\n\nKubernetes provides a Service to offer a unified traffic endpoint for the Pods. While it offers a VIP to the clients for access and Kubernetes ensures traffic balancing for the accessing back-end Pods, it has a limitation of routing traffic from outside the cluster. The Kubernetes Service setting of \"NodePort\" was created to overcome this issue. \n\nBy setting up a mapping to a specific port of all nodes in the cluster, a NodePort Service redirects traffic from the outside to the inside of the cluster. When a NodePort Service is created, Kubernetes control plane allocates its corresponding ports in two ways. The first is dynamic, where Kubernetes control plane automatically assigns an unused port at the creation time. The second is static, which assigns a port within the nodeport port range configuration. It is crucial to assign a unique nodePort across the entire cluster while manually assigning nodePort, or it will result in an error if a service of type NodePort already uses that port. \n\nSometimes, there is a need to run a NodePort Service on well-known ports so that other components and users inside or outside the cluster can use them. In such cases, users need to reserve the required ports before using them. Kubernetes 1.27 introduced a new feature gate \"ServiceNodePortStaticSubrange\" that allows users to use a different port allocation strategy for type NodePort Services. Enabling this feature gate will divide the port range for NodePort Services based on a formula that uses nodeport size and determines the size of the static port range.\n\nHere are a few examples of different port ranges and their band offset values:\n<div className=\"table-3\">\n\n| Range properties | Values |\n| --- | --- |\n| service-node-port-range | 30000-32767 |\n| Band Offset | 86 |\n| Static band start | 30000 |\n| Static band end | 30085 |\n| Dynamic band start | 30086 |\n| Dynamic band end | 32767 |\n\n</div>\n<br />\n<div className=\"table-3\">\n\n| Range properties | Values |\n| --- | --- |\n| service-node-port-range | 30000-30015 |\n| Band Offset | 16 |\n| Static band start | 30000 |\n| Static band end | 30015 |\n| Dynamic band start | N/A |\n| Dynamic band end | N/A |\n\n</div>\n\nNodePort Services can be useful in many scenarios. For example, consider a user that needs to expose a Minio object storage service on Kubernetes to clients running outside the Kubernetes cluster. The agreed port is 30009, and the user needs to create a Service as follows:\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio\nspec:\n  ports:\n  - name: api\n    nodePort: 30009\n    port: 9000\n    protocol: TCP\n    targetPort: 9000\n  selector:\n    app: minio\n  type: NodePort\n```\nIf the port required for the Minio Service is not reserved and another NodePort (or possibly LoadBalancer) Service is created and dynamically allocated before or concurrently with the Minio Service, the TCP port 30009 might be allocated to that other Service. In this case, creation of the Minio Service will fail due to a node port collision. \n\nIn conclusion, using the NodePort Service will help Kubernetes users by allowing traffic to be routed from outside to inside the cluster, providing a unified traffic endpoint for the Pods. By enabling the ServiceNodePortStaticSubrange feature gate, users can adopt a different port allocation strategy, reducing the risk of collisions while using a different range of ports.\n\n</BlogWrapper>","frontmatter":{"title":"Kubernetes NodePorts - Static and Dynamic Assignments","subtitle":"Avoiding Port Collisions","date":"May 12th, 2023","author":"Lee Calcote","thumbnail":{"extension":"webp","publicURL":"/static/7ff62eaeba27512aa31c04535a060e72/k8s-nodeports.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRr4AAABXRUJQVlA4ILIAAABQBACdASoUABQAPtFcpU6oJSMiKAqpABoJagCdAA7o/xI+daMhu4ZyKIc8AP5sMkrm/KSMElhNhyFx1gsn7iAKtDbnb2EOpN1Lu4ERxHR8dZzo2nLfaBjTkL7HUoWJ2ZeN3kbroyt5qSiHJrjn1s8R/0YvwxgPzzQOJ6Z6/jny3fK+Q/p/RG8WRYpz7F7kTd49j//OPWbkB/vQwHCyP2a8pf47eyzW5wuh8SG+Fee1AAAA"},"images":{"fallback":{"src":"/static/7ff62eaeba27512aa31c04535a060e72/c1587/k8s-nodeports.webp","srcSet":"/static/7ff62eaeba27512aa31c04535a060e72/4f03f/k8s-nodeports.webp 750w,\n/static/7ff62eaeba27512aa31c04535a060e72/c1587/k8s-nodeports.webp 800w","sizes":"100vw"},"sources":[]},"width":1,"height":1}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/7ff62eaeba27512aa31c04535a060e72/k8s-nodeports.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRr4AAABXRUJQVlA4ILIAAABQBACdASoUABQAPtFcpU6oJSMiKAqpABoJagCdAA7o/xI+daMhu4ZyKIc8AP5sMkrm/KSMElhNhyFx1gsn7iAKtDbnb2EOpN1Lu4ERxHR8dZzo2nLfaBjTkL7HUoWJ2ZeN3kbroyt5qSiHJrjn1s8R/0YvwxgPzzQOJ6Z6/jny3fK+Q/p/RG8WRYpz7F7kTd49j//OPWbkB/vQwHCyP2a8pf47eyzW5wuh8SG+Fee1AAAA"},"images":{"fallback":{"src":"/static/7ff62eaeba27512aa31c04535a060e72/5f169/k8s-nodeports.webp","srcSet":"/static/7ff62eaeba27512aa31c04535a060e72/d66e1/k8s-nodeports.webp 125w,\n/static/7ff62eaeba27512aa31c04535a060e72/e7160/k8s-nodeports.webp 250w,\n/static/7ff62eaeba27512aa31c04535a060e72/5f169/k8s-nodeports.webp 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":500}}}},"fields":{"slug":"/blog/kubernetes/kubernetes-nodeports-static-and-dynamic-assignments"}},{"id":"9c8aba10-84c0-5025-887b-010e9cbc0667","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\nimport Button from \"../../../../reusecore/Button\";\n\n<BlogWrapper>\n\nKubernetes is an open-source container orchestration system for automating the deployment, scaling, and management of containerized applications. As part of its functionality, Kubernetes offers a feature called \"Admission Controllers\" that allow administrators to enforce certain policies on resources being created in the cluster.\n\nIn this blog post, we will be discussing a new feature in Kubernetes called \"Validating Admission Policies\" which is currently in alpha stage. This feature allows administrators to define custom validation rules for resources being created in the cluster and enforce those rules using admission controllers.\n\n## What are Admission Controllers?\n\nAdmission controllers are pluggable components in the Kubernetes API server that intercept requests to create, update, or delete resources in the cluster. They allow administrators to enforce certain policies on these requests before they are persisted in the etcd database and acted upon by the Kubernetes control plane.\n\nThere are various types of admission controllers available in Kubernetes, such as:\n\n- **NamespaceLifecycle**: This admission controller enforces policies related to namespace creation and deletion.\n- **LimitRanger**: This admission controller enforces resource limits on pods, such as CPU and memory limits.\n- **PodSecurityPolicy**: This admission controller enforces security policies on pods, such as privileged mode, host networking, and volumes.\n\n## Validating Admission Policies\n\nValidating admission policies allow administrators to define custom validation rules for resources being created in the cluster. These rules can be defined using a custom resource definition (CRD) called \"ValidatingWebhookConfiguration\" and are enforced by the ValidatingAdmissionWebhook admission controller.\n\nFor example, an administrator may want to enforce a policy that requires all pods in the cluster to have a specific label. They can define this rule using a ValidatingWebhookConfiguration CRD and configure the ValidatingAdmissionWebhook admission controller to enforce it. Any request to create a pod that does not have the required label will be rejected by the admission controller.\n\nValidating admission policies also allow administrators to use external webhooks to perform the validation. This can be useful when the validation logic is complex or requires access to external resources.\n\n## Conclusion\n\nValidating admission policies is a new feature in Kubernetes that allows administrators to define custom validation rules for resources being created in the cluster. These rules can be enforced using the ValidatingAdmissionWebhook admission controller, and external webhooks can also be used for complex validation logic. This feature can be useful for enforcing policies and ensuring compliance in a Kubernetes cluster.\n\n</BlogWrapper>","frontmatter":{"title":"What are Kubernetes Validating Admission Controllers?","subtitle":"A close cousin to Validating Admission Webhooks","date":"February 3rd, 2023","author":"Lee Calcote","thumbnail":{"extension":"webp","publicURL":"/static/0cda4651daf491c6b40dd404799ca32c/kubernetes-new.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRpwAAABXRUJQVlA4IJAAAABQBACdASoUABQAPtFgqE+oJSOiKAgBABoJbADBzYrIcLaTwo9+PMnGQagAAP72Av+XnXpDovXZeeq4tTUzLytHh8EOuq1sofHN2IthfXcoC9jFvtZhVo+tDrkfaFYPTkFWBWNyondfXcOX/mOVgrxqRCV7kvjREw4wZJCrfL9qeXzh9e/A/AD1++Ek5WwAAAA="},"images":{"fallback":{"src":"/static/0cda4651daf491c6b40dd404799ca32c/3987a/kubernetes-new.webp","srcSet":"/static/0cda4651daf491c6b40dd404799ca32c/4f03f/kubernetes-new.webp 750w,\n/static/0cda4651daf491c6b40dd404799ca32c/3987a/kubernetes-new.webp 900w","sizes":"100vw"},"sources":[]},"width":1,"height":1}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/4fe429ff2237fa91df4168525bb25419/kubernetes-new-dark.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRnYBAABXRUJQVlA4WAoAAAAQAAAAEwAAEwAAQUxQSLAAAAABgCMAAOHmYpuT+YPkBckH7C1pf9Cp3TzZ7WTb7mTbdie7xzgPiIgJAFI1z5uhikYcXPkDLwoMuEisW//g9w4I3uWRw+GHvq+P22vn3u6v1eGkh+B09c3seN3vnzmc8QPs2S8sHRr4AjPDpYMP1a1lDfVV279D+DDZ4N3kZX3bTfvj9xwrjGwB/O3oWO94AsHzvWcLBmC6ACO8pmMB/kjSgLDErLKysrKigMdjJ4ZLqVZQOCCgAAAAUAQAnQEqFAAUAD7RXqhPqCSjoigIAQAaCWwAsQWgJYz3ksiGQV2/pilkAADsw+6hq/cA70EZQnxQiDQur0nITK3Vk/X0a4mvrPL9VqKWuo91tPOP2mzL3NrOliPXZ4F6vkz5Oquym+BBBVS4AM9mRrF3dykeG8Q5yHw80ozivZetdD37I5//8WlnKn5Zb4/XAc7vyMlQ8r7aCdP07KxAAA=="},"images":{"fallback":{"src":"/static/4fe429ff2237fa91df4168525bb25419/5f169/kubernetes-new-dark.webp","srcSet":"/static/4fe429ff2237fa91df4168525bb25419/d66e1/kubernetes-new-dark.webp 125w,\n/static/4fe429ff2237fa91df4168525bb25419/e7160/kubernetes-new-dark.webp 250w,\n/static/4fe429ff2237fa91df4168525bb25419/5f169/kubernetes-new-dark.webp 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":500}}}},"fields":{"slug":"/blog/kubernetes/what-are-kubernetes-validating-admission-controllers"}},{"id":"6a0f849a-8fcf-586a-9cf3-0beaac085455","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\nimport Button from \"../../../../reusecore/Button\";\n\n<BlogWrapper>\nAs the final Kubernetes release of 2022, Kubernetes 1.26 is an exciting new release of the popular container orchestration platform. It offers a number of new features and improvements that will help platform engineers and DevOps engineers manage their Kubernetes clusters more effectively. Here are some of the highlights of this release.\n<div className=\"intro\">\n      As a longstanding CNCF member, Layer5 has donated two of its open source projects to the CNCF: <Link to=\"/cloud-native-management/meshery\">Meshery</Link> and <Link to=\"/projects/cloud-native-performance\">Service Mesh Performance</Link>. As an end-to-end, open-source, multi-cluster Kuberentes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you.\n</div>\n\nWhile there are a number of enhancments tracked in this release (38), you need to be aware that there are also a number of features being deprecated (10) in 1.26. In this article, we will focus on some highlighted enhancements, important deprecations, and removals so that you can be confident before upgrading your clusters. \n\nWe'll breakdown new K8s features by category, starting with networking.\n\n## Networking in Kubernetes 1.26\n\n### [Service Internal Traffic Policy](https://github.com/kubernetes/enhancements/issues/2086) [Stable]\n\nWhen requests are made to a Kubernetes service, they are randomly distributed to all available endpoints. The new enhancement enriches the API of a service to use node-local and topology-aware routing for internal traffic. The new internalTrafficPolicy field has two options: Cluster (default) and Local. The Cluster option works like before and tries distributing requests to all available endpoints. On the other hand, the Local option only sends requests to node-local endpoints and drops the request if there is no available instance on the same node. The Local option is useful for sending metrics or logs to an agent running as a DaemonSet. \n\n### [Reserve Service IP Ranges for Dynamic and Static IP Allocation](https://github.com/kubernetes/enhancements/issues/3070) [Stable]\n\nKubernetes services are assigned a virtual ClusterIP to be reachable inside the cluster. The ClusterIP is either assigned dynamically from a configured Service IP range, or statically set while creating the service resource. There was no possibility of knowing whether another service in the cluster had already used the static ClusterIP before this new stable enhancement. With this change, the IP range is divided into two; this prevents conflicts between services implementing dynamic IP allocation and static IP assignment. The flag --service-cluster-ip-range, with CIDR notation, is part of the Kubernetes API server configuration and is ready to use with the 1.26 release. \n\n### [Support of Mixed Protocols in Services with Type LoadBalancer](https://github.com/kubernetes/enhancements/issues/1435) [Stable]\n\nKubernetes Services that use the LoadBalancer type have only supported a single Layer 4 protocol until now. With this enhancement going from graduating to stable in v1.26, it is possible to define a mix of protocols in the same service definition. In other words, this enhancement allows a LoadBalancer Service to serve different protocols (e.g. UDP, TCP) under the same port (e.g. 443). For example, serving both UDP and TCP requests for a DNS or SIP server on the same port. For instance, you can expose a DNS server with a single load balancer IP for both TCP and UDP requests, such as the following:\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: multi-protocol-dns-server\nspec:\n  type: LoadBalancer\n  ports:\n    - name: dns-udp\n      port: 53\n      protocol: UDP\n    - name: dns-tcp\n      port: 53\n      protocol: TCP\n  selector:\n    app: dns-server\n```\n\n## Security in Kubernetes 1.26\n\n### [kubelet Credential Provider](https://github.com/kubernetes/enhancements/issues/2133) [Stable]\n\nThe kubelet agent has a built-in credential provider mechanism to retrieve credentials for container image registries. It natively supports Azure, Google Cloud, and AWS container image registries for dynamically retrieving their credentials. The new stable enhancement in v1.26 offers a replacement for the in-tree implementations, and creates an API for extensible plugins in the future. \n\n### [SignedSigning Release Artifacts](https://github.com/kubernetes/enhancements/issues/3031) [Beta]\n\nEvery Kubernetes release produces a set of artifacts such as binaries, container images, documentation, and metadata. Since the 1.24 release, the artifacts have been signed as an alpha feature. In the 1.26 release, artifact signing graduates to beta to increase software supply chain security for the Kubernetes release process and mitigate man-in-the-middle attacks.\n\n### [Reduction of Secret-Based Service Account Tokens](https://github.com/kubernetes/enhancements/issues/2799) [Beta]\n\n`BoundServiceAccountTokenVolume` has been GA since version 1.22: Service account tokens for pods are obtained via the TokenRequest API and stored as a projected volume. The new enhancement, in beta, eliminates the need to auto-generate secret-based service account tokens. In addition, Kubernetes will warn about using auto-created secret-based service account tokens, and purge the unused ones.\n\n### [Windows Privileged Containers](https://github.com/kubernetes/enhancements/issues/1981) [Stable] and [Host Networking](https://github.com/kubernetes/enhancements/issues/3503) [Alpha] \n\nPrivileged containers are the ones that have similar access and capabilities to the host processes running on the servers. In Linux environments, they are used heavily in Kubernetes for storage, networking, and management. In this release, support for privileged containers for the Windows environment graduates to stable. Management of processes is heavily different from the operating system standpoint in Linux and Windows. Therefore, privileged containers will also work differently in two environments, but they will ensure the same level of security and operational experience.\n\nIn addition, there is a new alpha-level enhancement in this release to support host networking for Windows pods. Currently, Windows has all the functionality to make containers use the networking namespace of the nodes. The new alpha enhancement enables this functionality from the Kubernetes side, increasing the parity between Linux and Windows containers.\n\n### [Self-User Attribute and Authentication API](https://github.com/kubernetes/enhancements/issues/33250) [Alpha] \n\nKubernetes has no resources to identify and manage users as part of its API. Instead, it uses authenticators to get user attributes from tokens, certificates, OIDC providers, or webhooks. The new alpha feature adds a new API endpoint to see what attributes the current users have. The new API is under authentication.k8s.io with the name SelfSubjectReview, and there is a new corresponding command as well: kubectl auth who-am-i. The new feature will reduce the obscurity of complex authentication and help users debug the authentication stack. \n\n## Scheduling in Kubernetes 1.26\n\n### [Non-Graceful Node Shutdown for StatefulSet Pods](https://github.com/kubernetes/enhancements/issues/2268) [Beta]\n\nAs a platform Kubernetes is hardened and has been deploy by thousands and thousands of users. Hardening of Kubernetes makes itself resistant to disasters. The kubelet agent that runs on each node in a Kubernetes cluster already uses graceful node shutdown to detect and offboard workloads to other nodes. However, when the shutdown is not detected by the kubelet, the pods of a `StatefulSet` are stuck as `Terminating` and not transferred to a healthy node. The kubelet on the downed node will not delete its pods from Kubernetes API, and the StatefulSet controller will not create new pods with the same name. This happens due to a conflict in the Kubernetes machinery. With this enhancement moving into beta, though, pods will be forcefully deleted along with their volume attachments and new pods will be migrated (created) on healthy nodes.\n\n### [Pod Scheduling Readiness](https://github.com/kubernetes/enhancements/issues/3521) [Alpha]\n\nCurrently, pods are considered ready for scheduling as soon as they are created. However, not every pod requires a node, resource allocation, and the start of all its containers immediately after its creation. The new alpha enhancement adds an API to mark pods with their scheduling status: paused and ready. Pods with the .spec.schedulingGates field will be parked in the scheduler and only be assigned to nodes when they are ready to be scheduled.\n\n### [kubectl explain to use OpenAPI v3 for ](https://github.com/kubernetes/enhancements/issues/3515) [Alpha]\n\nUse of OpenAPI v3 means supporting rich type information in `kubectl explan`. Kubernetes has supported OpenAPI v3 as a beta since version 1.24. This richer representation of the fields in the Kubernetes API, means that users can use the `kubectl explain` command to get information that is only detailed in  OpenAPI v3, and not the subset defined OpenAPI v2.\n\n## Deprecations and Removals\n\nConsistent to the Kubernetes API lifecycle is deprecations and removals of APIs in each release. It is strongly suggested to check whether you are using the following APIs and flags before there are breaking changes.\n\n<ul>\n<li>\n  Removal of the `flowcontrol.apiserver.k8s.io/v1beta1` API group for `FlowSchema` and `PriorityLevelConfiguration` requires a migration to the v1beta2 API version.\n</li>\n<li>\n  Removal of the `autoscaling/v2beta2` API version for HorizontalPodAutoscaler requires a migration to the autoscaling/v2 API version. \n</li>\n<li>\n  Removal of legacy and vendor-specific authentication client-go and kubectl for Azure and Google Cloud requires migration to vendor-neutral authentication plugin mechanisms.\n</li>\n<li>\n  Removal of in-tree CSI integration for OpenStack‚Äînamely, the `cinder` volume type‚Äîrequires a migration to use the CSI driver for OpenStack. \n</li>\n<li>\n  Some unused options and flags for the kubectl run command are marked as deprecated in the 1.26 release, such as `--grace-period`, `--timeout`, and `--wait`.\n</li>\n</ul>\n\n## Last Kubernetes release of 2022\n\nKubernetes is an ever-evolving platform. For those of you running workloads on Kubernetes taking detailed note of API changes and enhancements is an important activity as you endevour to keep your clusters upgraded with release releases. A more secure, scalable, and flexible Kubernetes is our collective goal. Dign into more details about deprecation, removals, and the latest changes in the 1.26 [release notes](https://relnotes.k8s.io/).\n\nOn behalf of the Layer5 community and all of the CNCF projects that its contributors steward, thank you to everyone who participated in this Kubernetes release, and congratulations! \n\nAs an end-to-end, open-source, multi-cluster Kuberentes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you. \n\n</BlogWrapper>\n","frontmatter":{"title":"Kubernetes 1.26 Highlights, Features, and Deprecations","subtitle":null,"date":"December 6th, 2022","author":"Lee Calcote","thumbnail":{"extension":"webp","publicURL":"/static/0cda4651daf491c6b40dd404799ca32c/kubernetes-new.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRpwAAABXRUJQVlA4IJAAAABQBACdASoUABQAPtFgqE+oJSOiKAgBABoJbADBzYrIcLaTwo9+PMnGQagAAP72Av+XnXpDovXZeeq4tTUzLytHh8EOuq1sofHN2IthfXcoC9jFvtZhVo+tDrkfaFYPTkFWBWNyondfXcOX/mOVgrxqRCV7kvjREw4wZJCrfL9qeXzh9e/A/AD1++Ek5WwAAAA="},"images":{"fallback":{"src":"/static/0cda4651daf491c6b40dd404799ca32c/3987a/kubernetes-new.webp","srcSet":"/static/0cda4651daf491c6b40dd404799ca32c/4f03f/kubernetes-new.webp 750w,\n/static/0cda4651daf491c6b40dd404799ca32c/3987a/kubernetes-new.webp 900w","sizes":"100vw"},"sources":[]},"width":1,"height":1}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/4fe429ff2237fa91df4168525bb25419/kubernetes-new-dark.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRnYBAABXRUJQVlA4WAoAAAAQAAAAEwAAEwAAQUxQSLAAAAABgCMAAOHmYpuT+YPkBckH7C1pf9Cp3TzZ7WTb7mTbdie7xzgPiIgJAFI1z5uhikYcXPkDLwoMuEisW//g9w4I3uWRw+GHvq+P22vn3u6v1eGkh+B09c3seN3vnzmc8QPs2S8sHRr4AjPDpYMP1a1lDfVV279D+DDZ4N3kZX3bTfvj9xwrjGwB/O3oWO94AsHzvWcLBmC6ACO8pmMB/kjSgLDErLKysrKigMdjJ4ZLqVZQOCCgAAAAUAQAnQEqFAAUAD7RXqhPqCSjoigIAQAaCWwAsQWgJYz3ksiGQV2/pilkAADsw+6hq/cA70EZQnxQiDQur0nITK3Vk/X0a4mvrPL9VqKWuo91tPOP2mzL3NrOliPXZ4F6vkz5Oquym+BBBVS4AM9mRrF3dykeG8Q5yHw80ozivZetdD37I5//8WlnKn5Zb4/XAc7vyMlQ8r7aCdP07KxAAA=="},"images":{"fallback":{"src":"/static/4fe429ff2237fa91df4168525bb25419/5f169/kubernetes-new-dark.webp","srcSet":"/static/4fe429ff2237fa91df4168525bb25419/d66e1/kubernetes-new-dark.webp 125w,\n/static/4fe429ff2237fa91df4168525bb25419/e7160/kubernetes-new-dark.webp 250w,\n/static/4fe429ff2237fa91df4168525bb25419/5f169/kubernetes-new-dark.webp 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":500}}}},"fields":{"slug":"/blog/kubernetes/kubernetes-126-highlights-features-and-deprecations"}},{"id":"ee4ff46e-ae65-555e-835d-ca9902fd8876","body":"\nimport { BlogWrapper } from \"../../Blog.style.js\";\nimport { Link } from \"gatsby\";\nimport Button from \"../../../../reusecore/Button\";\n\n\n<BlogWrapper>\nAs a platform for developers and system administrators to easily deploy and manage applications in a distributed environment, Kubernetes clusters generate logs and lots of them. One of the key components of Kubernetes is its logging and instrumentation capabilities. The upcoming Kubernetes 1.26 release has a handful of noteworthy changes to its system component logger, <code>klog</code>.\n## Kubernetes System Log\n\nSystem component logs record events happening in K8s clusters. More than metrics or traces, logs are the telemetric signal often found to be most useful for debugging. You can configure K8s log verbosity to see more or less detail. Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or scheduler decisions).\n\n## Klog\n\n[klog](https://github.com/kubernetes/klog) is a Kubernetes logging library that provides an API for developers and system administrators to instrument their applications for logging and tracing. klog generates log messages for the Kubernetes system components. It provides a comprehensive set of features, including log levels, structured logging and logging context.\n\nKubernetes 1.23 introduced structured logging (in beta) in `klog`. Structured logging is a uniform structure in log messages allowing for programmatic extraction of information. Structured logs can be stored and processed with less effort and cost. The code which generates a log message determines whether it uses the traditional unstructured `klog` output or structured logging.\n\nAs a dependency to structured logging (gated behind `StructuredLogging` feature gate), Kubernetes 1.24 introducted contextual logging (in alpha) in `klog`. Contextual logging builds on top of structured logging. It is primarily about how developers use logging calls: code based on that concept is more flexible and supports additional use cases which will be the topic of a future blog post. \n\n### Klog Deprecations in Kubernetes 1.26\n\nKubernetes has recently announced that it intends to deprecate certain flags related to Klog in its components. This means that Klog-specific flags, such as `--klog-verbosity`, `--klog-vmodule` and `--klog-stderrthreshold` will no longer be supported. This is due to the fact that Klog has been largely superseded by the more comprehensive OpenTelemetry project, which provides a more complete solution for logging and instrumentation.\n\nThe deprecation of Klog-specific flags is a positive step forward for Kubernetes as it moves to OpenTelemetry. This move will ensure that Kubernetes is using the industry-standard logging and instrumentation solution. It will also provide developers and system administrators with a more comprehensive, reliable and consistent experience when instrumenting their applications.\nA goal of this deprecation is one of unblocking development of alternative logging formats. Why does Kubnernetes need another logging format? One reason is performance. Klog performance is much worse than alternatives, for example 7-8x than JSON format:\n\n|logger                 |time [ns/op]|bytes[B/op]|allocations[alloc/op]|\n|-----------------------|------------|-----------|---------------------|\n|Text Infof             |2252        |248        |3                    |\n|Text InfoS             |2455        |280        |3                    |\n|JSON Infof             |1406        |19         |1                    |\n|JSON InfoS             |319         |67         |1                    |\n\nProof of concept implementation of new logging formats were completed to assess the potentional gains of using an alternative format. Results measured on 30s benchmark for passing 2 arguments to format function.\n\n<div className=\"tip\">\n<h3>Tip: Logger Performance Comparison</h3>\n\nInterestingly, Klog isn't the fastest logger in the West, but Uber's open source project [zap ](https://github.com/uber-go/zap) appears to hold that title instead. The following performance test benchmark is an examle of one of a number of scenarios in which loggers can be performance analyzed.\n\n\nLog a message and 10 fields:\n| Package             |    Time     | Time % to zap | Objects Allocated |\n| :------------------ | :---------: | :-----------: | :---------------: |\n| zap                 | 2900 ns/op  |      +0%      |    5 allocs/op    |\n| zap (sugared)       | 3475 ns/op  |     +20%      |   10 allocs/op    |\n| zerolog             | 10639 ns/op |     +267%     |   32 allocs/op    |\n| go-kit              | 14434 ns/op |     +398%     |   59 allocs/op    |\n| logrus              | 17104 ns/op |     +490%     |   81 allocs/op    |\n| apex/log            | 32424 ns/op |    +1018%     |   66 allocs/op    |\n| log15               | 33579 ns/op |    +1058%     |   76 allocs/op    |\n</div>\n\nOutput will always be written to stderr, regardless of the output format. Output redirection is expected to be handled by the component which invokes a Kubernetes component. This can be a POSIX shell or a tool like systemd.\n\nThe deprecation of Klog-specific flags is part of a larger effort to transition Kubernetes to a more modern and comprehensive logging and instrumentation solution. This will provide Kubernetes users with a more reliable, secure and consistent experience when instrumenting and monitoring their applications.\n\nKubernetes will continue to provide support for Klog-specific flags for the foreseeable future. However, it is recommended that developers and system administrators begin transitioning their applications to the OpenTelemetry framework. This will ensure that their applications are using the industry-standard solution for logging and instrumentation.\n\nOverall, the deprecation of Klog-specific flags is a positive step forward for Kubernetes. It will ensure that Kubernetes users have access to the most reliable and comprehensive solution for logging and instrumentation. It will also help ensure that Kubernetes is using the industry-standard solution and will provide developers and system administrators with a more reliable and consistent experience when instrumenting their applications.\n</BlogWrapper>","frontmatter":{"title":"Structured logging in Kubernetes with Klog","subtitle":"Deprecating Klog flags in Kubernetes 1.26","date":"December 5th, 2022","author":"Lee Calcote","thumbnail":{"extension":"webp","publicURL":"/static/a125a7ad56dafd70886f09d8c408e114/kubernetes-logs.webp","childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/webp;base64,UklGRo4AAABXRUJQVlA4IIIAAAAQBACdASoUABQAPtFYpUyoJSOiKA1RABoJbADE2A9oOHBqhtd/PoCpgAD+9hPef0TEKVio6tt3FOets66Z3rYkj6VKa9nnk5BswW619qW3HimtX50rDTgg73noredVBvmuHNfM92EZ3yl/IPFRzvtyYwEvFfOxgqRkERYrQM26gAAA"},"images":{"fallback":{"src":"/static/a125a7ad56dafd70886f09d8c408e114/3987a/kubernetes-logs.webp","srcSet":"/static/a125a7ad56dafd70886f09d8c408e114/4f03f/kubernetes-logs.webp 750w,\n/static/a125a7ad56dafd70886f09d8c408e114/3987a/kubernetes-logs.webp 900w","sizes":"100vw"},"sources":[]},"width":1,"height":1}}},"darkthumbnail":{"extension":"webp","publicURL":"/static/36fd87e316aac4b3df8f3b1a24a3ffeb/kubernetes-logs-dark.webp","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRmwBAABXRUJQVlA4WAoAAAAQAAAAEwAAEwAAQUxQSLYAAAABgGNtm6rmnxncSt2Hyy7cNkClrbvuACqnooV4d0uquKfkUMYNnbknWUFETAD8a+TMU1M6mqgpleqTSb1Uqs7SJDlQXt/eXitBWGQyzyeTuZkHMiw4bmHgAST89BQmTMZCNlOoVguZhIihvDxdnhzNxr9LFQ2Vl51lu9UePvPgjo8PZqMvFnA0qWkpgziNdxKBmEGSg21y+fnu4iX8+Y0QmrhGMMZY/B0hQMeh8k4fAVaEdyL4S1ZQOCCQAAAA8AMAnQEqFAAUAD7RWqNNKCUjIigNUQAaCWwAvkgQ3sibXv8o78YB8AD+6/YtrSM9Rc5Lz22/yXt8aymW8E1+TIYMBxpYCXm0QnmjTq/+uaLWqy+vQVQ7N8FLsC99nCFhoHFMXPczNJAr19SR9ikZu45mfBuGjhMpGfqKbpZZoFUGz46t/rzj/oRetDiwAAAA"},"images":{"fallback":{"src":"/static/36fd87e316aac4b3df8f3b1a24a3ffeb/5f169/kubernetes-logs-dark.webp","srcSet":"/static/36fd87e316aac4b3df8f3b1a24a3ffeb/d66e1/kubernetes-logs-dark.webp 125w,\n/static/36fd87e316aac4b3df8f3b1a24a3ffeb/e7160/kubernetes-logs-dark.webp 250w,\n/static/36fd87e316aac4b3df8f3b1a24a3ffeb/5f169/kubernetes-logs-dark.webp 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":500}}}},"fields":{"slug":"/blog/kubernetes/structured-logging-in-kubernetes-with-klog"}}]}},"pageContext":{"category":"Kubernetes"}},"staticQueryHashes":["1485533831","4047814605","408154852","4152005505"],"slicesMap":{},"matchPath":"/blog/category/kubernetes"}